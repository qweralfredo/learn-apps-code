{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "capitulo-09-integracao-python-spark\n",
        "\"\"\"\n",
        "\n",
        "# capitulo-09-integracao-python-spark\n",
        "import duckdb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 1\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Ler tabela Delta completa para Pandas\n",
        "df = con.execute(\"\"\"\n",
        "    SELECT * FROM delta_scan('./sales')\n",
        "\"\"\").df()\n",
        "\n",
        "print(type(df))  # <class 'pandas.core.frame.DataFrame'>\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 2\n",
        "# Aplicar filtros antes de converter para Pandas\n",
        "df_filtered = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        order_date,\n",
        "        amount\n",
        "    FROM delta_scan('./sales')\n",
        "    WHERE order_date >= '2024-01-01'\n",
        "        AND amount > 100\n",
        "\"\"\").df()\n",
        "\n",
        "print(f\"Rows loaded: {len(df_filtered):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 3\n",
        "import pandas as pd\n",
        "from deltalake import write_deltalake\n",
        "\n",
        "# Criar DataFrame Pandas\n",
        "df = pd.DataFrame({\n",
        "    'id': range(1000),\n",
        "    'name': [f'Product {i}' for i in range(1000)],\n",
        "    'price': pd.Series([10.5 + i * 0.1 for i in range(1000)])\n",
        "})\n",
        "\n",
        "# Escrever como tabela Delta\n",
        "write_deltalake(\n",
        "    \"./products\",\n",
        "    df,\n",
        "    mode=\"overwrite\"\n",
        ")\n",
        "\n",
        "print(\"✓ Pandas DataFrame written to Delta table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 4\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "from deltalake import write_deltalake\n",
        "\n",
        "def etl_pipeline():\n",
        "    \"\"\"\n",
        "    ETL completo: Delta → Transform → Delta\n",
        "    \"\"\"\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Extract: Ler de Delta\n",
        "    print(\"1. Extracting from Delta...\")\n",
        "    df = con.execute(\"\"\"\n",
        "        SELECT * FROM delta_scan('./raw_data')\n",
        "    \"\"\").df()\n",
        "\n",
        "    # Transform: Processar com Pandas\n",
        "    print(\"2. Transforming with Pandas...\")\n",
        "    df['total_with_tax'] = df['amount'] * 1.1\n",
        "    df['category'] = df['product_name'].str.split().str[0]\n",
        "    df_aggregated = df.groupby('category').agg({\n",
        "        'amount': 'sum',\n",
        "        'total_with_tax': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Load: Escrever para Delta\n",
        "    print(\"3. Loading to Delta...\")\n",
        "    write_deltalake(\n",
        "        \"./processed_data\",\n",
        "        df_aggregated,\n",
        "        mode=\"overwrite\"\n",
        "    )\n",
        "\n",
        "    print(\"✓ ETL pipeline completed\")\n",
        "    return df_aggregated\n",
        "\n",
        "# Executar\n",
        "result = etl_pipeline()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 5\n",
        "import duckdb\n",
        "import polars as pl\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Ler Delta para Polars\n",
        "df_polars = con.execute(\"\"\"\n",
        "    SELECT * FROM delta_scan('./sales')\n",
        "\"\"\").pl()\n",
        "\n",
        "print(type(df_polars))  # <class 'polars.dataframe.frame.DataFrame'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 6\n",
        "import duckdb\n",
        "import polars as pl\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Ler de Delta\n",
        "df = con.execute(\"SELECT * FROM delta_scan('./sales')\").pl()\n",
        "\n",
        "# Processar com Polars (sintaxe lazy)\n",
        "result = (\n",
        "    df\n",
        "    .lazy()\n",
        "    .filter(pl.col('amount') > 100)\n",
        "    .with_columns([\n",
        "        (pl.col('amount') * 1.1).alias('amount_with_tax'),\n",
        "        pl.col('order_date').dt.year().alias('year')\n",
        "    ])\n",
        "    .group_by(['year', 'region'])\n",
        "    .agg([\n",
        "        pl.col('amount').sum().alias('total_amount'),\n",
        "        pl.col('order_id').count().alias('order_count')\n",
        "    ])\n",
        "    .sort('year', descending=True)\n",
        "    .collect()\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 7\n",
        "import polars as pl\n",
        "from deltalake import write_deltalake\n",
        "\n",
        "# Criar DataFrame Polars\n",
        "df = pl.DataFrame({\n",
        "    'id': range(1000),\n",
        "    'value': [i * 2 for i in range(1000)]\n",
        "})\n",
        "\n",
        "# Converter para Pandas primeiro (deltalake requer Pandas ou Arrow)\n",
        "df_pandas = df.to_pandas()\n",
        "\n",
        "# Escrever para Delta\n",
        "write_deltalake(\n",
        "    \"./polars_output\",\n",
        "    df_pandas,\n",
        "    mode=\"overwrite\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 8\n",
        "import duckdb\n",
        "import pyarrow as pa\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Ler Delta para Arrow Table\n",
        "arrow_table = con.execute(\"\"\"\n",
        "    SELECT * FROM delta_scan('./sales')\n",
        "\"\"\").arrow()\n",
        "\n",
        "print(type(arrow_table))  # <class 'pyarrow.lib.Table'>\n",
        "print(f\"Schema: {arrow_table.schema}\")\n",
        "print(f\"Rows: {arrow_table.num_rows:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 9\n",
        "import duckdb\n",
        "import pyarrow as pa\n",
        "import pandas as pd\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Delta → Arrow\n",
        "arrow_table = con.execute(\"SELECT * FROM delta_scan('./sales')\").arrow()\n",
        "\n",
        "# Arrow → Pandas (zero-copy quando possível)\n",
        "df_pandas = arrow_table.to_pandas()\n",
        "\n",
        "# Arrow → Parquet file\n",
        "import pyarrow.parquet as pq\n",
        "pq.write_table(arrow_table, 'output.parquet')\n",
        "\n",
        "# Parquet file → DuckDB\n",
        "result = con.execute(\"SELECT * FROM 'output.parquet'\").df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 10\n",
        "from deltalake import DeltaTable, write_deltalake\n",
        "import pandas as pd\n",
        "\n",
        "# Escrever tabela\n",
        "df = pd.DataFrame({'id': [1, 2, 3], 'value': ['a', 'b', 'c']})\n",
        "write_deltalake(\"./my_table\", df, mode=\"overwrite\")\n",
        "\n",
        "# Ler metadados\n",
        "dt = DeltaTable(\"./my_table\")\n",
        "print(f\"Version: {dt.version()}\")\n",
        "print(f\"Files: {dt.file_uris()}\")\n",
        "print(f\"Schema: {dt.schema()}\")\n",
        "\n",
        "# Histórico de versões\n",
        "print(\"\\nHistory:\")\n",
        "for entry in dt.history():\n",
        "    print(f\"  Version {entry['version']}: {entry['operation']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 11\n",
        "from deltalake import write_deltalake\n",
        "import pandas as pd\n",
        "\n",
        "# Primeira escrita\n",
        "df1 = pd.DataFrame({'id': [1, 2, 3], 'name': ['A', 'B', 'C']})\n",
        "write_deltalake(\"./table\", df1, mode=\"overwrite\")\n",
        "\n",
        "# Append: adiciona linhas\n",
        "df2 = pd.DataFrame({'id': [4, 5], 'name': ['D', 'E']})\n",
        "write_deltalake(\"./table\", df2, mode=\"append\")\n",
        "\n",
        "# Overwrite: substitui tudo\n",
        "df3 = pd.DataFrame({'id': [10, 20], 'name': ['X', 'Y']})\n",
        "write_deltalake(\"./table\", df3, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 12\n",
        "from deltalake import write_deltalake\n",
        "import pandas as pd\n",
        "\n",
        "# Tabela inicial\n",
        "df1 = pd.DataFrame({'id': [1, 2], 'name': ['A', 'B']})\n",
        "write_deltalake(\"./evolving_table\", df1, mode=\"overwrite\")\n",
        "\n",
        "# Adicionar nova coluna (schema evolution)\n",
        "df2 = pd.DataFrame({\n",
        "    'id': [3, 4],\n",
        "    'name': ['C', 'D'],\n",
        "    'new_column': [100, 200]\n",
        "})\n",
        "\n",
        "write_deltalake(\n",
        "    \"./evolving_table\",\n",
        "    df2,\n",
        "    mode=\"append\",\n",
        "    schema_mode=\"merge\"  # Permite adicionar colunas\n",
        ")\n",
        "\n",
        "# Ler com DuckDB\n",
        "import duckdb\n",
        "con = duckdb.connect()\n",
        "result = con.execute(\"SELECT * FROM delta_scan('./evolving_table')\").df()\n",
        "print(result)\n",
        "# Linhas antigas terão NULL na nova coluna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 13\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Criar Spark session com Delta Lake\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DeltaLakeApp\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"✓ Spark {spark.version} with Delta Lake initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 14\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, current_date\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Criar DataFrame\n",
        "df = spark.range(0, 100000).selectExpr(\n",
        "    \"id\",\n",
        "    \"id * 2 as value\",\n",
        "    \"current_date() as created_date\"\n",
        ")\n",
        "\n",
        "# Escrever como Delta table\n",
        "df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"./spark_delta_table\")\n",
        "\n",
        "print(\"✓ Delta table created with Spark\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 15\n",
        "import duckdb\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Ler tabela criada pelo Spark\n",
        "result = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) as total_rows,\n",
        "        AVG(value) as avg_value\n",
        "    FROM delta_scan('./spark_delta_table')\n",
        "\"\"\").fetchone()\n",
        "\n",
        "print(f\"Total rows: {result[0]:,}\")\n",
        "print(f\"Average value: {result[1]:,.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 16\n",
        "from pyspark.sql import SparkSession\n",
        "import duckdb\n",
        "\n",
        "def spark_to_duckdb_pipeline():\n",
        "    \"\"\"\n",
        "    Pipeline: Processar com Spark, analisar com DuckDB\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Processar com Spark (escala para grandes volumes)\n",
        "    spark = SparkSession.builder \\\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    print(\"1. Processing with Spark...\")\n",
        "\n",
        "    # Criar dataset grande\n",
        "    df = spark.range(0, 10_000_000).selectExpr(\n",
        "        \"id\",\n",
        "        \"id % 100 as category\",\n",
        "        \"rand() * 1000 as amount\",\n",
        "        \"current_date() - cast(rand() * 365 as int) as date\"\n",
        "    )\n",
        "\n",
        "    # Escrever particionado\n",
        "    df.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .partitionBy(\"category\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"./large_spark_table\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "    # 2. Analisar com DuckDB (queries analíticas rápidas)\n",
        "    print(\"2. Analyzing with DuckDB...\")\n",
        "\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Query analítica\n",
        "    analysis = con.execute(\"\"\"\n",
        "        SELECT\n",
        "            category,\n",
        "            COUNT(*) as total_records,\n",
        "            AVG(amount) as avg_amount,\n",
        "            MAX(amount) as max_amount\n",
        "        FROM delta_scan('./large_spark_table')\n",
        "        WHERE date >= CURRENT_DATE - INTERVAL '90 days'\n",
        "        GROUP BY category\n",
        "        ORDER BY avg_amount DESC\n",
        "        LIMIT 10\n",
        "    \"\"\").df()\n",
        "\n",
        "    print(\"\\nTop 10 categories by average amount:\")\n",
        "    print(analysis)\n",
        "\n",
        "# Executar\n",
        "spark_to_duckdb_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 17\n",
        "from pyspark.sql import SparkSession\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Criar tabela inicial\n",
        "df = spark.range(100).selectExpr(\"id\", \"id * 2 as value\")\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"./delta_advanced\")\n",
        "\n",
        "# Carregar como DeltaTable\n",
        "deltaTable = DeltaTable.forPath(spark, \"./delta_advanced\")\n",
        "\n",
        "# UPDATE\n",
        "deltaTable.update(\n",
        "    condition=col(\"id\") < 10,\n",
        "    set={\"value\": col(\"value\") * 10}\n",
        ")\n",
        "\n",
        "# DELETE\n",
        "deltaTable.delete(condition=col(\"id\") > 90)\n",
        "\n",
        "# MERGE (UPSERT)\n",
        "new_data = spark.range(95, 105).selectExpr(\"id\", \"id * 3 as value\")\n",
        "\n",
        "deltaTable.alias(\"target\").merge(\n",
        "    new_data.alias(\"source\"),\n",
        "    \"target.id = source.id\"\n",
        ").whenMatchedUpdate(\n",
        "    set={\"value\": col(\"source.value\")}\n",
        ").whenNotMatchedInsert(\n",
        "    values={\"id\": col(\"source.id\"), \"value\": col(\"source.value\")}\n",
        ").execute()\n",
        "\n",
        "# Ler resultado com DuckDB\n",
        "import duckdb\n",
        "con = duckdb.connect()\n",
        "result = con.execute(\"\"\"\n",
        "    SELECT * FROM delta_scan('./delta_advanced')\n",
        "    ORDER BY id\n",
        "\"\"\").df()\n",
        "\n",
        "print(\"After Spark operations:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 18\n",
        "from pyspark.sql import SparkSession\n",
        "import duckdb\n",
        "from datetime import datetime\n",
        "\n",
        "class DataLakehouseWorkflow:\n",
        "    \"\"\"\n",
        "    Workflow usando Spark para ETL pesado e DuckDB para analytics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, delta_path: str):\n",
        "        self.delta_path = delta_path\n",
        "\n",
        "    def heavy_etl_with_spark(self):\n",
        "        \"\"\"\n",
        "        ETL pesado com Spark (ex: processar TBs de dados)\n",
        "        \"\"\"\n",
        "        spark = SparkSession.builder \\\n",
        "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "            .master(\"local[*]\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "        print(\"Running heavy ETL with Spark...\")\n",
        "\n",
        "        # Simular processamento pesado\n",
        "        df = spark.range(0, 5_000_000).selectExpr(\n",
        "            \"id\",\n",
        "            \"id % 1000 as customer_id\",\n",
        "            \"rand() * 1000 as amount\",\n",
        "            \"date_sub(current_date(), cast(rand() * 365 as int)) as order_date\"\n",
        "        )\n",
        "\n",
        "        # Transformações complexas\n",
        "        from pyspark.sql.functions import col, when, sum as spark_sum\n",
        "\n",
        "        df_transformed = df \\\n",
        "            .withColumn(\"amount_category\",\n",
        "                when(col(\"amount\") < 100, \"small\")\n",
        "                .when(col(\"amount\") < 500, \"medium\")\n",
        "                .otherwise(\"large\")\n",
        "            ) \\\n",
        "            .withColumn(\"year\", col(\"order_date\").substr(1, 4).cast(\"int\")) \\\n",
        "            .withColumn(\"month\", col(\"order_date\").substr(6, 2).cast(\"int\"))\n",
        "\n",
        "        # Escrever resultado\n",
        "        df_transformed.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .partitionBy(\"year\", \"month\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(self.delta_path)\n",
        "\n",
        "        spark.stop()\n",
        "        print(f\"✓ ETL completed. Data written to {self.delta_path}\")\n",
        "\n",
        "    def fast_analytics_with_duckdb(self):\n",
        "        \"\"\"\n",
        "        Analytics rápidas com DuckDB\n",
        "        \"\"\"\n",
        "        con = duckdb.connect()\n",
        "\n",
        "        print(\"\\nRunning fast analytics with DuckDB...\")\n",
        "\n",
        "        # Queries analíticas interativas\n",
        "        queries = {\n",
        "            \"Total by category\": \"\"\"\n",
        "                SELECT\n",
        "                    amount_category,\n",
        "                    COUNT(*) as orders,\n",
        "                    SUM(amount) as total_amount\n",
        "                FROM delta_scan('{}')\n",
        "                GROUP BY amount_category\n",
        "            \"\"\",\n",
        "            \"Monthly trend (2024)\": \"\"\"\n",
        "                SELECT\n",
        "                    month,\n",
        "                    COUNT(*) as orders,\n",
        "                    AVG(amount) as avg_amount\n",
        "                FROM delta_scan('{}')\n",
        "                WHERE year = 2024\n",
        "                GROUP BY month\n",
        "                ORDER BY month\n",
        "            \"\"\",\n",
        "            \"Top 10 customers\": \"\"\"\n",
        "                SELECT\n",
        "                    customer_id,\n",
        "                    COUNT(*) as order_count,\n",
        "                    SUM(amount) as total_spent\n",
        "                FROM delta_scan('{}')\n",
        "                GROUP BY customer_id\n",
        "                ORDER BY total_spent DESC\n",
        "                LIMIT 10\n",
        "            \"\"\"\n",
        "        }\n",
        "\n",
        "        for name, query_template in queries.items():\n",
        "            query = query_template.format(self.delta_path)\n",
        "            result = con.execute(query).df()\n",
        "\n",
        "            print(f\"\\n{name}:\")\n",
        "            print(result.to_string(index=False))\n",
        "\n",
        "        con.close()\n",
        "\n",
        "# Executar workflow\n",
        "workflow = DataLakehouseWorkflow(\"./lakehouse_data\")\n",
        "workflow.heavy_etl_with_spark()\n",
        "workflow.fast_analytics_with_duckdb()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 19\n",
        "# Célula 1: Setup\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "from deltalake import write_deltalake\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Célula 2: Carregar dados\n",
        "df = con.execute(\"\"\"\n",
        "    SELECT * FROM delta_scan('./sales')\n",
        "    WHERE order_date >= '2024-01-01'\n",
        "\"\"\").df()\n",
        "\n",
        "display(df.head(10))\n",
        "\n",
        "# Célula 3: Análise exploratória\n",
        "summary = df.describe()\n",
        "display(summary)\n",
        "\n",
        "# Célula 4: Visualização\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df.groupby('region')['amount'].sum().plot(kind='bar')\n",
        "plt.title('Sales by Region')\n",
        "plt.ylabel('Total Amount')\n",
        "plt.show()\n",
        "\n",
        "# Célula 5: Agregação\n",
        "monthly = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        DATE_TRUNC('month', order_date) as month,\n",
        "        COUNT(*) as orders,\n",
        "        SUM(amount) as revenue\n",
        "    FROM delta_scan('./sales')\n",
        "    WHERE order_date >= '2024-01-01'\n",
        "    GROUP BY 1\n",
        "    ORDER BY 1\n",
        "\"\"\").df()\n",
        "\n",
        "display(monthly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 20\n",
        "# Heavy processing: Spark\n",
        "spark_df.write.format(\"delta\").save(\"./processed\")\n",
        "\n",
        "# Analytics: DuckDB\n",
        "duckdb.execute(\"SELECT * FROM delta_scan('./processed')\").df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 21\n",
        "# Eficiente: zero-copy\n",
        "arrow_table = con.execute(\"SELECT * FROM delta_scan('./data')\").arrow()\n",
        "df = arrow_table.to_pandas()\n",
        "\n",
        "# Menos eficiente: cópia\n",
        "df = con.execute(\"SELECT * FROM delta_scan('./data')\").df()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}