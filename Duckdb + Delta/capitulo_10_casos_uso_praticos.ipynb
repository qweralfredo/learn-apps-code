{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb32a78",
   "metadata": {},
   "source": [
    "## 3. Melhores Práticas Gerais\n",
    "\n",
    "**Arquitetura:**\n",
    "- Use Spark para ETL pesado (TBs)\n",
    "- Use DuckDB para analytics (GBs-TBs)\n",
    "- Use Pandas/Polars para transformações\n",
    "\n",
    "**Performance:**\n",
    "- Aplique partition pruning\n",
    "- Use projection pushdown\n",
    "- Cache queries frequentes\n",
    "- Configure row groups adequadamente\n",
    "\n",
    "**Segurança:**\n",
    "- Use secrets para credenciais\n",
    "- Implemente rotação de credenciais\n",
    "- Valide qualidade de dados\n",
    "\n",
    "**Organização:**\n",
    "- Estruture código modular\n",
    "- Implemente logging\n",
    "- Adicione testes automatizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc174be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class QualityIssue:\n",
    "    table: str\n",
    "    column: str\n",
    "    issue_type: str\n",
    "    severity: str\n",
    "    count: int\n",
    "\n",
    "class DataQualityMonitor:\n",
    "    \"\"\"Monitor de qualidade de dados\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.con = duckdb.connect()\n",
    "        self.issues: List[QualityIssue] = []\n",
    "    \n",
    "    def check_nulls(self, table_path: str, critical_columns: List[str]) -> List[QualityIssue]:\n",
    "        \"\"\"Verificar valores nulos\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for column in critical_columns:\n",
    "            query = f\"\"\"\n",
    "            SELECT COUNT(*) as total, SUM(CASE WHEN \"{column}\" IS NULL THEN 1 ELSE 0 END) as nulls\n",
    "            FROM delta_scan('{table_path}')\n",
    "            \"\"\"\n",
    "            result = self.con.execute(query).fetchone()\n",
    "            total, nulls = result\n",
    "            \n",
    "            if nulls > 0:\n",
    "                null_pct = (nulls / total) * 100\n",
    "                issues.append(QualityIssue(\n",
    "                    table=table_path,\n",
    "                    column=column,\n",
    "                    issue_type='null_values',\n",
    "                    severity='high' if null_pct > 5 else 'medium',\n",
    "                    count=nulls\n",
    "                ))\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def check_duplicates(self, table_path: str, key_columns: List[str]) -> List[QualityIssue]:\n",
    "        \"\"\"Verificar duplicatas\"\"\"\n",
    "        key_cols = ', '.join(f'\"{col}\"' for col in key_columns)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        WITH dups AS (\n",
    "            SELECT {key_cols}, COUNT(*) as cnt\n",
    "            FROM delta_scan('{table_path}')\n",
    "            GROUP BY {key_cols}\n",
    "            HAVING COUNT(*) > 1\n",
    "        )\n",
    "        SELECT COUNT(*) as dup_groups, SUM(cnt) as total_dups\n",
    "        FROM dups\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.con.execute(query).fetchone()\n",
    "        dup_groups, total_dups = result\n",
    "        \n",
    "        if dup_groups and dup_groups > 0:\n",
    "            return [QualityIssue(\n",
    "                table=table_path,\n",
    "                column=','.join(key_columns),\n",
    "                issue_type='duplicates',\n",
    "                severity='high',\n",
    "                count=total_dups\n",
    "            )]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def run_all_checks(self, table_path: str, critical_cols: List[str], key_cols: List[str]):\n",
    "        \"\"\"Executar todas as verificações\"\"\"\n",
    "        print(f\"Running quality checks on {table_path}...\")\n",
    "        \n",
    "        self.issues.extend(self.check_nulls(table_path, critical_cols))\n",
    "        self.issues.extend(self.check_duplicates(table_path, key_cols))\n",
    "        \n",
    "        if self.issues:\n",
    "            print(f\"\\n⚠ {len(self.issues)} issues encontrados:\")\n",
    "            for issue in self.issues:\n",
    "                print(f\"  - {issue.severity.upper()}: {issue.issue_type} em {issue.column} ({issue.count})\")\n",
    "        else:\n",
    "            print(\"\\n✓ Sem issues de qualidade!\")\n",
    "\n",
    "# Uso\n",
    "monitor = DataQualityMonitor()\n",
    "monitor.run_all_checks(\n",
    "    table_path='./sales_partitioned',\n",
    "    critical_cols=['order_id', 'amount'],\n",
    "    key_cols=['order_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f6a81",
   "metadata": {},
   "source": [
    "## 2. Data Quality Monitor\n",
    "\n",
    "Monitor de qualidade com checagens automatizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EcommerceAnalytics:\n",
    "    \"\"\"Pipeline de analytics para e-commerce\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = base_path\n",
    "        self.con = duckdb.connect()\n",
    "    \n",
    "    def get_sales_metrics(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"Métricas de vendas\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            DATE_TRUNC('day', order_date) as date,\n",
    "            COUNT(DISTINCT order_id) as orders,\n",
    "            COUNT(DISTINCT customer_id) as unique_customers,\n",
    "            SUM(amount) as revenue,\n",
    "            AVG(amount) as avg_order_value,\n",
    "            -- Moving average 7 dias\n",
    "            AVG(SUM(amount)) OVER (\n",
    "                ORDER BY DATE_TRUNC('day', order_date)\n",
    "                ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "            ) as revenue_7d_ma\n",
    "        FROM delta_scan('{self.base_path}')\n",
    "        WHERE order_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        GROUP BY date\n",
    "        ORDER BY date DESC\n",
    "        \"\"\"\n",
    "        return self.con.execute(query).df()\n",
    "    \n",
    "    def get_customer_segments(self) -> pd.DataFrame:\n",
    "        \"\"\"Segmentação RFM\"\"\"\n",
    "        query = f\"\"\"\n",
    "        WITH rfm AS (\n",
    "            SELECT\n",
    "                customer_id,\n",
    "                DATEDIFF('day', MAX(order_date), CURRENT_DATE) as recency,\n",
    "                COUNT(*) as frequency,\n",
    "                SUM(amount) as monetary\n",
    "            FROM delta_scan('{self.base_path}')\n",
    "            GROUP BY customer_id\n",
    "        )\n",
    "        SELECT\n",
    "            CASE\n",
    "                WHEN recency <= 30 AND frequency >= 10 THEN 'Champions'\n",
    "                WHEN recency <= 60 AND frequency >= 5 THEN 'Loyal'\n",
    "                WHEN recency <= 90 THEN 'Promising'\n",
    "                WHEN recency > 180 AND frequency >= 5 THEN 'At Risk'\n",
    "                ELSE 'Hibernating'\n",
    "            END as segment,\n",
    "            COUNT(*) as customer_count,\n",
    "            AVG(recency) as avg_recency,\n",
    "            AVG(frequency) as avg_frequency,\n",
    "            AVG(monetary) as avg_monetary\n",
    "        FROM rfm\n",
    "        GROUP BY segment\n",
    "        \"\"\"\n",
    "        return self.con.execute(query).df()\n",
    "\n",
    "# Uso\n",
    "pipeline = EcommerceAnalytics('./sales_partitioned')\n",
    "\n",
    "# Métricas últimos 30 dias\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "\n",
    "metrics = pipeline.get_sales_metrics(start_date, end_date)\n",
    "print(\"=== SALES METRICS ===\")\n",
    "print(metrics.head())\n",
    "\n",
    "segments = pipeline.get_customer_segments()\n",
    "print(\"\\n=== CUSTOMER SEGMENTS ===\")\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f34154",
   "metadata": {},
   "source": [
    "## 1. E-commerce Analytics Pipeline\n",
    "\n",
    "Pipeline completo para análise de vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e968cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação\n",
    "%pip install duckdb deltalake pyarrow pandas -q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
