{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "capitulo-10-casos-uso-praticos\n",
        "\"\"\"\n",
        "\n",
        "# capitulo-10-casos-uso-praticos\n",
        "import duckdb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 1\n",
        "import duckdb\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List\n",
        "import pandas as pd\n",
        "\n",
        "class EcommerceAnalyticsPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline de analytics para e-commerce\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, s3_base_path: str):\n",
        "        self.s3_base_path = s3_base_path\n",
        "        self.con = duckdb.connect()\n",
        "        self._setup_secrets()\n",
        "\n",
        "    def _setup_secrets(self):\n",
        "        \"\"\"Configurar acesso S3\"\"\"\n",
        "        self.con.execute(\"LOAD httpfs\")\n",
        "        self.con.execute(\"\"\"\n",
        "            CREATE SECRET (\n",
        "                TYPE S3,\n",
        "                PROVIDER credential_chain\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "    def get_sales_metrics(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calcular métricas principais de vendas\n",
        "        \"\"\"\n",
        "        query = f\"\"\"\n",
        "        WITH daily_sales AS (\n",
        "            SELECT\n",
        "                DATE_TRUNC('day', order_date) as date,\n",
        "                COUNT(DISTINCT order_id) as orders,\n",
        "                COUNT(DISTINCT customer_id) as unique_customers,\n",
        "                SUM(total_amount) as revenue,\n",
        "                AVG(total_amount) as avg_order_value\n",
        "            FROM delta_scan('{self.s3_base_path}/sales')\n",
        "            WHERE order_date BETWEEN '{start_date}' AND '{end_date}'\n",
        "            GROUP BY 1\n",
        "        )\n",
        "        SELECT\n",
        "            date,\n",
        "            orders,\n",
        "            unique_customers,\n",
        "            revenue,\n",
        "            avg_order_value,\n",
        "            -- Moving averages\n",
        "            AVG(revenue) OVER (\n",
        "                ORDER BY date\n",
        "                ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
        "            ) as revenue_7d_ma,\n",
        "            -- Growth rates\n",
        "            (revenue - LAG(revenue, 7) OVER (ORDER BY date)) /\n",
        "                LAG(revenue, 7) OVER (ORDER BY date) * 100 as wow_growth_pct\n",
        "        FROM daily_sales\n",
        "        ORDER BY date DESC\n",
        "        \"\"\"\n",
        "\n",
        "        return self.con.execute(query).df()\n",
        "\n",
        "    def get_product_performance(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Análise de performance por produto\n",
        "        \"\"\"\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            p.product_id,\n",
        "            p.product_name,\n",
        "            p.category,\n",
        "            COUNT(DISTINCT s.order_id) as times_ordered,\n",
        "            SUM(s.quantity) as total_quantity_sold,\n",
        "            SUM(s.line_total) as total_revenue,\n",
        "            AVG(s.line_total / s.quantity) as avg_selling_price,\n",
        "            COUNT(DISTINCT s.customer_id) as unique_buyers\n",
        "        FROM delta_scan('{self.s3_base_path}/order_items') s\n",
        "        JOIN delta_scan('{self.s3_base_path}/products') p\n",
        "            ON s.product_id = p.product_id\n",
        "        WHERE s.order_date BETWEEN '{start_date}' AND '{end_date}'\n",
        "        GROUP BY p.product_id, p.product_name, p.category\n",
        "        ORDER BY total_revenue DESC\n",
        "        LIMIT 100\n",
        "        \"\"\"\n",
        "\n",
        "        return self.con.execute(query).df()\n",
        "\n",
        "    def get_customer_segments(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Segmentação RFM de clientes\n",
        "        \"\"\"\n",
        "        query = f\"\"\"\n",
        "        WITH customer_metrics AS (\n",
        "            SELECT\n",
        "                customer_id,\n",
        "                MAX(order_date) as last_order_date,\n",
        "                COUNT(DISTINCT order_id) as frequency,\n",
        "                SUM(total_amount) as monetary\n",
        "            FROM delta_scan('{self.s3_base_path}/sales')\n",
        "            GROUP BY customer_id\n",
        "        ),\n",
        "        rfm_scored AS (\n",
        "            SELECT\n",
        "                customer_id,\n",
        "                DATEDIFF('day', last_order_date, CURRENT_DATE) as recency,\n",
        "                frequency,\n",
        "                monetary,\n",
        "                -- RFM scoring (1-5)\n",
        "                NTILE(5) OVER (ORDER BY DATEDIFF('day', last_order_date, CURRENT_DATE) DESC) as r_score,\n",
        "                NTILE(5) OVER (ORDER BY frequency) as f_score,\n",
        "                NTILE(5) OVER (ORDER BY monetary) as m_score\n",
        "            FROM customer_metrics\n",
        "        )\n",
        "        SELECT\n",
        "            CASE\n",
        "                WHEN r_score >= 4 AND f_score >= 4 THEN 'Champions'\n",
        "                WHEN r_score >= 3 AND f_score >= 3 THEN 'Loyal Customers'\n",
        "                WHEN r_score >= 4 AND f_score < 3 THEN 'Promising'\n",
        "                WHEN r_score < 3 AND f_score >= 4 THEN 'At Risk'\n",
        "                WHEN r_score < 3 AND f_score < 3 THEN 'Hibernating'\n",
        "                ELSE 'Others'\n",
        "            END as segment,\n",
        "            COUNT(*) as customer_count,\n",
        "            AVG(recency) as avg_recency_days,\n",
        "            AVG(frequency) as avg_frequency,\n",
        "            AVG(monetary) as avg_monetary\n",
        "        FROM rfm_scored\n",
        "        GROUP BY segment\n",
        "        ORDER BY avg_monetary DESC\n",
        "        \"\"\"\n",
        "\n",
        "        return self.con.execute(query).df()\n",
        "\n",
        "    def generate_daily_report(self, date: str = None) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Gerar relatório diário completo\n",
        "        \"\"\"\n",
        "        if date is None:\n",
        "            date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "        # Período: últimos 30 dias\n",
        "        end_date = date\n",
        "        start_date = (datetime.strptime(date, '%Y-%m-%d') - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "\n",
        "        print(f\"Generating report for {start_date} to {end_date}...\")\n",
        "\n",
        "        report = {\n",
        "            'sales_metrics': self.get_sales_metrics(start_date, end_date),\n",
        "            'product_performance': self.get_product_performance(start_date, end_date),\n",
        "            'customer_segments': self.get_customer_segments()\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "    def close(self):\n",
        "        self.con.close()\n",
        "\n",
        "\n",
        "# Uso\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = EcommerceAnalyticsPipeline('s3://my-data-lake/ecommerce')\n",
        "\n",
        "    # Gerar relatório\n",
        "    report = pipeline.generate_daily_report()\n",
        "\n",
        "    print(\"\\n=== SALES METRICS ===\")\n",
        "    print(report['sales_metrics'].head(10))\n",
        "\n",
        "    print(\"\\n=== TOP PRODUCTS ===\")\n",
        "    print(report['product_performance'].head(10))\n",
        "\n",
        "    print(\"\\n=== CUSTOMER SEGMENTS ===\")\n",
        "    print(report['customer_segments'])\n",
        "\n",
        "    # Exportar para análise adicional\n",
        "    report['sales_metrics'].to_csv('daily_sales.csv', index=False)\n",
        "    report['product_performance'].to_csv('product_performance.csv', index=False)\n",
        "\n",
        "    pipeline.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 2\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class QualityIssue:\n",
        "    table: str\n",
        "    column: str\n",
        "    issue_type: str\n",
        "    severity: str\n",
        "    count: int\n",
        "    sample_values: List\n",
        "\n",
        "class DataQualityMonitor:\n",
        "    \"\"\"\n",
        "    Monitor de qualidade de dados para tabelas Delta\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.con = duckdb.connect()\n",
        "        self.issues: List[QualityIssue] = []\n",
        "\n",
        "    def check_nulls(self, table_path: str, critical_columns: List[str]) -> List[QualityIssue]:\n",
        "        \"\"\"\n",
        "        Verificar valores nulos em colunas críticas\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "\n",
        "        for column in critical_columns:\n",
        "            query = f\"\"\"\n",
        "            SELECT\n",
        "                COUNT(*) as total_rows,\n",
        "                SUM(CASE WHEN \"{column}\" IS NULL THEN 1 ELSE 0 END) as null_count\n",
        "            FROM delta_scan('{table_path}')\n",
        "            \"\"\"\n",
        "\n",
        "            result = self.con.execute(query).fetchone()\n",
        "            total_rows, null_count = result\n",
        "\n",
        "            if null_count > 0:\n",
        "                null_pct = (null_count / total_rows) * 100\n",
        "\n",
        "                issue = QualityIssue(\n",
        "                    table=table_path,\n",
        "                    column=column,\n",
        "                    issue_type='null_values',\n",
        "                    severity='high' if null_pct > 5 else 'medium',\n",
        "                    count=null_count,\n",
        "                    sample_values=[]\n",
        "                )\n",
        "                issues.append(issue)\n",
        "\n",
        "        return issues\n",
        "\n",
        "    def check_duplicates(self, table_path: str, key_columns: List[str]) -> List[QualityIssue]:\n",
        "        \"\"\"\n",
        "        Verificar duplicatas baseado em colunas chave\n",
        "        \"\"\"\n",
        "        key_cols = ', '.join(f'\"{col}\"' for col in key_columns)\n",
        "\n",
        "        query = f\"\"\"\n",
        "        WITH duplicates AS (\n",
        "            SELECT\n",
        "                {key_cols},\n",
        "                COUNT(*) as dup_count\n",
        "            FROM delta_scan('{table_path}')\n",
        "            GROUP BY {key_cols}\n",
        "            HAVING COUNT(*) > 1\n",
        "        )\n",
        "        SELECT COUNT(*) as duplicate_groups, SUM(dup_count) as total_duplicates\n",
        "        FROM duplicates\n",
        "        \"\"\"\n",
        "\n",
        "        result = self.con.execute(query).fetchone()\n",
        "        dup_groups, total_dups = result\n",
        "\n",
        "        if dup_groups and dup_groups > 0:\n",
        "            issue = QualityIssue(\n",
        "                table=table_path,\n",
        "                column=','.join(key_columns),\n",
        "                issue_type='duplicates',\n",
        "                severity='high',\n",
        "                count=total_dups,\n",
        "                sample_values=[]\n",
        "            )\n",
        "            return [issue]\n",
        "\n",
        "        return []\n",
        "\n",
        "    def check_outliers(self, table_path: str, numeric_columns: List[str]) -> List[QualityIssue]:\n",
        "        \"\"\"\n",
        "        Detectar outliers usando IQR method\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "\n",
        "        for column in numeric_columns:\n",
        "            query = f\"\"\"\n",
        "            WITH stats AS (\n",
        "                SELECT\n",
        "                    APPROX_QUANTILE(\"{column}\", 0.25) as q1,\n",
        "                    APPROX_QUANTILE(\"{column}\", 0.75) as q3\n",
        "                FROM delta_scan('{table_path}')\n",
        "                WHERE \"{column}\" IS NOT NULL\n",
        "            ),\n",
        "            bounds AS (\n",
        "                SELECT\n",
        "                    q1 - 1.5 * (q3 - q1) as lower_bound,\n",
        "                    q3 + 1.5 * (q3 - q1) as upper_bound\n",
        "                FROM stats\n",
        "            )\n",
        "            SELECT COUNT(*) as outlier_count\n",
        "            FROM delta_scan('{table_path}'), bounds\n",
        "            WHERE \"{column}\" < lower_bound OR \"{column}\" > upper_bound\n",
        "            \"\"\"\n",
        "\n",
        "            result = self.con.execute(query).fetchone()\n",
        "            outlier_count = result[0]\n",
        "\n",
        "            if outlier_count > 0:\n",
        "                issue = QualityIssue(\n",
        "                    table=table_path,\n",
        "                    column=column,\n",
        "                    issue_type='outliers',\n",
        "                    severity='medium',\n",
        "                    count=outlier_count,\n",
        "                    sample_values=[]\n",
        "                )\n",
        "                issues.append(issue)\n",
        "\n",
        "        return issues\n",
        "\n",
        "    def check_schema_drift(self, table_path: str, expected_schema: Dict[str, str]) -> List[QualityIssue]:\n",
        "        \"\"\"\n",
        "        Verificar se schema mudou inesperadamente\n",
        "        \"\"\"\n",
        "        # Obter schema atual\n",
        "        query = f\"DESCRIBE SELECT * FROM delta_scan('{table_path}') LIMIT 0\"\n",
        "        current_schema_df = self.con.execute(query).df()\n",
        "\n",
        "        current_schema = dict(zip(\n",
        "            current_schema_df['column_name'],\n",
        "            current_schema_df['column_type']\n",
        "        ))\n",
        "\n",
        "        issues = []\n",
        "\n",
        "        # Verificar colunas faltantes\n",
        "        for col, expected_type in expected_schema.items():\n",
        "            if col not in current_schema:\n",
        "                issue = QualityIssue(\n",
        "                    table=table_path,\n",
        "                    column=col,\n",
        "                    issue_type='missing_column',\n",
        "                    severity='high',\n",
        "                    count=1,\n",
        "                    sample_values=[]\n",
        "                )\n",
        "                issues.append(issue)\n",
        "            elif current_schema[col] != expected_type:\n",
        "                issue = QualityIssue(\n",
        "                    table=table_path,\n",
        "                    column=col,\n",
        "                    issue_type='type_mismatch',\n",
        "                    severity='high',\n",
        "                    count=1,\n",
        "                    sample_values=[f\"Expected: {expected_type}, Got: {current_schema[col]}\"]\n",
        "                )\n",
        "                issues.append(issue)\n",
        "\n",
        "        return issues\n",
        "\n",
        "    def run_full_check(\n",
        "        self,\n",
        "        table_path: str,\n",
        "        critical_columns: List[str],\n",
        "        key_columns: List[str],\n",
        "        numeric_columns: List[str],\n",
        "        expected_schema: Dict[str, str]\n",
        "    ) -> List[QualityIssue]:\n",
        "        \"\"\"\n",
        "        Executar verificação completa de qualidade\n",
        "        \"\"\"\n",
        "        print(f\"Running quality checks on {table_path}...\")\n",
        "\n",
        "        all_issues = []\n",
        "\n",
        "        # Nulls\n",
        "        print(\"  Checking for null values...\")\n",
        "        all_issues.extend(self.check_nulls(table_path, critical_columns))\n",
        "\n",
        "        # Duplicates\n",
        "        print(\"  Checking for duplicates...\")\n",
        "        all_issues.extend(self.check_duplicates(table_path, key_columns))\n",
        "\n",
        "        # Outliers\n",
        "        print(\"  Checking for outliers...\")\n",
        "        all_issues.extend(self.check_outliers(table_path, numeric_columns))\n",
        "\n",
        "        # Schema drift\n",
        "        print(\"  Checking schema...\")\n",
        "        all_issues.extend(self.check_schema_drift(table_path, expected_schema))\n",
        "\n",
        "        self.issues.extend(all_issues)\n",
        "        return all_issues\n",
        "\n",
        "    def generate_report(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Gerar relatório de qualidade\n",
        "        \"\"\"\n",
        "        if not self.issues:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        report_data = [\n",
        "            {\n",
        "                'table': issue.table,\n",
        "                'column': issue.column,\n",
        "                'issue_type': issue.issue_type,\n",
        "                'severity': issue.severity,\n",
        "                'count': issue.count\n",
        "            }\n",
        "            for issue in self.issues\n",
        "        ]\n",
        "\n",
        "        return pd.DataFrame(report_data)\n",
        "\n",
        "\n",
        "# Uso\n",
        "if __name__ == \"__main__\":\n",
        "    monitor = DataQualityMonitor()\n",
        "\n",
        "    # Definir expectativas\n",
        "    expected_schema = {\n",
        "        'order_id': 'BIGINT',\n",
        "        'customer_id': 'BIGINT',\n",
        "        'order_date': 'DATE',\n",
        "        'total_amount': 'DOUBLE'\n",
        "    }\n",
        "\n",
        "    # Executar verificações\n",
        "    issues = monitor.run_full_check(\n",
        "        table_path='./sales',\n",
        "        critical_columns=['order_id', 'customer_id', 'total_amount'],\n",
        "        key_columns=['order_id'],\n",
        "        numeric_columns=['total_amount'],\n",
        "        expected_schema=expected_schema\n",
        "    )\n",
        "\n",
        "    # Gerar relatório\n",
        "    report = monitor.generate_report()\n",
        "\n",
        "    if len(report) > 0:\n",
        "        print(\"\\n⚠ QUALITY ISSUES FOUND:\")\n",
        "        print(report.to_string(index=False))\n",
        "\n",
        "        # Exportar\n",
        "        report.to_csv('data_quality_report.csv', index=False)\n",
        "    else:\n",
        "        print(\"\\n✓ No quality issues found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 3\n",
        "import duckdb\n",
        "from flask import Flask, jsonify\n",
        "from datetime import datetime, timedelta\n",
        "from functools import lru_cache\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "class DashboardBackend:\n",
        "    \"\"\"\n",
        "    Backend para dashboard em tempo real\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, delta_path: str):\n",
        "        self.delta_path = delta_path\n",
        "        # Usar conexão compartilhada\n",
        "        self.con = duckdb.connect(':memory:', read_only=False)\n",
        "        self.con.execute(\"LOAD delta\")\n",
        "        self._setup_views()\n",
        "\n",
        "    def _setup_views(self):\n",
        "        \"\"\"\n",
        "        Criar views para queries frequentes\n",
        "        \"\"\"\n",
        "        self.con.execute(f\"\"\"\n",
        "            CREATE OR REPLACE VIEW sales AS\n",
        "            SELECT * FROM delta_scan('{self.delta_path}/sales')\n",
        "        \"\"\")\n",
        "\n",
        "        self.con.execute(f\"\"\"\n",
        "            CREATE OR REPLACE VIEW products AS\n",
        "            SELECT * FROM delta_scan('{self.delta_path}/products')\n",
        "        \"\"\")\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def get_metrics_cached(self, date_str: str):\n",
        "        \"\"\"\n",
        "        Obter métricas com cache (atualiza a cada minuto)\n",
        "        \"\"\"\n",
        "        # Cache baseado em timestamp arredondado para minuto\n",
        "        # Isso permite cache de ~1 minuto\n",
        "        minute_timestamp = int(time.time() / 60)\n",
        "\n",
        "        return self._get_metrics_impl(date_str, minute_timestamp)\n",
        "\n",
        "    def _get_metrics_impl(self, date_str: str, _cache_key: int):\n",
        "        \"\"\"\n",
        "        Implementação interna (não fazer cache diretamente desta)\n",
        "        \"\"\"\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(DISTINCT order_id) as total_orders,\n",
        "            COUNT(DISTINCT customer_id) as unique_customers,\n",
        "            SUM(total_amount) as revenue,\n",
        "            AVG(total_amount) as avg_order_value\n",
        "        FROM sales\n",
        "        WHERE DATE_TRUNC('day', order_date) = '{date_str}'\n",
        "        \"\"\"\n",
        "\n",
        "        result = self.con.execute(query).fetchone()\n",
        "\n",
        "        return {\n",
        "            'date': date_str,\n",
        "            'total_orders': result[0],\n",
        "            'unique_customers': result[1],\n",
        "            'revenue': float(result[2]) if result[2] else 0,\n",
        "            'avg_order_value': float(result[3]) if result[3] else 0\n",
        "        }\n",
        "\n",
        "    def get_hourly_trend(self, date_str: str):\n",
        "        \"\"\"\n",
        "        Tendência horária de vendas\n",
        "        \"\"\"\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            DATE_TRUNC('hour', order_timestamp) as hour,\n",
        "            COUNT(*) as orders,\n",
        "            SUM(total_amount) as revenue\n",
        "        FROM sales\n",
        "        WHERE DATE_TRUNC('day', order_date) = '{date_str}'\n",
        "        GROUP BY hour\n",
        "        ORDER BY hour\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.con.execute(query).df()\n",
        "        return df.to_dict('records')\n",
        "\n",
        "    def get_top_products(self, limit: int = 10):\n",
        "        \"\"\"\n",
        "        Top produtos do dia\n",
        "        \"\"\"\n",
        "        today = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            p.product_name,\n",
        "            COUNT(*) as times_ordered,\n",
        "            SUM(s.quantity) as quantity_sold,\n",
        "            SUM(s.line_total) as revenue\n",
        "        FROM sales s\n",
        "        JOIN products p ON s.product_id = p.product_id\n",
        "        WHERE DATE_TRUNC('day', s.order_date) = '{today}'\n",
        "        GROUP BY p.product_name\n",
        "        ORDER BY revenue DESC\n",
        "        LIMIT {limit}\n",
        "        \"\"\"\n",
        "\n",
        "        df = self.con.execute(query).df()\n",
        "        return df.to_dict('records')\n",
        "\n",
        "\n",
        "# Inicializar backend\n",
        "backend = DashboardBackend('./data_lake')\n",
        "\n",
        "# Endpoints Flask\n",
        "@app.route('/api/metrics')\n",
        "def metrics():\n",
        "    \"\"\"Métricas do dia atual\"\"\"\n",
        "    today = datetime.now().strftime('%Y-%m-%d')\n",
        "    data = backend.get_metrics_cached(today)\n",
        "    return jsonify(data)\n",
        "\n",
        "@app.route('/api/metrics/<date>')\n",
        "def metrics_by_date(date):\n",
        "    \"\"\"Métricas de data específica\"\"\"\n",
        "    data = backend.get_metrics_cached(date)\n",
        "    return jsonify(data)\n",
        "\n",
        "@app.route('/api/trend/hourly')\n",
        "def hourly_trend():\n",
        "    \"\"\"Tendência horária\"\"\"\n",
        "    today = datetime.now().strftime('%Y-%m-%d')\n",
        "    data = backend.get_hourly_trend(today)\n",
        "    return jsonify(data)\n",
        "\n",
        "@app.route('/api/products/top')\n",
        "def top_products():\n",
        "    \"\"\"Top produtos\"\"\"\n",
        "    data = backend.get_top_products(limit=10)\n",
        "    return jsonify(data)\n",
        "\n",
        "@app.route('/api/health')\n",
        "def health():\n",
        "    \"\"\"Health check\"\"\"\n",
        "    return jsonify({'status': 'ok', 'timestamp': datetime.now().isoformat()})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, port=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 4\n",
        "import duckdb\n",
        "from deltalake import write_deltalake\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class DataLakeMigration:\n",
        "    \"\"\"\n",
        "    Migração de PostgreSQL para Delta Lake\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pg_connection_string: str, delta_base_path: str):\n",
        "        self.pg_conn = pg_connection_string\n",
        "        self.delta_path = delta_base_path\n",
        "        self.con = duckdb.connect()\n",
        "\n",
        "        # Carregar extensões\n",
        "        self.con.execute(\"INSTALL postgres\")\n",
        "        self.con.execute(\"LOAD postgres\")\n",
        "\n",
        "    def migrate_table(self, table_name: str, partition_column: str = None):\n",
        "        \"\"\"\n",
        "        Migrar tabela do Postgres para Delta\n",
        "        \"\"\"\n",
        "        print(f\"Migrating table: {table_name}\")\n",
        "\n",
        "        # 1. Attach PostgreSQL\n",
        "        self.con.execute(f\"\"\"\n",
        "            ATTACH '{self.pg_conn}' AS pg (TYPE postgres)\n",
        "        \"\"\")\n",
        "\n",
        "        # 2. Ler dados do PostgreSQL\n",
        "        print(\"  Reading from PostgreSQL...\")\n",
        "        df = self.con.execute(f\"\"\"\n",
        "            SELECT * FROM pg.public.{table_name}\n",
        "        \"\"\").df()\n",
        "\n",
        "        print(f\"  Rows to migrate: {len(df):,}\")\n",
        "\n",
        "        # 3. Adicionar metadata de migração\n",
        "        df['_migrated_at'] = datetime.now()\n",
        "        df['_source'] = 'postgresql'\n",
        "\n",
        "        # 4. Escrever para Delta\n",
        "        print(\"  Writing to Delta Lake...\")\n",
        "        delta_table_path = f\"{self.delta_path}/{table_name}\"\n",
        "\n",
        "        write_deltalake(\n",
        "            delta_table_path,\n",
        "            df,\n",
        "            partition_by=[partition_column] if partition_column else None,\n",
        "            mode=\"overwrite\"\n",
        "        )\n",
        "\n",
        "        print(f\"  ✓ Migration completed: {delta_table_path}\")\n",
        "\n",
        "        # 5. Verificar\n",
        "        count = self.con.execute(f\"\"\"\n",
        "            SELECT COUNT(*) FROM delta_scan('{delta_table_path}')\n",
        "        \"\"\").fetchone()[0]\n",
        "\n",
        "        print(f\"  Verification: {count:,} rows in Delta table\")\n",
        "\n",
        "        return delta_table_path\n",
        "\n",
        "    def incremental_sync(self, table_name: str, timestamp_column: str):\n",
        "        \"\"\"\n",
        "        Sincronização incremental (apenas novos/alterados)\n",
        "        \"\"\"\n",
        "        delta_table_path = f\"{self.delta_path}/{table_name}\"\n",
        "\n",
        "        # Obter último timestamp migrado\n",
        "        last_sync = self.con.execute(f\"\"\"\n",
        "            SELECT MAX({timestamp_column}) as last_ts\n",
        "            FROM delta_scan('{delta_table_path}')\n",
        "        \"\"\").fetchone()[0]\n",
        "\n",
        "        print(f\"Last sync: {last_sync}\")\n",
        "\n",
        "        # Ler apenas registros novos\n",
        "        self.con.execute(f\"ATTACH '{self.pg_conn}' AS pg (TYPE postgres)\")\n",
        "\n",
        "        new_data = self.con.execute(f\"\"\"\n",
        "            SELECT *\n",
        "            FROM pg.public.{table_name}\n",
        "            WHERE {timestamp_column} > '{last_sync}'\n",
        "        \"\"\").df()\n",
        "\n",
        "        if len(new_data) == 0:\n",
        "            print(\"No new data to sync\")\n",
        "            return\n",
        "\n",
        "        print(f\"Syncing {len(new_data):,} new rows...\")\n",
        "\n",
        "        # Adicionar metadata\n",
        "        new_data['_migrated_at'] = datetime.now()\n",
        "        new_data['_source'] = 'postgresql'\n",
        "\n",
        "        # Append para Delta\n",
        "        write_deltalake(\n",
        "            delta_table_path,\n",
        "            new_data,\n",
        "            mode=\"append\"\n",
        "        )\n",
        "\n",
        "        print(\"✓ Incremental sync completed\")\n",
        "\n",
        "    def hybrid_query(self, query: str):\n",
        "        \"\"\"\n",
        "        Query híbrida: dados de PostgreSQL + Delta Lake\n",
        "        \"\"\"\n",
        "        self.con.execute(f\"ATTACH '{self.pg_conn}' AS pg (TYPE postgres)\")\n",
        "\n",
        "        # Exemplo: JOIN entre Postgres e Delta\n",
        "        result = self.con.execute(query).df()\n",
        "        return result\n",
        "\n",
        "\n",
        "# Uso\n",
        "if __name__ == \"__main__\":\n",
        "    migration = DataLakeMigration(\n",
        "        pg_connection_string='host=localhost port=5432 dbname=mydb user=user password=pass',\n",
        "        delta_base_path='./data_lake'\n",
        "    )\n",
        "\n",
        "    # Migração inicial\n",
        "    migration.migrate_table('orders', partition_column='order_date')\n",
        "    migration.migrate_table('customers')\n",
        "\n",
        "    # Sincronização incremental periódica\n",
        "    migration.incremental_sync('orders', timestamp_column='updated_at')\n",
        "\n",
        "    # Query híbrida (exemplo)\n",
        "    result = migration.hybrid_query(\"\"\"\n",
        "        SELECT\n",
        "            d.customer_id,\n",
        "            d.total_orders_delta,\n",
        "            p.customer_name,\n",
        "            p.email\n",
        "        FROM (\n",
        "            SELECT customer_id, COUNT(*) as total_orders_delta\n",
        "            FROM delta_scan('./data_lake/orders')\n",
        "            GROUP BY customer_id\n",
        "        ) d\n",
        "        JOIN pg.public.customers p ON d.customer_id = p.id\n",
        "        ORDER BY total_orders_delta DESC\n",
        "        LIMIT 10\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"\\nHybrid Query Result:\")\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 5\n",
        "# ✓ Use partition pruning\n",
        "# WHERE year = 2024 AND month = 1\n",
        "\n",
        "# ✓ Projection pushdown\n",
        "# SELECT id, name FROM delta_scan('./table')\n",
        "\n",
        "# ✓ Cache queries frequentes\n",
        "# @lru_cache(maxsize=128)\n",
        "# def get_metrics(date): ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 6\n",
        "# Para dados grandes: Spark write + DuckDB read\n",
        "# spark_df.write.format(\"delta\").save(\"./data\")\n",
        "# duckdb.execute(\"SELECT * FROM delta_scan('./data')\")\n",
        "\n",
        "# Para dados médios: DuckDB direto\n",
        "# write_deltalake(\"./data\", df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 7\n",
        "# ✓ Use secrets\n",
        "# con.execute(\"CREATE SECRET (TYPE S3, PROVIDER credential_chain)\")\n",
        "\n",
        "# ✓ Read-only quando possível\n",
        "# con = duckdb.connect('db.duckdb', read_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 8\n",
        "# ✓ Estrutura modular\n",
        "# class DataPipeline:\n",
        "#     def extract(self): ...\n",
        "#     def transform(self): ...\n",
        "#     def load(self): ...\n",
        "\n",
        "# ✓ Logging\n",
        "# import logging\n",
        "# logging.info(f\"Processed {count} rows\")\n",
        "\n",
        "# ✓ Testes\n",
        "# def test_pipeline():\n",
        "#     assert pipeline.run() == expected_result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
