{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 08 Particionamento Data Skipping\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_08_particionamento_data_skipping\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_08_particionamento_data_skipping\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "from deltalake import write_deltalake\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Criar DataFrame com dados para particionar\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        i as order_id,\n",
    "        'Customer ' || (i % 1000) as customer_name,\n",
    "        ['US', 'UK', 'BR', 'JP'][i % 4 + 1] as country,\n",
    "        CAST('2024-01-01' AS DATE) + (i % 365) * INTERVAL '1 day' as order_date,\n",
    "        RANDOM() * 1000 as amount\n",
    "    FROM range(0, 100000) tbl(i)\n",
    "\"\"\").df()\n",
    "\n",
    "# Adicionar colunas de partição explícitas\n",
    "df['year'] = df['order_date'].dt.year\n",
    "df['month'] = df['order_date'].dt.month\n",
    "\n",
    "# Escrever tabela particionada\n",
    "write_deltalake(\n",
    "    \"./sales_partitioned\",\n",
    "    df,\n",
    "    partition_by=[\"year\", \"month\"],\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print(\"✓ Partitioned table created!\")\n",
    "\n",
    "# Verificar estrutura\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"./sales_partitioned\"):\n",
    "    level = root.replace(\"./sales_partitioned\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "\n",
    "# Exemplo/Bloco 2\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError:\n",
    "    SparkSession = None\n",
    "    print('PySpark not installed, skipping Spark examples')\n",
    "from pyspark.sql.functions import col, year, month\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.range(0, 100000).selectExpr(\n",
    "    \"id as order_id\",\n",
    "    \"concat('Customer ', id % 1000) as customer_name\",\n",
    "    \"date_add('2024-01-01', cast(id % 365 as int)) as order_date\",\n",
    "    \"rand() * 1000 as amount\"\n",
    ")\n",
    "\n",
    "# Adicionar colunas de partição\n",
    "df = df.withColumn(\"year\", year(col(\"order_date\"))) \\\n",
    "       .withColumn(\"month\", month(col(\"order_date\")))\n",
    "\n",
    "# Escrever particionado\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"./sales_partitioned_spark\")\n",
    "\n",
    "# Exemplo/Bloco 3\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Habilitar profiling\n",
    "con.execute(\"SET enable_profiling=true\")\n",
    "con.execute(\"SET profiling_mode='detailed'\")\n",
    "\n",
    "# Query com partition filter\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM delta_scan('./sales_partitioned')\n",
    "    WHERE year = 2024 AND month = 1\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Result: {result[0]:,} rows\")\n",
    "\n",
    "# Ver quais arquivos foram lidos\n",
    "profile = con.execute(\"PRAGMA last_profiling_output\").fetchone()[0]\n",
    "print(\"\\nProfile:\")\n",
    "print(profile)\n",
    "\n",
    "# Buscar por \"files read\" ou \"partitions scanned\"\n",
    "\n",
    "# Exemplo/Bloco 4\n",
    "partition_by=[\"year\", \"month\"]\n",
    "   partition_by=[\"date\"]\n",
    "\n",
    "# Exemplo/Bloco 5\n",
    "partition_by=[\"region\"]  # 5-10 regiões\n",
    "   partition_by=[\"country\"]  # ~200 países\n",
    "\n",
    "# Exemplo/Bloco 6\n",
    "partition_by=[\"year\", \"month\", \"day\"]\n",
    "   partition_by=[\"country\", \"state\"]\n",
    "\n",
    "# Exemplo/Bloco 7\n",
    "partition_by=[\"customer_id\"]  # Milhões de valores\n",
    "   partition_by=[\"order_id\"]  # Cada valor é único\n",
    "\n",
    "# Exemplo/Bloco 8\n",
    "partition_by=[\"description\"]  # Nunca usado em WHERE\n",
    "\n",
    "# Exemplo/Bloco 9\n",
    "partition_by=[\"year\", \"month\", \"day\", \"hour\", \"minute\"]  # Muito granular\n",
    "\n",
    "# Exemplo/Bloco 10\n",
    "import duckdb\n",
    "\n",
    "def analyze_partitions(table_path: str):\n",
    "    \"\"\"\n",
    "    Analisar distribuição de tamanho das partições\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    # Não há função direta, mas podemos usar filesystem\n",
    "    import os\n",
    "\n",
    "    partition_sizes = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(table_path):\n",
    "        if files and root != table_path and '_delta_log' not in root:\n",
    "            # Calcular tamanho da partição\n",
    "            size = sum(\n",
    "                os.path.getsize(os.path.join(root, f))\n",
    "                for f in files\n",
    "                if f.endswith('.parquet')\n",
    "            )\n",
    "            partition = root.replace(table_path, '').strip('/')\n",
    "            partition_sizes[partition] = size\n",
    "\n",
    "    # Análise\n",
    "    total_size = sum(partition_sizes.values())\n",
    "    avg_size = total_size / len(partition_sizes) if partition_sizes else 0\n",
    "\n",
    "    print(f\"Total partitions: {len(partition_sizes)}\")\n",
    "    print(f\"Total size: {total_size / 1024**3:.2f} GB\")\n",
    "    print(f\"Average partition size: {avg_size / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Partições muito pequenas ou grandes\n",
    "    small = [p for p, s in partition_sizes.items() if s < 10*1024**2]  # < 10MB\n",
    "    large = [p for p, s in partition_sizes.items() if s > 10*1024**3]  # > 10GB\n",
    "\n",
    "    if small:\n",
    "        print(f\"\\n⚠ Warning: {len(small)} partitions < 10MB\")\n",
    "        print(\"Consider using coarser partitioning\")\n",
    "\n",
    "    if large:\n",
    "        print(f\"\\n⚠ Warning: {len(large)} partitions > 10GB\")\n",
    "        print(\"Consider using finer partitioning\")\n",
    "\n",
    "# Uso\n",
    "analyze_partitions(\"./sales_partitioned\")\n",
    "\n",
    "# Exemplo/Bloco 11\n",
    "import duckdb\n",
    "import time\n",
    "\n",
    "def measure_data_skipping(table_path: str, filter_clause: str):\n",
    "    \"\"\"\n",
    "    Medir efetividade de data skipping\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"SET enable_profiling=true\")\n",
    "\n",
    "    # Query com filtro\n",
    "    query = f\"\"\"\n",
    "        SELECT COUNT(*), AVG(amount)\n",
    "        FROM delta_scan('{table_path}')\n",
    "        WHERE {filter_clause}\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    result = con.execute(query).fetchone()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(f\"Filter: {filter_clause}\")\n",
    "    print(f\"Results: {result}\")\n",
    "    print(f\"Time: {elapsed:.3f}s\")\n",
    "\n",
    "    # Ver profile para entender quais arquivos foram lidos\n",
    "    # (DuckDB não expõe isso diretamente, mas pode inferir do tempo)\n",
    "\n",
    "    return elapsed\n",
    "\n",
    "# Comparar diferentes filtros\n",
    "print(\"=== FILTER EFFECTIVENESS ===\\n\")\n",
    "\n",
    "# Filtro seletivo (deve ser rápido)\n",
    "t1 = measure_data_skipping(\n",
    "    './sales_partitioned',\n",
    "    \"year = 2024 AND month = 1 AND amount > 9000\"\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Filtro menos seletivo (mais lento)\n",
    "t2 = measure_data_skipping(\n",
    "    './sales_partitioned',\n",
    "    \"amount > 100\"\n",
    ")\n",
    "\n",
    "print(f\"\\nSpeedup with selective filter: {t2/t1:.2f}x\")\n",
    "\n",
    "# Exemplo/Bloco 12\n",
    "import duckdb\n",
    "from deltalake import write_deltalake\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Ler tabela não-particionada\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT * FROM delta_scan('./sales_old')\n",
    "\"\"\").df()\n",
    "\n",
    "# Adicionar colunas de partição\n",
    "df['year'] = df['order_date'].dt.year\n",
    "df['month'] = df['order_date'].dt.month\n",
    "\n",
    "# Escrever nova tabela particionada\n",
    "write_deltalake(\n",
    "    \"./sales_new_partitioned\",\n",
    "    df,\n",
    "    partition_by=[\"year\", \"month\"],\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print(\"✓ Table repartitioned!\")\n",
    "\n",
    "# Exemplo/Bloco 13\n",
    "# Particionamento atual: muito granular (por dia)\n",
    "# Novo particionamento: por mês\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Ler tabela com partições diárias\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT * FROM delta_scan('./sales_daily_partitions')\n",
    "\"\"\").df()\n",
    "\n",
    "# Re-particionar por mês\n",
    "df['year'] = df['order_date'].dt.year\n",
    "df['month'] = df['order_date'].dt.month\n",
    "\n",
    "write_deltalake(\n",
    "    \"./sales_monthly_partitions\",\n",
    "    df,\n",
    "    partition_by=[\"year\", \"month\"],\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Exemplo/Bloco 14\n",
    "import duckdb\n",
    "from deltalake import write_deltalake\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def create_temporal_partitioned_table():\n",
    "    \"\"\"\n",
    "    Criar tabela otimizada para queries temporais\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    # Gerar dados para 2 anos\n",
    "    df = con.execute(\"\"\"\n",
    "        SELECT\n",
    "            i as event_id,\n",
    "            'Event-' || i as event_name,\n",
    "            CAST('2023-01-01' AS DATE) + (i % 730) * INTERVAL '1 day' as event_date,\n",
    "            ['web', 'mobile', 'api'][i % 3 + 1] as source,\n",
    "            ['US', 'UK', 'BR', 'JP', 'DE'][i % 5 + 1] as country,\n",
    "            RANDOM() * 100 as metric_value\n",
    "        FROM range(0, 1000000) tbl(i)\n",
    "    \"\"\").df()\n",
    "\n",
    "    # Adicionar colunas de partição hierárquicas\n",
    "    df['year'] = df['event_date'].dt.year\n",
    "    df['month'] = df['event_date'].dt.month\n",
    "\n",
    "    # Escrever particionado\n",
    "    write_deltalake(\n",
    "        \"./events_partitioned\",\n",
    "        df,\n",
    "        partition_by=[\"year\", \"month\"],\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "\n",
    "    print(\"✓ Temporal table created with 2 years of data\")\n",
    "    print(f\"  Total rows: {len(df):,}\")\n",
    "    print(f\"  Partitions: year × month = 24 partitions\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def benchmark_temporal_queries(table_path: str):\n",
    "    \"\"\"\n",
    "    Benchmark queries temporais\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    queries = [\n",
    "        (\"Single month\", \"year = 2024 AND month = 1\"),\n",
    "        (\"Quarter\", \"year = 2024 AND month BETWEEN 1 AND 3\"),\n",
    "        (\"Year\", \"year = 2024\"),\n",
    "        (\"Last 30 days\", \"event_date >= '2024-12-01'\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== TEMPORAL QUERY PERFORMANCE ===\")\n",
    "\n",
    "    for name, where_clause in queries:\n",
    "        import time\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT\n",
    "                COUNT(*) as events,\n",
    "                COUNT(DISTINCT country) as countries,\n",
    "                AVG(metric_value) as avg_metric\n",
    "            FROM delta_scan('{table_path}')\n",
    "            WHERE {where_clause}\n",
    "        \"\"\"\n",
    "\n",
    "        start = time.time()\n",
    "        result = con.execute(query).fetchone()\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Filter: {where_clause}\")\n",
    "        print(f\"  Events: {result[0]:,}\")\n",
    "        print(f\"  Time: {elapsed:.3f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Criar tabela\n",
    "    df = create_temporal_partitioned_table()\n",
    "\n",
    "    # Benchmark\n",
    "    benchmark_temporal_queries(\"./events_partitioned\")\n",
    "\n",
    "# Exemplo/Bloco 15\n",
    "# Nota: write support ainda não disponível no DuckDB\n",
    "# Deletion vectors são criados por Spark/Databricks\n",
    "# DuckDB lê corretamente tabelas com deletion vectors\n",
    "\n",
    "import duckdb\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def has_module(name):\n",
    "    return importlib.util.find_spec(name) is not None\n",
    "\n",
    "def safe_install_ext(con, ext_name):\n",
    "    try:\n",
    "        con.execute(f\"INSTALL {ext_name}\")\n",
    "        con.execute(f\"LOAD {ext_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to install/load {ext_name} extension: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Ler tabela com deletion vectors\n",
    "# (DuckDB automaticamente aplica deletion vectors)\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM delta_scan('./table_with_deletes')\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Rows (após aplicar deletion vectors): {result[0]:,}\")\n",
    "\n",
    "# Exemplo/Bloco 16\n",
    "# Análise de padrões de query\n",
    "common_filters = {\n",
    "    \"temporal\": 0.80,  # 80% queries filtram por data\n",
    "    \"region\": 0.40,    # 40% filtram por região\n",
    "    \"status\": 0.20     # 20% filtram por status\n",
    "}\n",
    "\n",
    "# Particionar pelas colunas mais filtradas\n",
    "# partition_by=[\"date\", \"region\"]\n",
    "\n",
    "# Exemplo/Bloco 17\n",
    "# ❌ Ruim: muitas partições pequenas\n",
    "partition_by=[\"year\", \"month\", \"day\", \"hour\"]\n",
    "\n",
    "# ✓ Bom: partições balanceadas\n",
    "partition_by=[\"year\", \"month\"]\n",
    "\n",
    "# Exemplo/Bloco 18\n",
    "# Script de monitoramento periódico\n",
    "def check_partition_health(table_path):\n",
    "    sizes = analyze_partition_sizes(table_path)\n",
    "\n",
    "    if any(s < 10*1024**2 for s in sizes):  # < 10MB\n",
    "        print(\"⚠ Consider consolidating small partitions\")\n",
    "\n",
    "    if any(s > 10*1024**3 for s in sizes):  # > 10GB\n",
    "        print(\"⚠ Consider splitting large partitions\")\n",
    "\n",
    "# Exemplo/Bloco 19\n",
    "# Via Databricks Delta Lake\n",
    "# (DuckDB não cria, mas lê eficientemente)\n",
    "# OPTIMIZE table ZORDER BY (column1, column2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}