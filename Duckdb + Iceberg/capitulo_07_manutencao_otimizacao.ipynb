{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6feb6aa2",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 07 - Manuten√ß√£o e Otimiza√ß√£o\n",
    "\n",
    "## üßπ Objetivo\n",
    "\n",
    "Tabelas Iceberg acumulam \"lixo\" (arquivos de dados antigos, snapshots expirados, metadados obsoletos) ao longo do tempo.\n",
    "Para manter a performance e reduzir custos de storage, precisamos de rotinas de manuten√ß√£o.\n",
    "\n",
    "Neste cap√≠tulo vamos explorar:\n",
    "1.  **Expire Snapshots**: Remover vers√µes antigas e liberar espa√ßo em disco.\n",
    "2.  **Rewrite Data Files (Compaction)**: Compactar muitos arquivos pequenos em poucos arquivos grandes.\n",
    "3.  **Remove Orphan Files**: Limpar arquivos que n√£o s√£o referenciados por nenhum snapshot.\n",
    "\n",
    "## üîß Requisitos\n",
    "\n",
    "- Configura√ß√£o autom√°tica inclu√≠da neste notebook.\n",
    "- Se a tabela `default.sales` n√£o existir, ser√° criada com dados de exemplo.\n",
    "- Se j√° existir, ser√° validada e complementada se necess√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe6ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tabela existente encontrada: default.sales\n",
      "üîç Verificando hist√≥rico de snapshots...\n",
      "‚ö†Ô∏è Hist√≥rico insuficiente. Gerando dados de teste...\n",
      "   -> Snapshot 1 criado.\n",
      "   -> Snapshot 2 criado.\n",
      "   -> Snapshot 3 criado.\n",
      "‚úÖ Ambiente pronto! Total de snapshots: 3\n",
      "Schema atual:\n",
      "table {\n",
      "  1: id: required long\n",
      "  2: created_at: required timestamp\n",
      "  3: product_name: optional string\n",
      "  4: amount: optional double\n",
      "  5: quantity: optional long\n",
      "  6: category: optional string\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pyiceberg\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import NestedField, StringType, LongType, DoubleType, TimestampType\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pyiceberg.exceptions import NoSuchNamespaceError\n",
    "\n",
    "# Configura√ß√£o Paths\n",
    "WAREHOUSE_PATH = './iceberg_warehouse'\n",
    "CATALOG_DB = f\"{WAREHOUSE_PATH}/catalog.db\"\n",
    "\n",
    "# Garantir diret√≥rio\n",
    "os.makedirs(WAREHOUSE_PATH, exist_ok=True)\n",
    "\n",
    "# Configurar cat√°logo\n",
    "catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"type\": \"sql\",\n",
    "        \"uri\": f\"sqlite:///{CATALOG_DB}\",\n",
    "        \"warehouse\": f\"file://{os.path.abspath(WAREHOUSE_PATH)}\",\n",
    "    }\n",
    ")\n",
    "\n",
    "def setup_environment(catalog):\n",
    "    \"\"\"Garante que a tabela e os dados necess√°rios existem para a aula.\"\"\"\n",
    "    table_name = \"default.sales\"\n",
    "    namespace = \"default\"\n",
    "\n",
    "    # Criar namespace se n√£o existir\n",
    "    try:\n",
    "        catalog.create_namespace(namespace)\n",
    "        print(f\"‚úÖ Namespace '{namespace}' criado.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # 1. Definir Schema\n",
    "    schema = Schema(\n",
    "        NestedField(1, \"id\", LongType(), required=True),\n",
    "        NestedField(2, \"created_at\", TimestampType(), required=True),\n",
    "        NestedField(3, \"product_name\", StringType(), required=False),\n",
    "        NestedField(4, \"amount\", DoubleType(), required=False),\n",
    "        NestedField(5, \"quantity\", LongType(), required=False),\n",
    "        NestedField(6, \"category\", StringType(), required=False)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            tbl = catalog.load_table(table_name)\n",
    "            print(f\"‚úÖ Tabela existente encontrada: {table_name}\")\n",
    "            \n",
    "            # Verificar se tem coluna category\n",
    "            fields = [f.name for f in tbl.schema().fields]\n",
    "            if \"category\" not in fields:\n",
    "                print(\"‚ö†Ô∏è Coluna 'category' ausente. Adicionando...\")\n",
    "                with tbl.update_schema() as update:\n",
    "                    update.add_column(\"category\", StringType())\n",
    "                print(\"‚úÖ Coluna 'category' adicionada.\")\n",
    "\n",
    "        except Exception:\n",
    "            print(f\"‚ö†Ô∏è Tabela n√£o encontrada. Criando {table_name}...\")\n",
    "            tbl = catalog.create_table(table_name, schema)\n",
    "            print(f\"‚úÖ Tabela criada.\")\n",
    "\n",
    "        # 2. Gerar Hist√≥rico (Snapshots)\n",
    "        print(\"üîç Verificando hist√≥rico de snapshots...\")\n",
    "        if len(tbl.snapshots()) < 3:\n",
    "            print(\"‚ö†Ô∏è Hist√≥rico insuficiente. Gerando dados de teste...\")\n",
    "            \n",
    "            # Schema expl√≠cito do PyArrow para garantir compatibilidade (campos required/non-null)\n",
    "            pa_schema = pa.schema([\n",
    "                ('id', pa.int64(), False), # False = Not Nullable\n",
    "                ('created_at', pa.timestamp('us'), False), # False = Not Nullable\n",
    "                ('product_name', pa.string()),\n",
    "                ('amount', pa.float64()),\n",
    "                ('quantity', pa.int64()),\n",
    "                ('category', pa.string())\n",
    "            ])\n",
    "            \n",
    "            for i in range(3):\n",
    "                df = pd.DataFrame({\n",
    "                    'id': range(i*10, i*10 + 5),\n",
    "                    'created_at': [datetime.now()] * 5,\n",
    "                    'product_name': [f'Product_{k}' for k in range(5)],\n",
    "                    'amount': [100.0 + k for k in range(5)],\n",
    "                    'quantity': [1] * 5,\n",
    "                    'category': ['Electronics' if k % 2 == 0 else 'Books' for k in range(5)]\n",
    "                })\n",
    "                df['created_at'] = df['created_at'].astype('datetime64[us]')\n",
    "                \n",
    "                # Usar schema expl√≠cito na convers√£o\n",
    "                pa_table = pa.Table.from_pandas(df, schema=pa_schema)\n",
    "                tbl.append(pa_table)\n",
    "                print(f\"   -> Snapshot {i+1} criado.\")\n",
    "                time.sleep(1) \n",
    "        \n",
    "        print(f\"‚úÖ Ambiente pronto! Total de snapshots: {len(tbl.snapshots())}\")\n",
    "        return tbl\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro fatal no setup: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Executar Setup\n",
    "tbl = setup_environment(catalog)\n",
    "print(f\"Schema atual:\\n{tbl.schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ebb12",
   "metadata": {},
   "source": [
    "## 1. Analisando Snapshots\n",
    "\n",
    "Cada opera√ß√£o de escrita (append, overwrite, update schema) cria um novo snapshot.\n",
    "Vamos listar o hist√≥rico atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcef7435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de snapshots: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "snapshot_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "operation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "records",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "16b3b4d7-46a7-4b48-97eb-c2b0eb1fb020",
       "rows": [
        [
         "0",
         "458005602930949974",
         "2026-01-22 18:50:40.787000",
         "Operation.APPEND",
         "5"
        ],
        [
         "1",
         "6407515911490158060",
         "2026-01-22 18:50:41.856000",
         "Operation.APPEND",
         "5"
        ],
        [
         "2",
         "3509319127170459358",
         "2026-01-22 18:50:42.969000",
         "Operation.APPEND",
         "5"
        ],
        [
         "3",
         "2007133789407081121",
         "2026-01-22 18:51:48.356000",
         "Operation.DELETE",
         null
        ],
        [
         "4",
         "8774728470651936363",
         "2026-01-22 18:51:48.443000",
         "Operation.APPEND",
         "15"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>operation</th>\n",
       "      <th>records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458005602930949974</td>\n",
       "      <td>2026-01-22 18:50:40.787</td>\n",
       "      <td>Operation.APPEND</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6407515911490158060</td>\n",
       "      <td>2026-01-22 18:50:41.856</td>\n",
       "      <td>Operation.APPEND</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3509319127170459358</td>\n",
       "      <td>2026-01-22 18:50:42.969</td>\n",
       "      <td>Operation.APPEND</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007133789407081121</td>\n",
       "      <td>2026-01-22 18:51:48.356</td>\n",
       "      <td>Operation.DELETE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8774728470651936363</td>\n",
       "      <td>2026-01-22 18:51:48.443</td>\n",
       "      <td>Operation.APPEND</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           snapshot_id               timestamp         operation records\n",
       "0   458005602930949974 2026-01-22 18:50:40.787  Operation.APPEND       5\n",
       "1  6407515911490158060 2026-01-22 18:50:41.856  Operation.APPEND       5\n",
       "2  3509319127170459358 2026-01-22 18:50:42.969  Operation.APPEND       5\n",
       "3  2007133789407081121 2026-01-22 18:51:48.356  Operation.DELETE    None\n",
       "4  8774728470651936363 2026-01-22 18:51:48.443  Operation.APPEND      15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def list_snapshots(table):\n",
    "    snapshots = table.snapshots()\n",
    "    data = []\n",
    "    for s in snapshots:\n",
    "        ts = datetime.fromtimestamp(s.timestamp_ms / 1000)\n",
    "        data.append({\n",
    "            \"snapshot_id\": s.snapshot_id,\n",
    "            \"timestamp\": ts,\n",
    "            \"operation\": s.summary.get(\"operation\", \"unknown\"),\n",
    "            \"records\": s.summary.get(\"added-records\", \"0\")\n",
    "        })\n",
    "    return pd.DataFrame(data).sort_values(\"timestamp\")\n",
    "\n",
    "df_snaps = list_snapshots(tbl)\n",
    "print(f\"Total de snapshots: {len(df_snaps)}\")\n",
    "df_snaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df9431",
   "metadata": {},
   "source": [
    "## 2. Expire Snapshots (Limpando o Hist√≥rico)\n",
    "\n",
    "Para liberar espa√ßo, removemos snapshots antigos que n√£o s√£o mais necess√°rios para Time Travel.\n",
    "Isso remove a refer√™ncia no metadata. (Obs: A remo√ß√£o f√≠sica dos arquivos √≥rf√£os √© um passo subsequente, geralmente feito via `remove_orphan_files`, mas aqui focaremos na limpeza do metadata).\n",
    "\n",
    "Vamos definir uma data de corte. Tudo anterior a essa data ser√° marcado para expira√ß√£o.\n",
    "\n",
    "> **Nota**: O PyIceberg executa isso atualizando o arquivo de metadados (`metadata.json`). A exclus√£o f√≠sica dos arquivos de dados antigos depende da implementa√ß√£o do Catalog/IO (em S3/Cloud geralmente funciona bem, localmente o suporte varia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentando expirar snapshots anteriores a: 2026-01-22 18:50:42.969000\n",
      "Opera√ß√£o n√£o suportada ou erro: 'Table' object has no attribute 'expire_snapshots'\n"
     ]
    }
   ],
   "source": [
    "### ‚ö†Ô∏è Nota sobre Expire Snapshots\n",
    "\n",
    "Atualmente, o **PyIceberg** fornece suporte robusto para Leitura, Escrita e Evolu√ß√£o de Metadados.\n",
    "Entretanto, a API de Manuten√ß√£o (`expire_snapshots`, `remove_orphan_files`) ainda est√° em desenvolvimento ativo.\n",
    "\n",
    "Em ambientes de produ√ß√£o integrados, essa limpeza costuma ser feita via procedimentos do **Spark** ou **Trino**:\n",
    "```sql\n",
    "-- Exemplo em Spark/Trino\n",
    "CALL catalog.system.expire_snapshots(table => 'default.sales', older_than => TIMESTAMP '...');\n",
    "```\n",
    "\n",
    "O PyIceberg permite gerenciar ramos e tags via `manage_snapshots()`, mas a limpeza final de metadata/arquivos ainda n√£o est√° exposta no objeto `Table`.\n",
    "Manteremos este t√≥pico como conceitual por enquanto.\n",
    "\n",
    "---\n",
    "Vamos focar no que conseguimos fazer localmente: **Otimizar a estrutura f√≠sica dos dados (Compaction/Rewrite)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170cf456",
   "metadata": {},
   "source": [
    "## 3. Otimiza√ß√£o via Particionamento\n",
    "\n",
    "Uma das maiores otimiza√ß√µes em Big Data √© o **Partition Pruning**.\n",
    "O Iceberg permite evoluir o esquema de particionamento (Partition Evolution) sem reescrever a tabela imediatamente.\n",
    "\n",
    "Vamos alterar a tabela para ser particionada por m√™s do campo `timestamp` (se existir) ou por `category`.\n",
    "Como adicionamos `category` no cap√≠tulo anterior, vamos usar esse campo se dispon√≠vel, ou `quantity` (bucket) se n√£o.\n",
    "Vou assumir que o campo `category` existe (criado no Cap 06). Caso n√£o exista (se o usu√°rio pulou), usamos outro.\n",
    "\n",
    "### Verificando Schema\n",
    "Vamos checar se temos `category` ou uma coluna de data `date_sale`. Temos `timestamp` no dataset original.\n",
    "No cap 06 adicionamos `category`.\n",
    "Vamos adicionar uma spec de parti√ß√£o: `identity(category)`.\n",
    "Isso vai separar fisicamente os arquivos por categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11cb0594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adicionando parti√ß√£o por 'category'...\n",
      "Spec de parti√ß√£o atualizada!\n",
      "Nova Spec:\n",
      "[\n",
      "  1000: category: identity(6)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Adicionando particionamento por 'category'\n",
    "    # Se 'category' for null, ficar√° numa pasta default.\n",
    "    # Nota: Dados antigos CONTINUAM n√£o particionados at√© serem reescritos.\n",
    "    \n",
    "    # Verificando se campo existe\n",
    "    schema_fields = [f.name for f in tbl.schema().fields]\n",
    "    if 'category' in schema_fields:\n",
    "        print(\"Adicionando parti√ß√£o por 'category'...\")\n",
    "        with tbl.update_spec() as update:\n",
    "            update.add_identity(\"category\")\n",
    "        print(\"Spec de parti√ß√£o atualizada!\")\n",
    "    else:\n",
    "        print(\"Campo 'category' n√£o encontrado. Adicione via Cap 06 ou use outro campo.\")\n",
    "        \n",
    "    print(f\"Nova Spec:\\n{tbl.spec()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao atualizar spec (talvez j√° particionado?): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc017e",
   "metadata": {},
   "source": [
    "## 4. Rewrite Data Files (Compacta√ß√£o e Reorganiza√ß√£o)\n",
    "\n",
    "Como ativamos o particionamento agora, os dados antigos **ainda est√£o na estrutura antiga** (sem pastas de parti√ß√£o).\n",
    "Para aplicar a nova estrutura e otimizar o tamanho dos arquivos (Compacta√ß√£o), precisamos reescrever os dados.\n",
    "\n",
    "Como o PyIceberg (ainda) n√£o possui um motor de compacta√ß√£o distribu√≠do nativo (como Spark), faremos uma **Compacta√ß√£o L√≥gica** via `overwrite`:\n",
    "1. Lemos todos os dados atuais.\n",
    "2. Reescrevemos para a tabela. O writer do PyIceberg detectar√° a nova `Partition Spec` e organizar√° os arquivos fisicamente.\n",
    "\n",
    "> **Cuidado**: Em produ√ß√£o com petabytes, use Spark/Trino procedures (`rewrite_data_files`). Para datasets que cabem na mem√≥ria/disco local, essa abordagem funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "453d67aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo 15 registros...\n",
      "Reescrevendo dados (Compacta√ß√£o + Particionamento)...\n",
      "Sucesso!\n"
     ]
    }
   ],
   "source": [
    "from pyiceberg.expressions import AlwaysTrue\n",
    "\n",
    "# 1. Ler dados atuais para mem√≥ria\n",
    "# (Em produ√ß√£o, far√≠amos isso em batches ou usar√≠amos Spark)\n",
    "arrow_data = tbl.scan().to_arrow()\n",
    "print(f\"Lendo {len(arrow_data)} registros...\")\n",
    "\n",
    "# 2. Executar Overwrite (Substitui√ß√£o Total)\n",
    "# Isso deleta logicamente os arquivos antigos e escreve novos j√° organizados pelas parti√ß√µes\n",
    "if len(arrow_data) > 0:\n",
    "    print(\"Reescrevendo dados (Compacta√ß√£o + Particionamento)...\")\n",
    "    tbl.overwrite(arrow_data, overwrite_filter=AlwaysTrue())\n",
    "    print(\"Sucesso!\")\n",
    "else:\n",
    "    print(\"Tabela vazia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd7271",
   "metadata": {},
   "source": [
    "## 5. Verifica√ß√£o da Estrutura F√≠sica\n",
    "\n",
    "Vamos verificar se os novos arquivos foram escritos nos diret√≥rios de parti√ß√£o (`category=...`).\n",
    "Faremos isso inspecionando o plano de scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8a1dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de arquivos de dados: 2\n",
      "--------------------------------------------------\n",
      "Arquivo 1: .../default/sales/data/category=Electronics/00000-0-bd56202a-7fca-428a-be21-b6004d20e794.parquet\n",
      "Arquivo 2: .../default/sales/data/category=Books/00000-1-bd56202a-7fca-428a-be21-b6004d20e794.parquet\n",
      "\n",
      "Se voc√™ v√™ pastas como 'category=...' no caminho, o Particionamento funcionou!\n",
      "Tipo de opera√ß√£o no √∫ltimo snapshot: Operation.APPEND\n"
     ]
    }
   ],
   "source": [
    "scan = tbl.scan()\n",
    "# plan_files() retorna um generator de FileScanTask\n",
    "tasks = list(scan.plan_files())\n",
    "\n",
    "print(f\"Total de arquivos de dados: {len(tasks)}\")\n",
    "print(\"-\" * 50)\n",
    "for i, task in enumerate(tasks):\n",
    "    # Pegando o caminho relativo para facilitar leitura\n",
    "    full_path = task.file.file_path\n",
    "    relative = full_path.split(\"iceberg_warehouse\")[-1] if \"iceberg_warehouse\" in full_path else full_path\n",
    "    print(f\"Arquivo {i+1}: ...{relative}\")\n",
    "    # Se particionamento funcionou, veremos 'category=Nome' no caminho\n",
    "\n",
    "# Conclus√£o\n",
    "print(\"\\nSe voc√™ v√™ pastas como 'category=...' no caminho, o Particionamento funcionou!\")\n",
    "print(f\"Tipo de opera√ß√£o no √∫ltimo snapshot: {tbl.current_snapshot().summary.get('operation')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
