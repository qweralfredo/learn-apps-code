{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 08 Particionamento Organizacao\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_08_particionamento_organizacao\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_08_particionamento_organizacao\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "# LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "# Query com filtro de data\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM iceberg_scan('s3://bucket/sales')\n",
    "    WHERE order_date >= '2024-01-01'\n",
    "      AND order_date < '2024-02-01'\n",
    "\"\"\").fetchone()\n",
    "\n",
    "# Iceberg lê apenas partições de janeiro 2024!\n",
    "# Partições de outros meses são ignoradas\n",
    "\n",
    "# Exemplo/Bloco 2\n",
    "import duckdb\n",
    "import time\n",
    "\n",
    "con = duckdb.connect()\n",
    "# LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "# Sem filtro de partição (lê tudo)\n",
    "start = time.time()\n",
    "count1 = con.execute(\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM iceberg_scan('s3://bucket/large_table')\n",
    "\"\"\").fetchone()[0]\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Com filtro de partição (lê só o necessário)\n",
    "start = time.time()\n",
    "count2 = con.execute(\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM iceberg_scan('s3://bucket/large_table')\n",
    "    WHERE event_date = '2024-01-15'\n",
    "\"\"\").fetchone()[0]\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"Sem filtro: {time1:.2f}s ({count1:,} linhas)\")\n",
    "print(f\"Com filtro: {time2:.2f}s ({count2:,} linhas)\")\n",
    "print(f\"Speedup: {time1/time2:.1f}x\")\n",
    "\n",
    "# Exemplo/Bloco 3\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "# LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "# Analisar distribuição de arquivos\n",
    "metadata = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        file_path,\n",
    "        record_count,\n",
    "        file_size_in_bytes / 1024 / 1024 as size_mb\n",
    "    FROM iceberg_metadata('s3://bucket/partitioned_table')\n",
    "    WHERE status = 'EXISTING'\n",
    "    ORDER BY file_path\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Distribuição de arquivos:\")\n",
    "print(metadata.groupby(\n",
    "    metadata['file_path'].str.extract(r'/(\\\\w+)=')[0]\n",
    ")['record_count'].agg(['count', 'sum', 'mean']))\n",
    "\n",
    "# Exemplo/Bloco 4\n",
    "import duckdb\n",
    "\n",
    "def find_small_partitions(table_path, threshold_mb=10):\n",
    "    \"\"\"Encontra partições pequenas que podem ser compactadas\"\"\"\n",
    "    con = duckdb.connect()\n",
    "    # LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "    small_files = con.execute(f\"\"\"\n",
    "        SELECT\n",
    "            file_path,\n",
    "            record_count,\n",
    "            file_size_in_bytes / 1024 / 1024 as size_mb\n",
    "        FROM iceberg_metadata('{table_path}')\n",
    "        WHERE status = 'EXISTING'\n",
    "          AND file_size_in_bytes / 1024 / 1024 < {threshold_mb}\n",
    "        ORDER BY size_mb\n",
    "    \"\"\").df()\n",
    "\n",
    "    return small_files\n",
    "\n",
    "# Usar\n",
    "small = find_small_partitions('s3://bucket/sales', threshold_mb=50)\n",
    "print(f\"Arquivos pequenos encontrados: {len(small)}\")\n",
    "\n",
    "# Exemplo/Bloco 5\n",
    "# Iceberg lê corretamente mesmo com múltiplos partition specs\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        date_trunc('month', order_date) as month,\n",
    "        count(*) as orders\n",
    "    FROM iceberg_scan('s3://bucket/evolved_table')\n",
    "    GROUP BY month\n",
    "    ORDER BY month\n",
    "\"\"\").df()\n",
    "\n",
    "# Funciona mesmo que:\n",
    "# - Dados de 2023 estejam particionados por dia\n",
    "# - Dados de 2024 estejam particionados por hora\n",
    "\n",
    "# Exemplo/Bloco 6\n",
    "import duckdb\n",
    "\n",
    "def analyze_partition_candidates(table_path):\n",
    "    \"\"\"Analisa quais colunas são boas candidatas para particionamento\"\"\"\n",
    "    con = duckdb.connect()\n",
    "    # LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "    # Pegar amostra\n",
    "    sample = con.execute(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM iceberg_scan('{table_path}')\n",
    "        USING SAMPLE 10%\n",
    "    \"\"\").df()\n",
    "\n",
    "    # Analisar cardinalidade de cada coluna\n",
    "    for col in sample.columns:\n",
    "        unique_count = sample[col].nunique()\n",
    "        total_count = len(sample)\n",
    "        cardinality_ratio = unique_count / total_count\n",
    "\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Valores únicos: {unique_count:,}\")\n",
    "        print(f\"  Ratio: {cardinality_ratio:.2%}\")\n",
    "\n",
    "        if 0.01 < cardinality_ratio < 0.5:\n",
    "            print(f\"  ✅ Boa candidata para particionamento\")\n",
    "        else:\n",
    "            print(f\"  ❌ Não recomendada\")\n",
    "        print()\n",
    "\n",
    "analyze_partition_candidates('s3://bucket/sales')\n",
    "\n",
    "# Exemplo/Bloco 7\n",
    "import duckdb\n",
    "\n",
    "def needs_compaction(table_path, ideal_size_mb=128):\n",
    "    \"\"\"Verifica se tabela precisa de compactação\"\"\"\n",
    "    con = duckdb.connect()\n",
    "    # LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "    stats = con.execute(f\"\"\"\n",
    "        SELECT\n",
    "            count(*) as file_count,\n",
    "            avg(file_size_in_bytes / 1024 / 1024) as avg_size_mb,\n",
    "            min(file_size_in_bytes / 1024 / 1024) as min_size_mb,\n",
    "            max(file_size_in_bytes / 1024 / 1024) as max_size_mb\n",
    "        FROM iceberg_metadata('{table_path}')\n",
    "        WHERE status = 'EXISTING'\n",
    "    \"\"\").fetchone()\n",
    "\n",
    "    file_count, avg_size, min_size, max_size = stats\n",
    "\n",
    "    print(f\"Estatísticas de arquivos:\")\n",
    "    print(f\"  Total: {file_count:,}\")\n",
    "    print(f\"  Tamanho médio: {avg_size:.2f} MB\")\n",
    "    print(f\"  Range: {min_size:.2f} - {max_size:.2f} MB\")\n",
    "\n",
    "    if avg_size < ideal_size_mb * 0.5:\n",
    "        print(f\"⚠️  Recomendado: Compactar arquivos\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"✅ Tamanho de arquivos OK\")\n",
    "        return False\n",
    "\n",
    "needs_compaction('s3://bucket/sales')\n",
    "\n",
    "# Exemplo/Bloco 8\n",
    "# Regra geral para particionamento por data:\n",
    "\n",
    "# Dados pequenos (< 1 GB/dia): Particionar por mês\n",
    "# Dados médios (1-100 GB/dia): Particionar por dia\n",
    "# Dados grandes (> 100 GB/dia): Particionar por hora\n",
    "\n",
    "# Evitar:\n",
    "# - Partições muito pequenas (< 100 MB)\n",
    "# - Partições muito grandes (> 1 GB)\n",
    "# - Muitas partições (> 10.000)\n",
    "\n",
    "# Exemplo/Bloco 9\n",
    "import duckdb\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def has_module(name):\n",
    "    return importlib.util.find_spec(name) is not None\n",
    "\n",
    "def safe_install_ext(con, ext_name):\n",
    "    try:\n",
    "        con.execute(f\"INSTALL {ext_name}\")\n",
    "        con.execute(f\"LOAD {ext_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to install/load {ext_name} extension: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "class IcebergPartitionMonitor:\n",
    "    def __init__(self, table_path):\n",
    "        self.table_path = table_path\n",
    "        self.con = duckdb.connect()\n",
    "        self.# LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "    def partition_stats(self):\n",
    "        \"\"\"Estatísticas por partição\"\"\"\n",
    "        return self.con.execute(f\"\"\"\n",
    "            SELECT\n",
    "                regexp_extract(file_path, '/(\\\\w+)=(\\\\w+)/', 1) as partition_key,\n",
    "                regexp_extract(file_path, '/(\\\\w+)=(\\\\w+)/', 2) as partition_value,\n",
    "                count(*) as file_count,\n",
    "                sum(record_count) as total_records,\n",
    "                sum(file_size_in_bytes) / 1024 / 1024 as total_mb\n",
    "            FROM iceberg_metadata('{self.table_path}')\n",
    "            WHERE status = 'EXISTING'\n",
    "            GROUP BY partition_key, partition_value\n",
    "            ORDER BY total_mb DESC\n",
    "        \"\"\").df()\n",
    "\n",
    "# Usar\n",
    "monitor = IcebergPartitionMonitor('s3://bucket/sales')\n",
    "stats = monitor.partition_stats()\n",
    "print(stats.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}