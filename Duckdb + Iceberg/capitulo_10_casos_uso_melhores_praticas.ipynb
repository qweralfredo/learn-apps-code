{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 10 Casos Uso Melhores Praticas\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_10_casos_uso_melhores_praticas\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_10_casos_uso_melhores_praticas\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "class LakehouseAnalytics:\n",
    "    def __init__(self, catalog_config):\n",
    "        self.con = duckdb.connect()\n",
    "        self.# LOAD iceberg handled by safe_install_ext\n",
    "        self.con.execute(\"LOAD httpfs\")\n",
    "\n",
    "        # Configurar catálogo\n",
    "        self.con.execute(f\"\"\"\n",
    "            CREATE SECRET cat_secret (\n",
    "                TYPE iceberg,\n",
    "                CLIENT_ID '{catalog_config['client_id']}',\n",
    "                CLIENT_SECRET '{catalog_config['client_secret']}',\n",
    "                OAUTH2_SERVER_URI '{catalog_config['oauth_uri']}'\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        self.con.execute(f\"\"\"\n",
    "            ATTACH '{catalog_config['warehouse']}' AS lakehouse (\n",
    "                TYPE iceberg,\n",
    "                SECRET cat_secret,\n",
    "                ENDPOINT '{catalog_config['endpoint']}'\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    def daily_sales_report(self, date):\n",
    "        \"\"\"Relatório diário de vendas\"\"\"\n",
    "        return self.con.execute(f\"\"\"\n",
    "            SELECT\n",
    "                product_category,\n",
    "                count(*) as total_orders,\n",
    "                sum(total_amount) as revenue,\n",
    "                avg(total_amount) as avg_order_value,\n",
    "                count(DISTINCT customer_id) as unique_customers\n",
    "            FROM lakehouse.sales.orders\n",
    "            WHERE order_date = '{date}'\n",
    "            GROUP BY product_category\n",
    "            ORDER BY revenue DESC\n",
    "        \"\"\").df()\n",
    "\n",
    "    def customer_segmentation(self, months_back=6):\n",
    "        \"\"\"Segmentação de clientes\"\"\"\n",
    "        return self.con.execute(f\"\"\"\n",
    "            WITH customer_stats AS (\n",
    "                SELECT\n",
    "                    customer_id,\n",
    "                    count(*) as order_count,\n",
    "                    sum(total_amount) as lifetime_value,\n",
    "                    max(order_date) as last_order_date,\n",
    "                    current_date - max(order_date) as days_since_last_order\n",
    "                FROM lakehouse.sales.orders\n",
    "                WHERE order_date >= current_date - INTERVAL '{months_back} months'\n",
    "                GROUP BY customer_id\n",
    "            )\n",
    "            SELECT\n",
    "                CASE\n",
    "                    WHEN lifetime_value > 10000 AND days_since_last_order < 30\n",
    "                        THEN 'VIP Active'\n",
    "                    WHEN lifetime_value > 10000 AND days_since_last_order >= 30\n",
    "                        THEN 'VIP At Risk'\n",
    "                    WHEN order_count > 5 AND days_since_last_order < 60\n",
    "                        THEN 'Regular'\n",
    "                    ELSE 'Occasional'\n",
    "                END as segment,\n",
    "                count(*) as customer_count,\n",
    "                avg(lifetime_value) as avg_ltv\n",
    "            FROM customer_stats\n",
    "            GROUP BY segment\n",
    "            ORDER BY avg_ltv DESC\n",
    "        \"\"\").df()\n",
    "\n",
    "# Usar\n",
    "analytics = LakehouseAnalytics(config)\n",
    "report = analytics.daily_sales_report('2024-01-15')\n",
    "print(report)\n",
    "\n",
    "# Exemplo/Bloco 2\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "\n",
    "class AuditedETLPipeline:\n",
    "    def __init__(self, source_table, target_table):\n",
    "        self.source = source_table\n",
    "        self.target = target_table\n",
    "        self.con = duckdb.connect()\n",
    "        self.# LOAD iceberg handled by safe_install_ext\n",
    "        self.con.execute(\"LOAD httpfs\")\n",
    "\n",
    "    def get_last_processed_snapshot(self):\n",
    "        \"\"\"Obtém ID do último snapshot processado\"\"\"\n",
    "        # Ler de tabela de controle ou arquivo\n",
    "        return None  # Implementar lógica de controle\n",
    "\n",
    "    def process_incremental(self):\n",
    "        \"\"\"Processa apenas dados novos\"\"\"\n",
    "        last_snapshot = self.get_last_processed_snapshot()\n",
    "\n",
    "        if last_snapshot:\n",
    "            # Ler apenas mudanças desde último snapshot\n",
    "            new_data = self.con.execute(f\"\"\"\n",
    "                SELECT * FROM iceberg_scan('{self.source}')\n",
    "                WHERE snapshot_id > {last_snapshot}\n",
    "            \"\"\").df()\n",
    "        else:\n",
    "            # Primeira execução - processar tudo\n",
    "            new_data = self.con.execute(f\"\"\"\n",
    "                SELECT * FROM iceberg_scan('{self.source}')\n",
    "            \"\"\").df()\n",
    "\n",
    "        # Processar e carregar\n",
    "        if len(new_data) > 0:\n",
    "            processed = self.transform(new_data)\n",
    "            self.load(processed)\n",
    "\n",
    "            # Registrar snapshot processado\n",
    "            current_snapshot = self.get_current_snapshot()\n",
    "            self.save_checkpoint(current_snapshot)\n",
    "\n",
    "        return len(new_data)\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Transformações de negócio\"\"\"\n",
    "        # Implementar lógica de transformação\n",
    "        return data\n",
    "\n",
    "    def load(self, data):\n",
    "        \"\"\"Carregar no destino\"\"\"\n",
    "        self.con.execute(f\"\"\"\n",
    "            INSERT INTO {self.target}\n",
    "            SELECT * FROM data\n",
    "        \"\"\")\n",
    "\n",
    "    def get_current_snapshot(self):\n",
    "        \"\"\"Obtém snapshot atual\"\"\"\n",
    "        result = self.con.execute(f\"\"\"\n",
    "            SELECT snapshot_id\n",
    "            FROM iceberg_snapshots('{self.source}')\n",
    "            ORDER BY sequence_number DESC\n",
    "            LIMIT 1\n",
    "        \"\"\").fetchone()\n",
    "        return result[0] if result else None\n",
    "\n",
    "    def save_checkpoint(self, snapshot_id):\n",
    "        \"\"\"Salva checkpoint para recuperação\"\"\"\n",
    "        # Implementar persistência do checkpoint\n",
    "        pass\n",
    "\n",
    "# Usar\n",
    "pipeline = AuditedETLPipeline(\n",
    "    source_table='s3://bucket/raw/events',\n",
    "    target_table='s3://bucket/processed/events'\n",
    ")\n",
    "records_processed = pipeline.process_incremental()\n",
    "print(f\"Processados: {records_processed:,} registros\")\n",
    "\n",
    "# Exemplo/Bloco 3\n",
    "import duckdb\n",
    "import streamlit as st\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RealtimeDashboard:\n",
    "    def __init__(self, table_path):\n",
    "        self.table = table_path\n",
    "        self.con = duckdb.connect()\n",
    "        self.# LOAD iceberg handled by safe_install_ext\n",
    "        self.con.execute(\"LOAD httpfs\")\n",
    "\n",
    "    def get_latest_metrics(self, hours_back=24):\n",
    "        \"\"\"Métricas das últimas N horas\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(hours=hours_back)\n",
    "\n",
    "        return self.con.execute(f\"\"\"\n",
    "            SELECT\n",
    "                date_trunc('hour', event_timestamp) as hour,\n",
    "                count(*) as total_events,\n",
    "                count(DISTINCT user_id) as active_users,\n",
    "                count(CASE WHEN event_type = 'purchase' THEN 1 END) as purchases,\n",
    "                sum(CASE WHEN event_type = 'purchase' THEN amount ELSE 0 END) as revenue\n",
    "            FROM iceberg_scan('{self.table}')\n",
    "            WHERE event_timestamp >= '{cutoff}'\n",
    "            GROUP BY hour\n",
    "            ORDER BY hour DESC\n",
    "        \"\"\").df()\n",
    "\n",
    "    def get_top_products(self, limit=10):\n",
    "        \"\"\"Produtos mais vendidos hoje\"\"\"\n",
    "        return self.con.execute(f\"\"\"\n",
    "            SELECT\n",
    "                product_name,\n",
    "                count(*) as sales_count,\n",
    "                sum(amount) as total_revenue\n",
    "            FROM iceberg_scan('{self.table}')\n",
    "            WHERE event_type = 'purchase'\n",
    "              AND event_date = current_date\n",
    "            GROUP BY product_name\n",
    "            ORDER BY sales_count DESC\n",
    "            LIMIT {limit}\n",
    "        \"\"\").df()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renderiza dashboard (exemplo com Streamlit)\"\"\"\n",
    "        st.title('Real-time Analytics Dashboard')\n",
    "\n",
    "        # Métricas\n",
    "        metrics = self.get_latest_metrics(hours_back=24)\n",
    "\n",
    "        # KPIs\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        col1.metric(\"Total Events\", f\"{metrics['total_events'].sum():,}\")\n",
    "        col2.metric(\"Active Users\", f\"{metrics['active_users'].sum():,}\")\n",
    "        col3.metric(\"Revenue\", f\"${metrics['revenue'].sum():,.2f}\")\n",
    "\n",
    "        # Gráfico\n",
    "        st.line_chart(metrics.set_index('hour')[['total_events', 'purchases']])\n",
    "\n",
    "        # Top produtos\n",
    "        st.subheader('Top 10 Products Today')\n",
    "        top_products = self.get_top_products()\n",
    "        st.dataframe(top_products)\n",
    "\n",
    "# Usar\n",
    "# dashboard = RealtimeDashboard('s3://bucket/events')\n",
    "# dashboard.render()\n",
    "\n",
    "# Exemplo/Bloco 4\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "\n",
    "class DataQualityMonitor:\n",
    "    def __init__(self, table_path):\n",
    "        self.table = table_path\n",
    "        self.con = duckdb.connect()\n",
    "        self.# LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "    def check_null_rates(self):\n",
    "        \"\"\"Verifica percentual de NULLs por coluna\"\"\"\n",
    "        # Obter colunas\n",
    "        columns = self.con.execute(f\"\"\"\n",
    "            DESCRIBE SELECT * FROM iceberg_scan('{self.table}') LIMIT 1\n",
    "        \"\"\").df()['column_name'].tolist()\n",
    "\n",
    "        # Calcular null rates\n",
    "        null_checks = []\n",
    "        for col in columns:\n",
    "            null_rate = self.con.execute(f\"\"\"\n",
    "                SELECT\n",
    "                    '{col}' as column_name,\n",
    "                    count(*) as total_rows,\n",
    "                    count({col}) as non_null_rows,\n",
    "                    (count(*) - count({col}))::FLOAT / count(*) * 100 as null_pct\n",
    "                FROM iceberg_scan('{self.table}')\n",
    "            \"\"\").fetchone()\n",
    "            null_checks.append(null_rate)\n",
    "\n",
    "        return pd.DataFrame(null_checks, columns=['column', 'total', 'non_null', 'null_pct'])\n",
    "\n",
    "    def check_duplicates(self, key_columns):\n",
    "        \"\"\"Verifica duplicatas\"\"\"\n",
    "        key_list = ', '.join(key_columns)\n",
    "\n",
    "        return self.con.execute(f\"\"\"\n",
    "            SELECT\n",
    "                count(*) as total_rows,\n",
    "                count(DISTINCT ({key_list})) as unique_keys,\n",
    "                count(*) - count(DISTINCT ({key_list})) as duplicate_count\n",
    "            FROM iceberg_scan('{self.table}')\n",
    "        \"\"\").fetchone()\n",
    "\n",
    "    def check_freshness(self, timestamp_column, max_delay_hours=24):\n",
    "        \"\"\"Verifica frescor dos dados\"\"\"\n",
    "        return self.con.execute(f\"\"\"\n",
    "            SELECT\n",
    "                max({timestamp_column}) as latest_timestamp,\n",
    "                current_timestamp - max({timestamp_column}) as data_age,\n",
    "                CASE\n",
    "                    WHEN current_timestamp - max({timestamp_column}) > INTERVAL '{max_delay_hours} hours'\n",
    "                    THEN 'STALE'\n",
    "                    ELSE 'FRESH'\n",
    "                END as status\n",
    "            FROM iceberg_scan('{self.table}')\n",
    "        \"\"\").fetchone()\n",
    "\n",
    "    def run_all_checks(self):\n",
    "        \"\"\"Executa todas as verificações\"\"\"\n",
    "        print(f\"=== Data Quality Report ===\")\n",
    "        print(f\"Table: {self.table}\")\n",
    "        print(f\"Time: {datetime.now()}\")\n",
    "        print()\n",
    "\n",
    "        # Null rates\n",
    "        print(\"Null Rates:\")\n",
    "        nulls = self.check_null_rates()\n",
    "        print(nulls[nulls['null_pct'] > 5])  # Mostrar colunas com >5% nulls\n",
    "        print()\n",
    "\n",
    "        # Duplicates\n",
    "        print(\"Duplicates:\")\n",
    "        dups = self.check_duplicates(['id'])\n",
    "        print(f\"Total: {dups[0]:,}, Unique: {dups[1]:,}, Duplicates: {dups[2]:,}\")\n",
    "        print()\n",
    "\n",
    "        # Freshness\n",
    "        print(\"Data Freshness:\")\n",
    "        fresh = self.check_freshness('event_timestamp')\n",
    "        print(f\"Latest: {fresh[0]}, Age: {fresh[1]}, Status: {fresh[2]}\")\n",
    "\n",
    "# Usar\n",
    "monitor = DataQualityMonitor('s3://bucket/events')\n",
    "monitor.run_all_checks()\n",
    "\n",
    "# Exemplo/Bloco 5\n",
    "# ✅ BOM: Nomes descritivos e consistentes\n",
    "catalog.sales.daily_orders\n",
    "catalog.sales.monthly_revenue_summary\n",
    "catalog.marketing.customer_segments\n",
    "\n",
    "# ❌ RUIM: Nomes genéricos\n",
    "catalog.default.table1\n",
    "catalog.default.data\n",
    "catalog.default.temp\n",
    "\n",
    "# Exemplo/Bloco 6\n",
    "# ✅ BOM: Schema bem definido com tipos apropriados\n",
    "CREATE TABLE catalog.sales.orders (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    order_date DATE,\n",
    "    order_timestamp TIMESTAMP,\n",
    "    total_amount DECIMAL(10, 2),\n",
    "    status VARCHAR,\n",
    "    items STRUCT(\n",
    "        product_id BIGINT,\n",
    "        quantity INTEGER,\n",
    "        price DECIMAL(10, 2)\n",
    "    )[]\n",
    ");\n",
    "\n",
    "# ❌ RUIM: Tudo como VARCHAR\n",
    "CREATE TABLE catalog.sales.orders (\n",
    "    order_id VARCHAR,\n",
    "    customer_id VARCHAR,\n",
    "    order_date VARCHAR,\n",
    "    total_amount VARCHAR,\n",
    "    status VARCHAR\n",
    ");\n",
    "\n",
    "# Exemplo/Bloco 7\n",
    "# ✅ BOM: Particionar por coluna frequentemente filtrada\n",
    "# Com cardinalidade apropriada\n",
    "PARTITIONED BY (date_trunc('day', order_date))\n",
    "\n",
    "# ✅ BOM: Partition evolution\n",
    "# Ano 1: day\n",
    "# Ano 2: hour (dados cresceram)\n",
    "\n",
    "# ❌ RUIM: Particionar por coluna única (ID)\n",
    "PARTITIONED BY (customer_id)  # Alta cardinalidade!\n",
    "\n",
    "# ❌ RUIM: Não particionar tabela grande\n",
    "# (tabela de 1 TB sem partições)\n",
    "\n",
    "# Exemplo/Bloco 8\n",
    "# ✅ BOM: Filtros em colunas de partição\n",
    "SELECT * FROM orders\n",
    "WHERE order_date >= '2024-01-01'\n",
    "  AND order_date < '2024-02-01';\n",
    "\n",
    "# ✅ BOM: Projection pushdown\n",
    "SELECT order_id, total_amount FROM orders;\n",
    "\n",
    "# ❌ RUIM: Sem filtros de partição\n",
    "SELECT * FROM orders\n",
    "WHERE customer_name LIKE 'John%';\n",
    "\n",
    "# ❌ RUIM: SELECT * desnecessário\n",
    "SELECT * FROM orders;\n",
    "\n",
    "# Exemplo/Bloco 9\n",
    "import duckdb\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RobustIcebergReader:\n",
    "    def __init__(self, table_path, max_retries=3):\n",
    "        self.table = table_path\n",
    "        self.max_retries = max_retries\n",
    "        self.con = duckdb.connect()\n",
    "        self.# LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "    def read_with_retry(self, query):\n",
    "        \"\"\"Lê com retry em caso de falha\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                result = self.con.execute(query).df()\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Tentativa {attempt + 1} falhou: {e}\")\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    logger.error(f\"Todas as tentativas falharam\")\n",
    "                    raise\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "    def safe_read(self, filter_date=None):\n",
    "        \"\"\"Leitura com tratamento de erro completo\"\"\"\n",
    "        try:\n",
    "            query = f\"SELECT * FROM iceberg_scan('{self.table}')\"\n",
    "            if filter_date:\n",
    "                query += f\" WHERE event_date = '{filter_date}'\"\n",
    "\n",
    "            return self.read_with_retry(query)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao ler tabela: {e}\")\n",
    "            # Retornar DataFrame vazio ou propagar erro\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Exemplo/Bloco 10\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def has_module(name):\n",
    "    return importlib.util.find_spec(name) is not None\n",
    "\n",
    "def safe_install_ext(con, ext_name):\n",
    "    try:\n",
    "        con.execute(f\"INSTALL {ext_name}\")\n",
    "        con.execute(f\"LOAD {ext_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to install/load {ext_name} extension: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "class IcebergMonitoring:\n",
    "    def __init__(self, table_path):\n",
    "        self.table = table_path\n",
    "        self.con = duckdb.connect()\n",
    "        self.# LOAD iceberg handled by safe_install_ext\n",
    "\n",
    "    def collect_metrics(self):\n",
    "        \"\"\"Coleta métricas da tabela\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # File stats\n",
    "        file_stats = self.con.execute(f\"\"\"\n",
    "            SELECT\n",
    "                count(*) as file_count,\n",
    "                sum(record_count) as total_records,\n",
    "                sum(file_size_in_bytes) / 1024 / 1024 / 1024 as size_gb,\n",
    "                avg(file_size_in_bytes) / 1024 / 1024 as avg_file_mb\n",
    "            FROM iceberg_metadata('{self.table}')\n",
    "            WHERE status = 'EXISTING'\n",
    "        \"\"\").fetchone()\n",
    "\n",
    "        metrics['files'] = {\n",
    "            'count': file_stats[0],\n",
    "            'total_records': file_stats[1],\n",
    "            'size_gb': round(file_stats[2], 2),\n",
    "            'avg_file_mb': round(file_stats[3], 2)\n",
    "        }\n",
    "\n",
    "        # Snapshot stats\n",
    "        snapshot_count = self.con.execute(f\"\"\"\n",
    "            SELECT count(*) FROM iceberg_snapshots('{self.table}')\n",
    "        \"\"\").fetchone()[0]\n",
    "\n",
    "        metrics['snapshots'] = {\n",
    "            'count': snapshot_count\n",
    "        }\n",
    "\n",
    "        # Timestamp\n",
    "        metrics['collected_at'] = datetime.now().isoformat()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def save_metrics(self, output_file):\n",
    "        \"\"\"Salva métricas em arquivo\"\"\"\n",
    "        metrics = self.collect_metrics()\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        print(f\"Métricas salvas em {output_file}\")\n",
    "\n",
    "# Usar\n",
    "monitor = IcebergMonitoring('s3://bucket/sales')\n",
    "monitor.save_metrics('metrics.json')\n",
    "\n",
    "# Exemplo/Bloco 11\n",
    "# Diagnóstico\n",
    "1. Use EXPLAIN ANALYZE para identificar gargalo\n",
    "2. Verifique se filtros de partição estão sendo usados\n",
    "3. Confirme projection pushdown\n",
    "4. Monitore I/O de rede (para cloud storage)\n",
    "5. Aumente threads se I/O-bound\n",
    "\n",
    "# Soluções\n",
    "- Adicionar filtros de partição\n",
    "- Especificar colunas (não SELECT *)\n",
    "- Aumentar threads para I/O paralelo\n",
    "- Considerar compactação de small files\n",
    "\n",
    "# Exemplo/Bloco 12\n",
    "# Soluções\n",
    "1. Aumentar memory_limit\n",
    "2. Processar em batches\n",
    "3. Usar temp_directory para spilling\n",
    "4. Reduzir threads (paradoxalmente pode ajudar)\n",
    "5. Otimizar query para usar menos memória\n",
    "\n",
    "# Exemplo/Bloco 13\n",
    "# Prevenção\n",
    "1. Usar ACID guarantees do Iceberg\n",
    "2. Evitar múltiplos writers simultâneos sem coordenação\n",
    "3. Usar catálogos com lock management\n",
    "4. Implementar retry logic com exponential backoff\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}