{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010e093",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 08 - Integra√ß√£o com Pandas e Polars\n",
    "\n",
    "## üêºüêª‚Äç‚ùÑÔ∏è Objetivo\n",
    "\n",
    "O ecossistema Python para dados vai muito al√©m do DuckDB. **Pandas** √© onipresente e **Polars** vem ganhando tra√ß√£o pela performance (escrito em Rust, assim como o Arrow).\n",
    "\n",
    "Neste cap√≠tulo, vamos explorar como o **Apache Iceberg** serve como camada de armazenamento unificada, permitindo que diferentes engines (DuckDB, Pandas, Polars) consumam e produzam dados na mesma tabela sem conflitos.\n",
    "\n",
    "### T√≥picos\n",
    "1.  **Pandas**: Leitura e Escrita via PyArrow.\n",
    "2.  **Polars**: Leitura \"Zero-Copy\" (via Arrow) e Escrita.\n",
    "3.  **Benchmark R√°pido**: Comparando tempos de leitura.\n",
    "\n",
    "## üîß Requisitos\n",
    "\n",
    "- PyIceberg, Pandas, Polars, DuckDB instalados.\n",
    "- Tabela `default.sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyiceberg\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import NestedField, StringType, LongType, DoubleType, TimestampType\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configura√ß√£o Paths\n",
    "WAREHOUSE_PATH = './iceberg_warehouse'\n",
    "CATALOG_DB = f\"{WAREHOUSE_PATH}/catalog.db\"\n",
    "os.makedirs(WAREHOUSE_PATH, exist_ok=True)\n",
    "\n",
    "# Configurar cat√°logo\n",
    "catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"type\": \"sql\",\n",
    "        \"uri\": f\"sqlite:///{CATALOG_DB}\",\n",
    "        \"warehouse\": f\"file://{os.path.abspath(WAREHOUSE_PATH)}\",\n",
    "    }\n",
    ")\n",
    "\n",
    "def setup_environment(catalog):\n",
    "    \"\"\"Garante ambiente pronto (tabela sales com dados).\"\"\"\n",
    "    table_name = \"default.sales\"\n",
    "    namespace = \"default\"\n",
    "    \n",
    "    try:\n",
    "        catalog.create_namespace(namespace)\n",
    "    except:\n",
    "        pass # Namespace j√° existe\n",
    "\n",
    "    schema = Schema(\n",
    "        NestedField(1, \"id\", LongType(), required=True),\n",
    "        NestedField(2, \"created_at\", TimestampType(), required=True),\n",
    "        NestedField(3, \"product_name\", StringType(), required=False),\n",
    "        NestedField(4, \"amount\", DoubleType(), required=False),\n",
    "        NestedField(5, \"quantity\", LongType(), required=False),\n",
    "        NestedField(6, \"category\", StringType(), required=False)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            tbl = catalog.load_table(table_name)\n",
    "            print(f\"‚úÖ Tabela encontrada: {table_name}\")\n",
    "            # Garantir schema atualizado\n",
    "            if \"category\" not in [f.name for f in tbl.schema().fields]:\n",
    "                with tbl.update_schema() as update:\n",
    "                    update.add_column(\"category\", StringType())\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Criando tabela {table_name}...\")\n",
    "            tbl = catalog.create_table(table_name, schema)\n",
    "\n",
    "        if len(tbl.snapshots()) < 1:\n",
    "            print(\"‚ö†Ô∏è Populando tabela vazia...\")\n",
    "            # Gerar dados iniciais\n",
    "            df = pd.DataFrame({'id': [1,2], 'created_at': [datetime.now()]*2, 'product_name': ['A','B'], 'amount': [10.0, 20.0], 'quantity': [1,1], 'category': ['X','Y']})\n",
    "            df['created_at'] = df['created_at'].astype('datetime64[us]')\n",
    "            pa_schema = pa.schema([('id', pa.int64(), False), ('created_at', pa.timestamp('us'), False), ('product_name', pa.string()), ('amount', pa.float64()), ('quantity', pa.int64()), ('category', pa.string())])\n",
    "            tbl.append(pa.Table.from_pandas(df, schema=pa_schema))\n",
    "        \n",
    "        return tbl\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro Setup: {e}\")\n",
    "        raise\n",
    "\n",
    "# Init\n",
    "tbl = setup_environment(catalog)\n",
    "print(f\"Snapshots: {len(tbl.snapshots())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2c673",
   "metadata": {},
   "source": [
    "## 1. Integra√ß√£o com Pandas\n",
    "\n",
    "Pandas √© o padr√£o da ind√∫stria para manipula√ß√£o de dataframes small/medium data.\n",
    "A integra√ß√£o com Iceberg ocorre via **PyArrow**.\n",
    "\n",
    "### Escrita (Pandas -> Iceberg)\n",
    "J√° usamos isso nos cap√≠tulos anteriores: `df -> pa.Table -> tbl.append()`.\n",
    "\n",
    "### Leitura (Iceberg -> Pandas)\n",
    "O m√©todo `.scan().to_pandas()` facilita a convers√£o. Note que para grandes volumes, isso pode estourar a mem√≥ria RAM. Para Big Data, use DuckDB ou Polars (Streaming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0becd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Escrita com Pandas ---\n",
    "print(\"üìù Escrevendo com Pandas...\")\n",
    "df_pandas = pd.DataFrame({\n",
    "    'id': [1001, 1002, 1003],\n",
    "    'created_at': [datetime.now()] * 3,\n",
    "    'product_name': ['Pandas_Toy', 'NumPy_Plush', 'Jupyter_Sticker'],\n",
    "    'amount': [50.5, 75.0, 5.0],\n",
    "    'quantity': [10, 5, 100],\n",
    "    'category': ['Swag', 'Toy', 'Stationery']\n",
    "})\n",
    "# Ajuste fino de tipos para casar com Iceberg (US timestamp)\n",
    "df_pandas['created_at'] = df_pandas['created_at'].astype('datetime64[us]')\n",
    "\n",
    "# Schema Enforcement\n",
    "# √â boa pr√°tica definir o schema PyArrow explicitamente para evitar erros de infer√™ncia (ex: double vs float, nullable)\n",
    "pa_schema = pa.schema([\n",
    "    ('id', pa.int64(), False),\n",
    "    ('created_at', pa.timestamp('us'), False),\n",
    "    ('product_name', pa.string()),\n",
    "    ('amount', pa.float64()),\n",
    "    ('quantity', pa.int64()),\n",
    "    ('category', pa.string())\n",
    "])\n",
    "\n",
    "# Append\n",
    "tbl.append(pa.Table.from_pandas(df_pandas, schema=pa_schema))\n",
    "print(\"‚úÖ Dados Pandas inseridos!\")\n",
    "\n",
    "# --- Leitura com Pandas ---\n",
    "print(\"\\nüìñ Lendo com Pandas (scan total)...\")\n",
    "start_time = time.time()\n",
    "df_read_pandas = tbl.scan().to_pandas()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Registros lidos: {len(df_read_pandas)}\")\n",
    "print(f\"Tempo Pandas: {end_time - start_time:.4f}s\")\n",
    "df_read_pandas.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6979a29",
   "metadata": {},
   "source": [
    "## 2. Integra√ß√£o com Polars\n",
    "\n",
    "**Polars** √© uma biblioteca de DataFrames moderna, escrita em Rust, focada em performance e processamento paralelo.\n",
    "Ela \"fala a l√≠ngua\" do Arrow nativamente, permitindo convers√µes **Zero-Copy** (quase instant√¢neas) de/para PyArrow.\n",
    "\n",
    "### Leitura (Iceberg -> Polars)\n",
    "Podemos converter diretamente de Arrow: `tbl.scan().to_arrow() -> pl.from_arrow()`.\n",
    "Se o PyIceberg tiver suporte nativo recente, `tbl.scan().to_polars()` pode funcionar tamb√©m. Vamos testar via Arrow para garantir compatibilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Escrita com Polars ---\n",
    "print(\"üìù Escrevendo com Polars...\")\n",
    "df_polars = pl.DataFrame({\n",
    "    'id': [2001, 2002],\n",
    "    'created_at': [datetime.now(), datetime.now()],\n",
    "    'product_name': ['Rust_Crab', 'Polars_Bear'],\n",
    "    'amount': [0.0, 999.9],\n",
    "    'quantity': [1, 1],\n",
    "    'category': ['Mascots', 'Animals']\n",
    "})\n",
    "\n",
    "# Polars -> Arrow -> Iceberg\n",
    "# Note como a convers√£o √© transparente\n",
    "pa_table_polars = df_polars.to_arrow()\n",
    "\n",
    "# Cast de tipos se necess√°rio (Polars usa nanosegundos por padr√£o, Iceberg requer micro)\n",
    "# Mas o PyArrow Table permite cast f√°cil. Vamos garantir schema.\n",
    "pa_table_polars = pa_table_polars.cast(pa_schema)\n",
    "\n",
    "tbl.append(pa_table_polars)\n",
    "print(\"‚úÖ Dados Polars inseridos!\")\n",
    "\n",
    "# --- Leitura com Polars ---\n",
    "print(\"\\nüìñ Lendo com Polars (via Arrow)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Verificar se existe .to_polars() nativo no scan (Feature recente do PyIceberg 0.7+)\n",
    "try:\n",
    "    df_read_polars = tbl.scan().to_polars()\n",
    "    print(\"üöÄ Usado .to_polars() nativo do PyIceberg!\")\n",
    "except AttributeError:\n",
    "    print(\"‚öôÔ∏è Usando fallback: .to_arrow() -> pl.from_arrow()\")\n",
    "    arrow_table = tbl.scan().to_arrow()\n",
    "    df_read_polars = pl.from_arrow(arrow_table)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Registros lidos: {len(df_read_polars)}\")\n",
    "print(f\"Tempo Polars: {end_time - start_time:.4f}s\")\n",
    "print(df_read_polars.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411322e0",
   "metadata": {},
   "source": [
    "## 3. Benchmark Simples: DuckDB vs Pandas vs Polars\n",
    "\n",
    "Apenas como curiosidade, vamos comparar o tempo de leitura dos 3 m√©todos na nossa tabela local.\n",
    "(Com poucos dados a diferen√ßa √© irrelevante, mas a sintaxe importa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ff7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Benchmark de Leitura (Full Scan) ---\")\n",
    "\n",
    "# 1. Pandas\n",
    "t0 = time.time()\n",
    "r_pd = tbl.scan().to_pandas()\n",
    "t_pd = time.time() - t0\n",
    "print(f\"Pandas Time: {t_pd:.5f}s (Rows: {len(r_pd)})\")\n",
    "\n",
    "# 2. Polars\n",
    "t0 = time.time()\n",
    "try:\n",
    "    r_pl = tbl.scan().to_polars()\n",
    "except:\n",
    "    r_pl = pl.from_arrow(tbl.scan().to_arrow())\n",
    "t_pl = time.time() - t0\n",
    "print(f\"Polars Time: {t_pl:.5f}s (Rows: {len(r_pl)})\")\n",
    "\n",
    "# 3. DuckDB (Lendo Arrow)\n",
    "t0 = time.time()\n",
    "arrow_tbl = tbl.scan().to_arrow()\n",
    "r_duck = duckdb.arrow(arrow_tbl).fetchall()\n",
    "t_duck = time.time() - t0\n",
    "print(f\"DuckDB Time: {t_duck:.5f}s (Rows: {len(r_duck)})\")\n",
    "\n",
    "print(\"\\nConclus√£o: Polars e DuckDB tendem a ser mais eficientes em datasets maiores devido ao manuseio zero-copy do Arrow.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
