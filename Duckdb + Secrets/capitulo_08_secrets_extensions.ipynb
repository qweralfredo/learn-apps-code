{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 08 Secrets Extensions\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_08_secrets_extensions\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_08_secrets_extensions\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Extensions que Suportam Secrets:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "Extension          Secret Types        Descrição\n",
    "────────────────────────────────────────────────────────\n",
    "httpfs             s3, r2, gcs, http   Cloud storage, HTTP APIs\n",
    "azure              azure               Azure Blob Storage\n",
    "mysql_scanner      mysql               MySQL database access\n",
    "postgres_scanner   postgres            PostgreSQL database access\n",
    "iceberg            iceberg             Apache Iceberg catalogs\n",
    "\n",
    "Fluxo de Trabalho:\n",
    "──────────────────\n",
    "\n",
    "1. INSTALL extension\n",
    "2. LOAD extension\n",
    "3. CREATE SECRET\n",
    "4. Use recursos que dependem do extension + secret\n",
    "\n",
    "Nota: Secrets só funcionam se extension estiver loaded!\n",
    "\"\"\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Verificar extensions instaladas\n",
    "try:\n",
    "    extensions = con.execute(\"\"\"\n",
    "        SELECT extension_name, installed, loaded\n",
    "        FROM duckdb_extensions()\n",
    "        WHERE extension_name IN ('httpfs', 'azure', 'mysql_scanner', 'postgres_scanner')\n",
    "    \"\"\").df()\n",
    "\n",
    "    print(\"\\nStatus de Extensions:\")\n",
    "    print(extensions)\n",
    "except:\n",
    "    print(\"\\nNota: Tabela duckdb_extensions() pode não estar disponível em todas as versões\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 2\n",
    "import duckdb\n",
    "\n",
    "def setup_extension_with_secret(extension_name, secret_config):\n",
    "    \"\"\"\n",
    "    Setup completo: install extension + create secret\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Setup: {extension_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # 1. Install\n",
    "    print(f\"1. Installing {extension_name}...\")\n",
    "    con.execute(f\"INSTALL {extension_name}\")\n",
    "    print(f\"   ✓ Installed\")\n",
    "\n",
    "    # 2. Load\n",
    "    print(f\"2. Loading {extension_name}...\")\n",
    "    con.execute(f\"LOAD {extension_name}\")\n",
    "    print(f\"   ✓ Loaded\")\n",
    "\n",
    "    # 3. Create secret\n",
    "    print(f\"3. Creating secret...\")\n",
    "    con.execute(secret_config)\n",
    "    print(f\"   ✓ Secret created\")\n",
    "\n",
    "    # 4. Verify\n",
    "    secrets = con.execute(\"SELECT name, type FROM duckdb_secrets()\").df()\n",
    "    print(f\"4. Verification:\")\n",
    "    print(f\"   Secrets: {len(secrets)}\")\n",
    "\n",
    "    con.close()\n",
    "    return True\n",
    "\n",
    "# Exemplo\n",
    "s3_config = \"\"\"\n",
    "    CREATE SECRET test_s3 (\n",
    "        TYPE s3,\n",
    "        KEY_ID 'key',\n",
    "        SECRET 'secret'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "setup_extension_with_secret('httpfs', s3_config)\n",
    "\n",
    "print(\"\"\"\n",
    "Ordem Importa!\n",
    "──────────────\n",
    "❌ Errado:\n",
    "   CREATE SECRET ...  # Erro! Extension não carregada\n",
    "   INSTALL httpfs\n",
    "   LOAD httpfs\n",
    "\n",
    "✓ Certo:\n",
    "   INSTALL httpfs\n",
    "   LOAD httpfs\n",
    "   CREATE SECRET ...  # OK! Extension já está loaded\n",
    "\"\"\")\n",
    "\n",
    "# Exemplo/Bloco 3\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Setup httpfs\n",
    "con.execute(\"INSTALL httpfs\")\n",
    "con.execute(\"LOAD httpfs\")\n",
    "\n",
    "print(\"\"\"\n",
    "httpfs Extension - S3 Support:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "Funcionalidades:\n",
    "────────────────\n",
    "✓ Ler/escrever de/para S3\n",
    "✓ Suporte a S3-compatible (MinIO, Wasabi, R2)\n",
    "✓ Múltiplos secrets com SCOPE\n",
    "✓ Credential chain support\n",
    "✓ Parquet, CSV, JSON diretamente de S3\n",
    "\"\"\")\n",
    "\n",
    "# Configurar múltiplos secrets\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET s3_main (\n",
    "        TYPE s3,\n",
    "        KEY_ID 'AKIAIOSFODNN7EXAMPLE',\n",
    "        SECRET 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "        REGION 'us-east-1',\n",
    "        SCOPE 's3://main-bucket/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET s3_analytics (\n",
    "        TYPE s3,\n",
    "        KEY_ID 'AKIAANALYTICS',\n",
    "        SECRET 'analytics_secret',\n",
    "        REGION 'us-west-2',\n",
    "        SCOPE 's3://analytics-bucket/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nExemplos de Queries:\")\n",
    "\n",
    "# Read Parquet\n",
    "query_read = \"\"\"\n",
    "SELECT *\n",
    "FROM 's3://main-bucket/data/sales/2024/01/data.parquet'\n",
    "WHERE amount > 1000\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "print(f\"\\n1. Read Parquet:\\n{query_read}\")\n",
    "\n",
    "# Write Parquet\n",
    "query_write = \"\"\"\n",
    "COPY (\n",
    "    SELECT\n",
    "        date_trunc('month', order_date) as month,\n",
    "        SUM(amount) as total\n",
    "    FROM orders\n",
    "    GROUP BY 1\n",
    ") TO 's3://analytics-bucket/reports/monthly_summary.parquet'\n",
    "(FORMAT PARQUET, COMPRESSION 'snappy')\n",
    "\"\"\"\n",
    "print(f\"\\n2. Write Parquet:\\n{query_write}\")\n",
    "\n",
    "# Read CSV\n",
    "query_csv = \"\"\"\n",
    "SELECT *\n",
    "FROM read_csv('s3://main-bucket/data/customers.csv')\n",
    "\"\"\"\n",
    "print(f\"\\n3. Read CSV:\\n{query_csv}\")\n",
    "\n",
    "# Read multiple files with glob\n",
    "query_glob = \"\"\"\n",
    "SELECT *\n",
    "FROM read_parquet('s3://main-bucket/data/sales/2024/*/*.parquet')\n",
    "\"\"\"\n",
    "print(f\"\\n4. Read with glob:\\n{query_glob}\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 4\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL httpfs\")\n",
    "con.execute(\"LOAD httpfs\")\n",
    "\n",
    "print(\"\"\"\n",
    "httpfs Extension - GCS Support:\n",
    "════════════════════════════════════════════════════════\n",
    "\"\"\")\n",
    "\n",
    "# Method 1: Service Account\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET gcs_sa (\n",
    "        TYPE gcs,\n",
    "        KEY_ID 'service-account@project.iam.gserviceaccount.com',\n",
    "        SECRET '/path/to/service-account-key.json',\n",
    "        SCOPE 'gs://my-gcs-bucket/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Method 2: Credential chain\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET gcs_adc (\n",
    "        TYPE gcs,\n",
    "        PROVIDER credential_chain,\n",
    "        SCOPE 'gs://another-bucket/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "GCS Queries:\n",
    "────────────\n",
    "\"\"\")\n",
    "\n",
    "queries = [\n",
    "    \"\"\"\n",
    "    -- Read from GCS\n",
    "    SELECT * FROM 'gs://my-gcs-bucket/data.parquet'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Write to GCS\n",
    "    COPY (SELECT * FROM my_table)\n",
    "    TO 'gs://my-gcs-bucket/export/data.parquet'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Read multiple files\n",
    "    SELECT * FROM read_parquet('gs://my-gcs-bucket/year=2024/month=*/*.parquet')\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"{i}.{query}\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 5\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL httpfs\")\n",
    "con.execute(\"LOAD httpfs\")\n",
    "\n",
    "print(\"\"\"\n",
    "httpfs Extension - HTTP Support:\n",
    "════════════════════════════════════════════════════════\n",
    "\"\"\")\n",
    "\n",
    "# Bearer token\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET api_bearer (\n",
    "        TYPE http,\n",
    "        BEARER_TOKEN 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...',\n",
    "        SCOPE 'https://api.example.com/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Basic auth\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET api_basic (\n",
    "        TYPE http,\n",
    "        USERNAME 'api_user',\n",
    "        PASSWORD 'api_password',\n",
    "        SCOPE 'https://internal-api.company.com/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Custom headers\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET api_custom (\n",
    "        TYPE http,\n",
    "        HEADERS MAP {\n",
    "            'Authorization': 'Bearer token123',\n",
    "            'X-API-Key': 'my_api_key',\n",
    "            'User-Agent': 'DuckDB/1.0'\n",
    "        },\n",
    "        SCOPE 'https://custom-api.example.com/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "HTTP Queries:\n",
    "─────────────\n",
    "\"\"\")\n",
    "\n",
    "examples = [\n",
    "    \"\"\"\n",
    "    -- Read JSON from API with bearer token\n",
    "    SELECT *\n",
    "    FROM read_json('https://api.example.com/data/users.json')\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Read CSV from authenticated endpoint\n",
    "    SELECT *\n",
    "    FROM read_csv('https://internal-api.company.com/reports/daily.csv')\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Read Parquet from URL with custom headers\n",
    "    SELECT *\n",
    "    FROM 'https://custom-api.example.com/datasets/large.parquet'\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, example in enumerate(examples, 1):\n",
    "    print(f\"{i}.{example}\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 6\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL httpfs\")\n",
    "con.execute(\"LOAD httpfs\")\n",
    "\n",
    "print(\"\"\"\n",
    "httpfs Extension - Cloudflare R2 Support:\n",
    "════════════════════════════════════════════════════════\n",
    "\"\"\")\n",
    "\n",
    "# R2 secret\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET r2_storage (\n",
    "        TYPE r2,\n",
    "        KEY_ID 'r2_access_key_id',\n",
    "        SECRET 'r2_secret_access_key',\n",
    "        ACCOUNT_ID 'cloudflare_account_id',\n",
    "        SCOPE 'r2://my-r2-bucket/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "R2 Features:\n",
    "────────────\n",
    "✓ S3-compatible API\n",
    "✓ Zero egress fees\n",
    "✓ Global distribution\n",
    "✓ Same query patterns as S3\n",
    "\n",
    "R2 Queries:\n",
    "───────────\n",
    "\"\"\")\n",
    "\n",
    "r2_queries = [\n",
    "    \"\"\"\n",
    "    -- Read from R2\n",
    "    SELECT * FROM 'r2://my-r2-bucket/data/events.parquet'\n",
    "    WHERE event_date >= '2024-01-01'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Write to R2\n",
    "    COPY (\n",
    "        SELECT * FROM processed_data\n",
    "    ) TO 'r2://my-r2-bucket/exports/processed.parquet'\n",
    "    (FORMAT PARQUET, COMPRESSION 'zstd')\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Read partitioned data\n",
    "    SELECT *\n",
    "    FROM read_parquet('r2://my-r2-bucket/events/year=*/month=*/*.parquet')\n",
    "    WHERE year = 2024 AND month = 1\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(r2_queries, 1):\n",
    "    print(f\"{i}.{query}\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 7\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL azure\")\n",
    "con.execute(\"LOAD azure\")\n",
    "\n",
    "print(\"\"\"\n",
    "azure Extension:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "Authentication Methods:\n",
    "───────────────────────\n",
    "1. Connection String\n",
    "2. Account Key\n",
    "3. Service Principal\n",
    "4. Managed Identity\n",
    "5. Credential Chain\n",
    "\"\"\")\n",
    "\n",
    "# Connection string\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET azure_conn (\n",
    "        TYPE azure,\n",
    "        CONNECTION_STRING 'DefaultEndpointsProtocol=https;AccountName=myaccount;AccountKey=key;EndpointSuffix=core.windows.net',\n",
    "        SCOPE 'azure://production-container/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Managed identity (for Azure VMs)\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET azure_mi (\n",
    "        TYPE azure,\n",
    "        PROVIDER managed_identity,\n",
    "        ACCOUNT_NAME 'myaccount',\n",
    "        SCOPE 'azure://analytics-container/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Service principal\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET azure_sp (\n",
    "        TYPE azure,\n",
    "        PROVIDER service_principal,\n",
    "        TENANT_ID '00000000-0000-0000-0000-000000000000',\n",
    "        CLIENT_ID '11111111-1111-1111-1111-111111111111',\n",
    "        CLIENT_SECRET 'client_secret',\n",
    "        ACCOUNT_NAME 'myaccount',\n",
    "        SCOPE 'azure://backup-container/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "Azure Queries:\n",
    "──────────────\n",
    "\"\"\")\n",
    "\n",
    "azure_queries = [\n",
    "    \"\"\"\n",
    "    -- Read from Azure Blob\n",
    "    SELECT *\n",
    "    FROM 'azure://production-container/data/sales.parquet'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Write to Azure Blob\n",
    "    COPY (SELECT * FROM analytics)\n",
    "    TO 'azure://analytics-container/reports/summary.parquet'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Read from hierarchical namespace (ADLS Gen2)\n",
    "    SELECT *\n",
    "    FROM 'azure://adls-container/bronze/events/2024/01/*.parquet'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Cross-container query\n",
    "    SELECT\n",
    "        p.product_id,\n",
    "        p.name,\n",
    "        s.quantity\n",
    "    FROM 'azure://production-container/products.parquet' p\n",
    "    JOIN 'azure://analytics-container/sales.parquet' s\n",
    "      ON p.product_id = s.product_id\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(azure_queries, 1):\n",
    "    print(f\"{i}.{query}\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 8\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL azure\")\n",
    "con.execute(\"LOAD azure\")\n",
    "\n",
    "print(\"\"\"\n",
    "Azure Data Lake Storage Gen2:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "ADLS Gen2 = Azure Blob Storage + Hierarchical Namespace\n",
    "\n",
    "Features:\n",
    "─────────\n",
    "✓ Hierarchical file system\n",
    "✓ POSIX permissions\n",
    "✓ Better performance for analytics\n",
    "✓ Direct compatibility with Hadoop/Spark\n",
    "\"\"\")\n",
    "\n",
    "# ADLS Gen2 secret\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET adls_gen2 (\n",
    "        TYPE azure,\n",
    "        PROVIDER service_principal,\n",
    "        TENANT_ID 'tenant-id',\n",
    "        CLIENT_ID 'client-id',\n",
    "        CLIENT_SECRET 'client-secret',\n",
    "        ACCOUNT_NAME 'adlsgen2account',\n",
    "        SCOPE 'azure://datalake/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "ADLS Gen2 Patterns:\n",
    "───────────────────\n",
    "\n",
    "Medallion Architecture:\n",
    "\"\"\")\n",
    "\n",
    "medallion = [\n",
    "    \"\"\"\n",
    "    -- Bronze Layer (raw data)\n",
    "    SELECT *\n",
    "    FROM 'azure://datalake/bronze/source_system/YYYY/MM/DD/*.parquet'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Silver Layer (cleaned, validated)\n",
    "    CREATE TABLE silver.customers AS\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        clean_name(name) as name,\n",
    "        validated_email(email) as email\n",
    "    FROM 'azure://datalake/silver/customers/*.parquet'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Gold Layer (business aggregates)\n",
    "    CREATE TABLE gold.customer_metrics AS\n",
    "    SELECT\n",
    "        date_trunc('month', order_date) as month,\n",
    "        customer_id,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(amount) as total_spent\n",
    "    FROM 'azure://datalake/gold/orders/*.parquet'\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, pattern in enumerate(medallion, 1):\n",
    "    print(f\"{i}.{pattern}\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 9\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL mysql_scanner\")\n",
    "con.execute(\"LOAD mysql_scanner\")\n",
    "\n",
    "print(\"\"\"\n",
    "mysql_scanner Extension:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "Features:\n",
    "─────────\n",
    "✓ ATTACH MySQL databases\n",
    "✓ Query MySQL tables directly\n",
    "✓ Push-down predicates\n",
    "✓ Type mapping\n",
    "✓ SSL support\n",
    "\"\"\")\n",
    "\n",
    "# MySQL secret\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET mysql_prod (\n",
    "        TYPE mysql,\n",
    "        HOST 'mysql.example.com',\n",
    "        PORT 3306,\n",
    "        DATABASE 'production',\n",
    "        USER 'readonly_user',\n",
    "        PASSWORD 'secure_password',\n",
    "        SSL_MODE 'REQUIRED'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# ATTACH database\n",
    "con.execute(\"\"\"\n",
    "    ATTACH 'mysql:production' AS mysql_prod (\n",
    "        TYPE mysql,\n",
    "        SECRET mysql_prod\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "MySQL Queries:\n",
    "──────────────\n",
    "\"\"\")\n",
    "\n",
    "mysql_queries = [\n",
    "    \"\"\"\n",
    "    -- Direct query on MySQL table\n",
    "    SELECT *\n",
    "    FROM mysql_prod.orders\n",
    "    WHERE order_date >= '2024-01-01'\n",
    "    LIMIT 100\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- JOIN MySQL with local data\n",
    "    SELECT\n",
    "        m.customer_id,\n",
    "        m.customer_name,\n",
    "        l.score\n",
    "    FROM mysql_prod.customers m\n",
    "    JOIN local_customer_scores l\n",
    "      ON m.customer_id = l.customer_id\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Aggregate from MySQL\n",
    "    SELECT\n",
    "        DATE_TRUNC('month', order_date) as month,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(amount) as total_amount\n",
    "    FROM mysql_prod.orders\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1 DESC\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Export MySQL to Parquet\n",
    "    COPY (\n",
    "        SELECT * FROM mysql_prod.large_table\n",
    "    ) TO 'export/mysql_data.parquet'\n",
    "    (FORMAT PARQUET, COMPRESSION 'snappy')\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(mysql_queries, 1):\n",
    "    print(f\"{i}.{query}\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 10\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "MySQL ETL Patterns com DuckDB:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "Pattern 1: MySQL → DuckDB → Parquet\n",
    "────────────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "etl_pattern_1 = \"\"\"\n",
    "-- 1. Setup\n",
    "INSTALL mysql_scanner;\n",
    "LOAD mysql_scanner;\n",
    "\n",
    "CREATE SECRET mysql_source (\n",
    "    TYPE mysql,\n",
    "    HOST 'mysql.example.com',\n",
    "    DATABASE 'source_db',\n",
    "    USER 'etl_user',\n",
    "    PASSWORD 'etl_password'\n",
    ");\n",
    "\n",
    "ATTACH 'mysql:source_db' AS mysql_src (\n",
    "    TYPE mysql,\n",
    "    SECRET mysql_source\n",
    ");\n",
    "\n",
    "-- 2. ETL: Incremental load\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM mysql_src.transactions\n",
    "    WHERE updated_at >= (\n",
    "        SELECT MAX(updated_at)\n",
    "        FROM 'data/transactions/*.parquet'\n",
    "    )\n",
    ") TO 'data/transactions/incremental.parquet'\n",
    "(FORMAT PARQUET, PARTITION_BY (year, month));\n",
    "\"\"\"\n",
    "\n",
    "print(etl_pattern_1)\n",
    "\n",
    "print(\"\"\"\n",
    "Pattern 2: Multiple MySQL → Unified View\n",
    "─────────────────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "etl_pattern_2 = \"\"\"\n",
    "-- Attach multiple MySQL databases\n",
    "ATTACH 'mysql:db1' AS mysql_db1 (TYPE mysql, SECRET mysql_secret1);\n",
    "ATTACH 'mysql:db2' AS mysql_db2 (TYPE mysql, SECRET mysql_secret2);\n",
    "ATTACH 'mysql:db3' AS mysql_db3 (TYPE mysql, SECRET mysql_secret3);\n",
    "\n",
    "-- Create unified view\n",
    "CREATE VIEW unified_customers AS\n",
    "SELECT 'db1' as source, * FROM mysql_db1.customers\n",
    "UNION ALL\n",
    "SELECT 'db2' as source, * FROM mysql_db2.customers\n",
    "UNION ALL\n",
    "SELECT 'db3' as source, * FROM mysql_db3.customers;\n",
    "\n",
    "-- Query across all databases\n",
    "SELECT\n",
    "    source,\n",
    "    COUNT(*) as customer_count,\n",
    "    SUM(total_orders) as total_orders\n",
    "FROM unified_customers\n",
    "GROUP BY source;\n",
    "\"\"\"\n",
    "\n",
    "print(etl_pattern_2)\n",
    "\n",
    "con = duckdb.connect()\n",
    "print(\"\\n✓ mysql_scanner patterns documented\")\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 11\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL postgres_scanner\")\n",
    "con.execute(\"LOAD postgres_scanner\")\n",
    "\n",
    "print(\"\"\"\n",
    "postgres_scanner Extension:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "Features:\n",
    "─────────\n",
    "✓ ATTACH PostgreSQL databases\n",
    "✓ Query PostgreSQL tables directly\n",
    "✓ Push-down predicates and aggregations\n",
    "✓ Type mapping (including JSONB, arrays)\n",
    "✓ SSL/TLS support\n",
    "✓ Connection pooling\n",
    "\"\"\")\n",
    "\n",
    "# PostgreSQL secrets for different environments\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET postgres_prod (\n",
    "        TYPE postgres,\n",
    "        HOST 'postgres.example.com',\n",
    "        PORT 5432,\n",
    "        DATABASE 'production',\n",
    "        USER 'readonly_user',\n",
    "        PASSWORD 'secure_password',\n",
    "        SSLMODE 'verify-full',\n",
    "        SSLROOTCERT '/path/to/root.crt'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET postgres_analytics (\n",
    "        TYPE postgres,\n",
    "        CONNECTION_STRING 'postgresql://analyst:pass@analytics.example.com:5432/analytics?sslmode=require'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# ATTACH databases\n",
    "con.execute(\"\"\"\n",
    "    ATTACH 'postgres:production' AS pg_prod (\n",
    "        TYPE postgres,\n",
    "        SECRET postgres_prod\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    ATTACH 'postgres:analytics' AS pg_analytics (\n",
    "        TYPE postgres,\n",
    "        SECRET postgres_analytics\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "PostgreSQL Queries:\n",
    "───────────────────\n",
    "\"\"\")\n",
    "\n",
    "postgres_queries = [\n",
    "    \"\"\"\n",
    "    -- Query PostgreSQL table\n",
    "    SELECT *\n",
    "    FROM pg_prod.orders\n",
    "    WHERE order_date >= CURRENT_DATE - INTERVAL '7 days'\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- JOIN PostgreSQL with DuckDB\n",
    "    SELECT\n",
    "        p.product_id,\n",
    "        p.name,\n",
    "        s.stock_level,\n",
    "        s.warehouse_location\n",
    "    FROM pg_prod.products p\n",
    "    JOIN local_stock_data s\n",
    "      ON p.product_id = s.product_id\n",
    "    WHERE s.stock_level < 10\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Aggregate from PostgreSQL\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as lifetime_value,\n",
    "        MAX(order_date) as last_order\n",
    "    FROM pg_prod.orders\n",
    "    GROUP BY customer_id\n",
    "    HAVING COUNT(*) > 5\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Cross-database analytics\n",
    "    SELECT\n",
    "        'Production' as db,\n",
    "        COUNT(*) as record_count\n",
    "    FROM pg_prod.transactions\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'Analytics' as db,\n",
    "        COUNT(*) as record_count\n",
    "    FROM pg_analytics.processed_transactions\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(postgres_queries, 1):\n",
    "    print(f\"{i}.{query}\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 12\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "PostgreSQL Advanced Patterns:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "Pattern 1: Incremental Sync\n",
    "────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "incremental_sync = \"\"\"\n",
    "-- Track last sync\n",
    "CREATE TABLE IF NOT EXISTS sync_metadata (\n",
    "    table_name VARCHAR,\n",
    "    last_sync_timestamp TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Incremental load from PostgreSQL\n",
    "CREATE TABLE updated_records AS\n",
    "SELECT *\n",
    "FROM pg_prod.events\n",
    "WHERE updated_at > (\n",
    "    SELECT COALESCE(MAX(last_sync_timestamp), '1970-01-01'::TIMESTAMP)\n",
    "    FROM sync_metadata\n",
    "    WHERE table_name = 'events'\n",
    ");\n",
    "\n",
    "-- Update sync metadata\n",
    "INSERT INTO sync_metadata\n",
    "VALUES ('events', CURRENT_TIMESTAMP);\n",
    "\n",
    "-- Export to Parquet\n",
    "COPY updated_records\n",
    "TO 'data/events/incremental.parquet'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\"\n",
    "\n",
    "print(incremental_sync)\n",
    "\n",
    "print(\"\"\"\n",
    "Pattern 2: PostgreSQL → DuckDB Analytics\n",
    "─────────────────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "analytics_pattern = \"\"\"\n",
    "-- Create analytics view in DuckDB\n",
    "CREATE VIEW customer_360 AS\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.name,\n",
    "    c.email,\n",
    "    c.segment,\n",
    "    o.order_count,\n",
    "    o.total_spent,\n",
    "    o.last_order_date,\n",
    "    s.support_tickets,\n",
    "    s.satisfaction_score\n",
    "FROM pg_prod.customers c\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(amount) as total_spent,\n",
    "        MAX(order_date) as last_order_date\n",
    "    FROM pg_prod.orders\n",
    "    GROUP BY customer_id\n",
    ") o ON c.customer_id = o.customer_id\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(*) as support_tickets,\n",
    "        AVG(satisfaction) as satisfaction_score\n",
    "    FROM pg_prod.support_interactions\n",
    "    GROUP BY customer_id\n",
    ") s ON c.customer_id = s.customer_id;\n",
    "\n",
    "-- Fast analytics on denormalized view\n",
    "SELECT\n",
    "    segment,\n",
    "    COUNT(*) as customers,\n",
    "    AVG(total_spent) as avg_lifetime_value,\n",
    "    AVG(satisfaction_score) as avg_satisfaction\n",
    "FROM customer_360\n",
    "WHERE order_count > 0\n",
    "GROUP BY segment\n",
    "ORDER BY avg_lifetime_value DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(analytics_pattern)\n",
    "\n",
    "print(\"\"\"\n",
    "Pattern 3: Cross-Database ETL\n",
    "──────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "cross_db_etl = \"\"\"\n",
    "-- Read from MySQL\n",
    "ATTACH 'mysql:orders_db' AS mysql_orders (TYPE mysql, SECRET mysql_secret);\n",
    "\n",
    "-- Read from PostgreSQL\n",
    "ATTACH 'postgres:customers_db' AS pg_customers (TYPE postgres, SECRET pg_secret);\n",
    "\n",
    "-- Join and export\n",
    "COPY (\n",
    "    SELECT\n",
    "        o.order_id,\n",
    "        o.order_date,\n",
    "        o.amount,\n",
    "        c.customer_name,\n",
    "        c.customer_segment,\n",
    "        c.customer_region\n",
    "    FROM mysql_orders.orders o\n",
    "    JOIN pg_customers.customers c\n",
    "      ON o.customer_id = c.customer_id\n",
    "    WHERE o.order_date >= '2024-01-01'\n",
    ") TO 'analytics/enriched_orders.parquet'\n",
    "(FORMAT PARQUET, COMPRESSION 'zstd');\n",
    "\"\"\"\n",
    "\n",
    "print(cross_db_etl)\n",
    "\n",
    "con = duckdb.connect()\n",
    "print(\"\\n✓ PostgreSQL patterns documented\")\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 13\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Extension Auto-loading:\n",
    "════════════════════════════════════════════════════════\n",
    "\n",
    "DuckDB pode instalar e carregar extensions automaticamente\n",
    "quando necessário.\n",
    "\n",
    "Configuration:\n",
    "──────────────\n",
    "\"\"\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Habilitar auto-install\n",
    "con.execute(\"SET autoinstall_known_extensions = true\")\n",
    "con.execute(\"SET autoload_known_extensions = true\")\n",
    "\n",
    "print(\"\"\"\n",
    "Configurações:\n",
    "──────────────\n",
    "autoinstall_known_extensions = true\n",
    "  → DuckDB instala extensions automaticamente quando necessário\n",
    "\n",
    "autoload_known_extensions = true\n",
    "  → DuckDB carrega extensions automaticamente\n",
    "\n",
    "Exemplo:\n",
    "────────\n",
    "\"\"\")\n",
    "\n",
    "example = \"\"\"\n",
    "-- Com auto-loading habilitado\n",
    "SET autoinstall_known_extensions = true;\n",
    "SET autoload_known_extensions = true;\n",
    "\n",
    "-- Não precisa de INSTALL/LOAD explícito!\n",
    "CREATE SECRET my_s3 (\n",
    "    TYPE s3,\n",
    "    KEY_ID 'key',\n",
    "    SECRET 'secret'\n",
    ");\n",
    "\n",
    "-- httpfs será instalado e loaded automaticamente\n",
    "SELECT * FROM 's3://bucket/file.parquet';\n",
    "\"\"\"\n",
    "\n",
    "print(example)\n",
    "\n",
    "print(\"\"\"\n",
    "Vantagens:\n",
    "──────────\n",
    "✓ Menos boilerplate\n",
    "✓ Código mais limpo\n",
    "✓ Fácil para iniciantes\n",
    "\n",
    "Desvantagens:\n",
    "─────────────\n",
    "✗ Menos controle\n",
    "✗ Pode instalar extensions inesperadas\n",
    "✗ Overhead de network em primeira uso\n",
    "\n",
    "Recomendação:\n",
    "─────────────\n",
    "Development: auto-loading OK\n",
    "Production: INSTALL/LOAD explícito\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 14\n",
    "# 1. Instale e carregue httpfs e azure extensions\n",
    "# 2. Crie secrets para S3, GCS, Azure\n",
    "# 3. Teste leitura de arquivos (se tiver acesso)\n",
    "# 4. Documente qual extension é necessária para cada tipo\n",
    "# 5. Teste which_secret() para cada provider\n",
    "\n",
    "# Sua solução aqui\n",
    "\n",
    "# Exemplo/Bloco 15\n",
    "# 1. Instale mysql_scanner e postgres_scanner\n",
    "# 2. Crie secrets para ambos\n",
    "# 3. Use ATTACH para conectar (se tiver databases)\n",
    "# 4. Crie query que une dados de ambos\n",
    "# 5. Export resultado para Parquet\n",
    "\n",
    "# Sua solução aqui\n",
    "\n",
    "# Exemplo/Bloco 16\n",
    "# 1. Crie função que setup_complete():\n",
    "#    - Verifica se extension está instalada\n",
    "#    - Instala se necessário\n",
    "#    - Load extension\n",
    "#    - Cria secret\n",
    "#    - Valida configuração\n",
    "# 2. Teste com httpfs\n",
    "# 3. Teste com azure\n",
    "# 4. Adicione error handling\n",
    "\n",
    "# Sua solução aqui\n",
    "\n",
    "# Exemplo/Bloco 17\n",
    "# 1. Configure PostgreSQL source com secret\n",
    "# 2. Configure S3 destination com secret\n",
    "# 3. Implemente ETL:\n",
    "#    - Read from PostgreSQL\n",
    "#    - Transform in DuckDB\n",
    "#    - Write to S3 Parquet\n",
    "# 4. Adicione incremental loading\n",
    "# 5. Adicione error handling e logging\n",
    "\n",
    "# Sua solução aqui\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}