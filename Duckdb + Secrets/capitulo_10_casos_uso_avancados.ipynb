{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Secrets-10-casos-uso-avancados\n",
        "\"\"\"\n",
        "\n",
        "# Secrets-10-casos-uso-avancados\n",
        "import duckdb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 1\n",
        "import duckdb\n",
        "\n",
        "print(\"\"\"\n",
        "Caso de Uso: Multi-Cloud ETL Pipeline\n",
        "════════════════════════════════════════════════════════\n",
        "\n",
        "Cenário:\n",
        "────────\n",
        "- Dados source em AWS S3\n",
        "- Processing intermediário em GCP\n",
        "- Resultado final em Azure\n",
        "- Tudo orquestrado via DuckDB\n",
        "\n",
        "Arquitetura:\n",
        "────────────\n",
        "AWS S3 (Raw) → DuckDB → GCS (Processed) → DuckDB → Azure (Final)\n",
        "\"\"\")\n",
        "\n",
        "con = duckdb.connect()\n",
        "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
        "con.execute(\"INSTALL azure; LOAD azure;\")\n",
        "\n",
        "# Configure secrets\n",
        "print(\"\\n1. Configurando secrets...\")\n",
        "\n",
        "# AWS S3 (source)\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET s3_source (\n",
        "        TYPE s3,\n",
        "        PROVIDER credential_chain,\n",
        "        CHAIN 'env;config',\n",
        "        SCOPE 's3://company-raw-data/'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# GCP (intermediate)\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET gcs_processing (\n",
        "        TYPE gcs,\n",
        "        PROVIDER credential_chain,\n",
        "        SCOPE 'gs://company-processing/'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# Azure (destination)\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET azure_final (\n",
        "        TYPE azure,\n",
        "        PROVIDER managed_identity,\n",
        "        ACCOUNT_NAME 'companyfinal',\n",
        "        SCOPE 'azure://final-data/'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "print(\"✓ Secrets configurados\")\n",
        "\n",
        "# ETL Pipeline\n",
        "etl_pipeline = \"\"\"\n",
        "-- Step 1: Extract from AWS S3\n",
        "CREATE TEMP TABLE raw_events AS\n",
        "SELECT\n",
        "    event_id,\n",
        "    user_id,\n",
        "    event_type,\n",
        "    timestamp,\n",
        "    properties\n",
        "FROM read_parquet('s3://company-raw-data/events/2024/01/*.parquet');\n",
        "\n",
        "-- Step 2: Transform\n",
        "CREATE TEMP TABLE processed_events AS\n",
        "SELECT\n",
        "    event_id,\n",
        "    user_id,\n",
        "    event_type,\n",
        "    DATE_TRUNC('hour', timestamp) as hour,\n",
        "    COUNT(*) OVER (PARTITION BY user_id) as user_event_count,\n",
        "    JSON_EXTRACT_STRING(properties, '$.country') as country,\n",
        "    JSON_EXTRACT_STRING(properties, '$.device') as device\n",
        "FROM raw_events\n",
        "WHERE timestamp >= '2024-01-01'\n",
        "    AND event_type IN ('page_view', 'click', 'purchase');\n",
        "\n",
        "-- Step 3: Load to GCS (intermediate)\n",
        "COPY processed_events\n",
        "TO 'gs://company-processing/events/processed.parquet'\n",
        "(FORMAT PARQUET, PARTITION_BY (country, hour));\n",
        "\n",
        "-- Step 4: Aggregations\n",
        "CREATE TEMP TABLE aggregated_metrics AS\n",
        "SELECT\n",
        "    hour,\n",
        "    country,\n",
        "    device,\n",
        "    event_type,\n",
        "    COUNT(*) as event_count,\n",
        "    COUNT(DISTINCT user_id) as unique_users\n",
        "FROM processed_events\n",
        "GROUP BY 1, 2, 3, 4;\n",
        "\n",
        "-- Step 5: Load to Azure (final)\n",
        "COPY aggregated_metrics\n",
        "TO 'azure://final-data/metrics/daily_metrics.parquet'\n",
        "(FORMAT PARQUET, COMPRESSION 'zstd');\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n2. ETL Pipeline:\")\n",
        "print(etl_pipeline)\n",
        "\n",
        "print(\"\"\"\n",
        "Vantagens Multi-Cloud:\n",
        "──────────────────────\n",
        "✓ Vendor lock-in mitigation\n",
        "✓ Cost optimization por workload\n",
        "✓ Geographic data residency\n",
        "✓ Best-of-breed services\n",
        "✓ Disaster recovery\n",
        "\n",
        "Desafios:\n",
        "─────────\n",
        "✗ Egress costs (exceto R2)\n",
        "✗ Complexidade de gestão\n",
        "✗ Multiple authentication systems\n",
        "✗ Data consistency\n",
        "✗ Latência cross-cloud\n",
        "\n",
        "Best Practices:\n",
        "───────────────\n",
        "✓ Minimize data movement\n",
        "✓ Use compression (zstd)\n",
        "✓ Partition appropriately\n",
        "✓ Monitor costs\n",
        "✓ Implement retry logic\n",
        "\"\"\")\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 2\n",
        "import duckdb\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"\"\"\n",
        "Incremental Multi-Cloud Sync:\n",
        "════════════════════════════════════════════════════════\n",
        "\"\"\")\n",
        "\n",
        "def incremental_multicloud_sync(source_secret, dest_secret, source_path, dest_path):\n",
        "    \"\"\"\n",
        "    Sync incremental entre clouds\n",
        "    \"\"\"\n",
        "    con = duckdb.connect('sync.duckdb')\n",
        "    con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Incremental Sync\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Source: {source_path}\")\n",
        "    print(f\"Dest: {dest_path}\")\n",
        "\n",
        "    # 1. Obter último sync\n",
        "    try:\n",
        "        last_sync = con.execute(\"\"\"\n",
        "            SELECT MAX(sync_timestamp) as last_sync\n",
        "            FROM sync_metadata\n",
        "            WHERE source_path = ?\n",
        "        \"\"\", [source_path]).fetchone()[0]\n",
        "    except:\n",
        "        # Primeira sync\n",
        "        con.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS sync_metadata (\n",
        "                source_path VARCHAR,\n",
        "                dest_path VARCHAR,\n",
        "                sync_timestamp TIMESTAMP,\n",
        "                records_synced BIGINT\n",
        "            )\n",
        "        \"\"\")\n",
        "        last_sync = datetime(2020, 1, 1)\n",
        "\n",
        "    print(f\"\\n1. Last sync: {last_sync}\")\n",
        "\n",
        "    # 2. Read incremental data\n",
        "    query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM read_parquet('{source_path}')\n",
        "        WHERE updated_at > '{last_sync}'\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"2. Reading incremental data...\")\n",
        "    incremental_data = con.execute(query)\n",
        "\n",
        "    # 3. Write to destination\n",
        "    print(f\"3. Writing to {dest_path}...\")\n",
        "    con.execute(f\"\"\"\n",
        "        COPY (\n",
        "            {query}\n",
        "        ) TO '{dest_path}'\n",
        "        (FORMAT PARQUET, COMPRESSION 'snappy', APPEND true)\n",
        "    \"\"\")\n",
        "\n",
        "    # 4. Update metadata\n",
        "    records_count = con.execute(f\"SELECT COUNT(*) FROM ({query})\").fetchone()[0]\n",
        "    con.execute(\"\"\"\n",
        "        INSERT INTO sync_metadata\n",
        "        VALUES (?, ?, ?, ?)\n",
        "    \"\"\", [source_path, dest_path, datetime.now(), records_count])\n",
        "\n",
        "    print(f\"4. Synced {records_count} records\")\n",
        "    print(f\"✓ Sync completed!\")\n",
        "\n",
        "    con.close()\n",
        "\n",
        "# Documentação\n",
        "print(\"\"\"\n",
        "Implementação:\n",
        "──────────────\n",
        "\n",
        "# Schedule com cron ou Airflow\n",
        "0 * * * * python incremental_sync.py  # A cada hora\n",
        "\n",
        "# Ou via Airflow DAG\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "\n",
        "dag = DAG(\n",
        "    'multicloud_sync',\n",
        "    schedule_interval='@hourly',\n",
        "    catchup=False\n",
        ")\n",
        "\n",
        "sync_task = PythonOperator(\n",
        "    task_id='incremental_sync',\n",
        "    python_callable=incremental_multicloud_sync,\n",
        "    op_kwargs={\n",
        "        'source_secret': 's3_source',\n",
        "        'dest_secret': 'azure_dest',\n",
        "        'source_path': 's3://source/data/*.parquet',\n",
        "        'dest_path': 'azure://dest/data/'\n",
        "    },\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "Monitoramento:\n",
        "──────────────\n",
        "- Records synced per run\n",
        "- Sync duration\n",
        "- Error rate\n",
        "- Data lag\n",
        "- Cost per sync\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 3\n",
        "import duckdb\n",
        "\n",
        "print(\"\"\"\n",
        "Medallion Architecture com DuckDB:\n",
        "════════════════════════════════════════════════════════\n",
        "\n",
        "Bronze → Silver → Gold\n",
        "\n",
        "Bronze: Raw data (S3)\n",
        "Silver: Cleaned, validated (GCS)\n",
        "Gold: Business aggregates (Azure)\n",
        "\"\"\")\n",
        "\n",
        "con = duckdb.connect()\n",
        "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
        "con.execute(\"INSTALL azure; LOAD azure;\")\n",
        "\n",
        "# Configure secrets por layer\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET bronze_s3 (\n",
        "        TYPE s3,\n",
        "        PROVIDER credential_chain,\n",
        "        SCOPE 's3://datalake-bronze/'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET silver_gcs (\n",
        "        TYPE gcs,\n",
        "        PROVIDER credential_chain,\n",
        "        SCOPE 'gs://datalake-silver/'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET gold_azure (\n",
        "        TYPE azure,\n",
        "        PROVIDER service_principal,\n",
        "        TENANT_ID 'tenant-id',\n",
        "        CLIENT_ID 'client-id',\n",
        "        CLIENT_SECRET 'client-secret',\n",
        "        ACCOUNT_NAME 'datalakegold',\n",
        "        SCOPE 'azure://gold/'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# Bronze → Silver\n",
        "bronze_to_silver = \"\"\"\n",
        "-- Bronze Layer: Raw events from S3\n",
        "CREATE OR REPLACE TEMP TABLE bronze_events AS\n",
        "SELECT *\n",
        "FROM read_parquet('s3://datalake-bronze/events/year=*/month=*/day=*/*.parquet');\n",
        "\n",
        "-- Silver Layer: Cleaned and validated\n",
        "CREATE OR REPLACE TEMP TABLE silver_events AS\n",
        "SELECT\n",
        "    event_id,\n",
        "    user_id,\n",
        "    -- Data quality: remove nulls\n",
        "    COALESCE(event_type, 'unknown') as event_type,\n",
        "    -- Data quality: valid timestamps only\n",
        "    CASE\n",
        "        WHEN timestamp >= '2020-01-01'\n",
        "         AND timestamp <= CURRENT_TIMESTAMP\n",
        "        THEN timestamp\n",
        "        ELSE NULL\n",
        "    END as timestamp,\n",
        "    -- Data quality: validate JSON\n",
        "    CASE\n",
        "        WHEN JSON_VALID(properties)\n",
        "        THEN properties\n",
        "        ELSE NULL\n",
        "    END as properties,\n",
        "    CURRENT_TIMESTAMP as processed_at\n",
        "FROM bronze_events\n",
        "WHERE user_id IS NOT NULL  -- Data quality rule\n",
        "    AND event_id IS NOT NULL;\n",
        "\n",
        "-- Write to Silver (GCS)\n",
        "COPY silver_events\n",
        "TO 'gs://datalake-silver/events/'\n",
        "(FORMAT PARQUET, PARTITION_BY (DATE_TRUNC('day', timestamp)));\n",
        "\"\"\"\n",
        "\n",
        "# Silver → Gold\n",
        "silver_to_gold = \"\"\"\n",
        "-- Silver Layer: Read cleaned data\n",
        "CREATE OR REPLACE TEMP TABLE silver_events AS\n",
        "SELECT *\n",
        "FROM read_parquet('gs://datalake-silver/events/year=*/month=*/day=*/*.parquet');\n",
        "\n",
        "-- Gold Layer: Business metrics\n",
        "CREATE OR REPLACE TEMP TABLE gold_user_metrics AS\n",
        "SELECT\n",
        "    user_id,\n",
        "    DATE_TRUNC('day', timestamp) as date,\n",
        "    COUNT(*) as total_events,\n",
        "    COUNT(DISTINCT event_type) as unique_event_types,\n",
        "    MIN(timestamp) as first_event,\n",
        "    MAX(timestamp) as last_event,\n",
        "    COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) as purchases,\n",
        "    SUM(CAST(JSON_EXTRACT(properties, '$.amount') AS DOUBLE)) as total_amount\n",
        "FROM silver_events\n",
        "WHERE timestamp IS NOT NULL\n",
        "GROUP BY user_id, DATE_TRUNC('day', timestamp);\n",
        "\n",
        "-- Write to Gold (Azure)\n",
        "COPY gold_user_metrics\n",
        "TO 'azure://gold/metrics/user_daily_metrics/'\n",
        "(FORMAT PARQUET, PARTITION_BY (date));\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n1. Bronze → Silver:\")\n",
        "print(bronze_to_silver)\n",
        "\n",
        "print(\"\\n2. Silver → Gold:\")\n",
        "print(silver_to_gold)\n",
        "\n",
        "print(\"\"\"\n",
        "Data Quality Rules:\n",
        "───────────────────\n",
        "\n",
        "Bronze:\n",
        "✓ Schema enforcement\n",
        "✓ Duplicate detection\n",
        "✓ Format validation\n",
        "\n",
        "Silver:\n",
        "✓ Null handling\n",
        "✓ Type validation\n",
        "✓ Referential integrity\n",
        "✓ Business rule validation\n",
        "✓ Deduplication\n",
        "\n",
        "Gold:\n",
        "✓ Aggregation accuracy\n",
        "✓ Metric definitions\n",
        "✓ SLA compliance\n",
        "✓ Data freshness\n",
        "\n",
        "Orchestration:\n",
        "──────────────\n",
        "1. Bronze ingestion: Real-time ou batch\n",
        "2. Silver processing: Hourly\n",
        "3. Gold aggregation: Daily\n",
        "4. Retention: Bronze (30d), Silver (1y), Gold (7y)\n",
        "\"\"\")\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 4\n",
        "import duckdb\n",
        "\n",
        "print(\"\"\"\n",
        "Lambda Architecture com DuckDB:\n",
        "════════════════════════════════════════════════════════\n",
        "\n",
        "Batch Layer + Speed Layer → Serving Layer\n",
        "\n",
        "Batch Layer: Complete historical data (S3)\n",
        "Speed Layer: Recent data, incremental (GCS)\n",
        "Serving Layer: Merged views (Azure)\n",
        "\"\"\")\n",
        "\n",
        "lambda_arch = \"\"\"\n",
        "-- Batch Layer: Historical aggregations\n",
        "CREATE OR REPLACE VIEW batch_metrics AS\n",
        "SELECT\n",
        "    user_id,\n",
        "    DATE_TRUNC('day', timestamp) as date,\n",
        "    COUNT(*) as event_count,\n",
        "    SUM(amount) as total_amount\n",
        "FROM read_parquet('s3://datalake/batch/events/**/*.parquet')\n",
        "WHERE timestamp < CURRENT_DATE\n",
        "GROUP BY 1, 2;\n",
        "\n",
        "-- Speed Layer: Real-time incremental\n",
        "CREATE OR REPLACE VIEW speed_metrics AS\n",
        "SELECT\n",
        "    user_id,\n",
        "    DATE_TRUNC('day', timestamp) as date,\n",
        "    COUNT(*) as event_count,\n",
        "    SUM(amount) as total_amount\n",
        "FROM read_parquet('gs://datalake/speed/events/**/*.parquet')\n",
        "WHERE timestamp >= CURRENT_DATE\n",
        "GROUP BY 1, 2;\n",
        "\n",
        "-- Serving Layer: Merged view\n",
        "CREATE OR REPLACE VIEW serving_metrics AS\n",
        "SELECT\n",
        "    COALESCE(b.user_id, s.user_id) as user_id,\n",
        "    COALESCE(b.date, s.date) as date,\n",
        "    COALESCE(b.event_count, 0) + COALESCE(s.event_count, 0) as event_count,\n",
        "    COALESCE(b.total_amount, 0) + COALESCE(s.total_amount, 0) as total_amount\n",
        "FROM batch_metrics b\n",
        "FULL OUTER JOIN speed_metrics s\n",
        "    ON b.user_id = s.user_id\n",
        "    AND b.date = s.date;\n",
        "\n",
        "-- Materialize to serving layer\n",
        "COPY serving_metrics\n",
        "TO 'azure://serving/metrics/user_metrics.parquet'\n",
        "(FORMAT PARQUET);\n",
        "\"\"\"\n",
        "\n",
        "print(lambda_arch)\n",
        "\n",
        "print(\"\"\"\n",
        "Vantagens Lambda:\n",
        "─────────────────\n",
        "✓ Real-time + historical data\n",
        "✓ Fault tolerance\n",
        "✓ Scalability\n",
        "✓ Reprocessing capability\n",
        "\n",
        "DuckDB Role:\n",
        "────────────\n",
        "✓ Query engine para batch layer\n",
        "✓ Merge de batch + speed layers\n",
        "✓ Serving layer queries\n",
        "✓ Data quality checks\n",
        "\n",
        "Alternativa: Kappa Architecture\n",
        "────────────────────────────────\n",
        "Use apenas streaming (sem batch layer)\n",
        "DuckDB queries real-time stream storage\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 5\n",
        "import duckdb\n",
        "\n",
        "print(\"\"\"\n",
        "Database Federation com DuckDB:\n",
        "════════════════════════════════════════════════════════\n",
        "\n",
        "Cenário:\n",
        "────────\n",
        "- Users em PostgreSQL\n",
        "- Orders em MySQL\n",
        "- Products em S3 Parquet\n",
        "- Analytics em DuckDB\n",
        "\"\"\")\n",
        "\n",
        "con = duckdb.connect()\n",
        "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
        "con.execute(\"INSTALL postgres_scanner; LOAD postgres_scanner;\")\n",
        "con.execute(\"INSTALL mysql_scanner; LOAD mysql_scanner;\")\n",
        "\n",
        "# Configure secrets\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET postgres_users (\n",
        "        TYPE postgres,\n",
        "        HOST 'postgres.example.com',\n",
        "        DATABASE 'users_db',\n",
        "        USER 'readonly',\n",
        "        PASSWORD 'password',\n",
        "        SSLMODE 'verify-full'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET mysql_orders (\n",
        "        TYPE mysql,\n",
        "        HOST 'mysql.example.com',\n",
        "        DATABASE 'orders_db',\n",
        "        USER 'readonly',\n",
        "        PASSWORD 'password',\n",
        "        SSL_MODE 'REQUIRED'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "con.execute(\"\"\"\n",
        "    CREATE SECRET s3_products (\n",
        "        TYPE s3,\n",
        "        PROVIDER credential_chain,\n",
        "        SCOPE 's3://company-data/products/'\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# ATTACH databases\n",
        "con.execute(\"\"\"\n",
        "    ATTACH 'postgres:users_db' AS pg_users (\n",
        "        TYPE postgres,\n",
        "        SECRET postgres_users\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "con.execute(\"\"\"\n",
        "    ATTACH 'mysql:orders_db' AS mysql_orders (\n",
        "        TYPE mysql,\n",
        "        SECRET mysql_orders\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# Federation query\n",
        "federation_query = \"\"\"\n",
        "-- Customer 360 View: Data de 3 fontes diferentes\n",
        "CREATE OR REPLACE VIEW customer_360 AS\n",
        "SELECT\n",
        "    -- PostgreSQL: User data\n",
        "    u.user_id,\n",
        "    u.email,\n",
        "    u.name,\n",
        "    u.country,\n",
        "    u.created_at as user_since,\n",
        "\n",
        "    -- MySQL: Order data\n",
        "    o.order_stats.total_orders,\n",
        "    o.order_stats.total_spent,\n",
        "    o.order_stats.last_order_date,\n",
        "    o.order_stats.avg_order_value,\n",
        "\n",
        "    -- S3: Product preferences\n",
        "    p.product_prefs.favorite_category,\n",
        "    p.product_prefs.categories_purchased,\n",
        "    p.product_prefs.product_diversity_score\n",
        "\n",
        "FROM pg_users.users u\n",
        "\n",
        "-- Join com MySQL orders\n",
        "LEFT JOIN (\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        STRUCT_PACK(\n",
        "            total_orders := COUNT(*),\n",
        "            total_spent := SUM(amount),\n",
        "            last_order_date := MAX(order_date),\n",
        "            avg_order_value := AVG(amount)\n",
        "        ) as order_stats\n",
        "    FROM mysql_orders.orders\n",
        "    GROUP BY customer_id\n",
        ") o ON u.user_id = o.customer_id\n",
        "\n",
        "-- Join com S3 products\n",
        "LEFT JOIN (\n",
        "    SELECT\n",
        "        user_id,\n",
        "        STRUCT_PACK(\n",
        "            favorite_category := MODE(category),\n",
        "            categories_purchased := COUNT(DISTINCT category),\n",
        "            product_diversity_score := COUNT(DISTINCT product_id) / COUNT(*)\n",
        "        ) as product_prefs\n",
        "    FROM read_parquet('s3://company-data/products/user_products.parquet')\n",
        "    GROUP BY user_id\n",
        ") p ON u.user_id = p.user_id\n",
        "\n",
        "WHERE u.status = 'active';\n",
        "\n",
        "-- Analytics query\n",
        "SELECT\n",
        "    country,\n",
        "    COUNT(*) as customers,\n",
        "    AVG(order_stats.total_orders) as avg_orders_per_customer,\n",
        "    AVG(order_stats.total_spent) as avg_lifetime_value,\n",
        "    AVG(product_prefs.product_diversity_score) as avg_diversity\n",
        "FROM customer_360\n",
        "GROUP BY country\n",
        "ORDER BY customers DESC;\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nFederation Query:\")\n",
        "print(federation_query)\n",
        "\n",
        "print(\"\"\"\n",
        "Performance Optimization:\n",
        "─────────────────────────\n",
        "✓ Push-down predicates to source databases\n",
        "✓ Filter early (WHERE clauses)\n",
        "✓ Aggregate at source when possible\n",
        "✓ Use covering indexes on source databases\n",
        "✓ Materialize frequently-used joins\n",
        "✓ Partition S3 data appropriately\n",
        "\n",
        "Monitoring:\n",
        "───────────\n",
        "- Query execution time\n",
        "- Data volume transferred\n",
        "- Source database load\n",
        "- Cache hit rate\n",
        "- Cost per query\n",
        "\"\"\")\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 6\n",
        "import duckdb\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\"\"\n",
        "CDC Pattern com DuckDB:\n",
        "════════════════════════════════════════════════════════\n",
        "\n",
        "Tracking changes across multiple databases\n",
        "\"\"\")\n",
        "\n",
        "cdc_pattern = \"\"\"\n",
        "-- CDC tracking table\n",
        "CREATE TABLE IF NOT EXISTS cdc_watermarks (\n",
        "    source_database VARCHAR,\n",
        "    source_table VARCHAR,\n",
        "    last_sync_timestamp TIMESTAMP,\n",
        "    last_sync_id BIGINT,\n",
        "    records_synced BIGINT\n",
        ");\n",
        "\n",
        "-- CDC extraction from PostgreSQL\n",
        "CREATE OR REPLACE TEMP TABLE pg_changes AS\n",
        "SELECT\n",
        "    'postgres' as source_db,\n",
        "    'users' as source_table,\n",
        "    *\n",
        "FROM pg_users.users\n",
        "WHERE updated_at > (\n",
        "    SELECT COALESCE(MAX(last_sync_timestamp), '1970-01-01'::TIMESTAMP)\n",
        "    FROM cdc_watermarks\n",
        "    WHERE source_database = 'postgres'\n",
        "        AND source_table = 'users'\n",
        ");\n",
        "\n",
        "-- CDC extraction from MySQL\n",
        "CREATE OR REPLACE TEMP TABLE mysql_changes AS\n",
        "SELECT\n",
        "    'mysql' as source_db,\n",
        "    'orders' as source_table,\n",
        "    *\n",
        "FROM mysql_orders.orders\n",
        "WHERE updated_at > (\n",
        "    SELECT COALESCE(MAX(last_sync_timestamp), '1970-01-01'::TIMESTAMP)\n",
        "    FROM cdc_watermarks\n",
        "    WHERE source_database = 'mysql'\n",
        "        AND source_table = 'orders'\n",
        ");\n",
        "\n",
        "-- Merge changes to data lake\n",
        "COPY pg_changes\n",
        "TO 's3://datalake/cdc/users/incremental.parquet'\n",
        "(FORMAT PARQUET, APPEND true);\n",
        "\n",
        "COPY mysql_changes\n",
        "TO 's3://datalake/cdc/orders/incremental.parquet'\n",
        "(FORMAT PARQUET, APPEND true);\n",
        "\n",
        "-- Update watermarks\n",
        "INSERT INTO cdc_watermarks\n",
        "SELECT\n",
        "    'postgres' as source_database,\n",
        "    'users' as source_table,\n",
        "    MAX(updated_at) as last_sync_timestamp,\n",
        "    MAX(user_id) as last_sync_id,\n",
        "    COUNT(*) as records_synced\n",
        "FROM pg_changes\n",
        "UNION ALL\n",
        "SELECT\n",
        "    'mysql' as source_database,\n",
        "    'orders' as source_table,\n",
        "    MAX(updated_at) as last_sync_timestamp,\n",
        "    MAX(order_id) as last_sync_id,\n",
        "    COUNT(*) as records_synced\n",
        "FROM mysql_changes;\n",
        "\n",
        "-- Compaction: Merge incremental → full\n",
        "CREATE OR REPLACE TABLE users_full AS\n",
        "SELECT DISTINCT ON (user_id)\n",
        "    *\n",
        "FROM (\n",
        "    SELECT * FROM read_parquet('s3://datalake/cdc/users/*.parquet')\n",
        ")\n",
        "ORDER BY user_id, updated_at DESC;\n",
        "\n",
        "-- Export compacted\n",
        "COPY users_full\n",
        "TO 's3://datalake/full/users/snapshot.parquet'\n",
        "(FORMAT PARQUET, COMPRESSION 'zstd');\n",
        "\"\"\"\n",
        "\n",
        "print(cdc_pattern)\n",
        "\n",
        "print(\"\"\"\n",
        "CDC Best Practices:\n",
        "───────────────────\n",
        "✓ Use updated_at timestamps\n",
        "✓ Implement watermark tracking\n",
        "✓ Handle deletes (soft delete ou tombstones)\n",
        "✓ Periodic full snapshots\n",
        "✓ Compaction strategy\n",
        "✓ Idempotent processing\n",
        "✓ Monitoring e alerting\n",
        "\n",
        "Scheduling:\n",
        "───────────\n",
        "High-frequency: Every 5 minutes\n",
        "Standard: Hourly\n",
        "Low-frequency: Daily\n",
        "Compaction: Weekly\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 7\n",
        "import duckdb\n",
        "\n",
        "print(\"\"\"\n",
        "Performance Optimization Strategies:\n",
        "════════════════════════════════════════════════════════\n",
        "\"\"\")\n",
        "\n",
        "# Optimization examples\n",
        "optimizations = \"\"\"\n",
        "1. Predicate Pushdown\n",
        "   ──────────────────\n",
        "   ❌ Slow:\n",
        "   SELECT * FROM 's3://bucket/data/*.parquet'\n",
        "   WHERE date >= '2024-01-01';\n",
        "\n",
        "   ✓ Fast (Hive Partitioning):\n",
        "   SELECT * FROM 's3://bucket/data/year=2024/month=01/*.parquet';\n",
        "\n",
        "2. Projection Pushdown\n",
        "   ────────────────────\n",
        "   ❌ Slow:\n",
        "   SELECT user_id, amount\n",
        "   FROM 's3://bucket/wide_table.parquet';  -- Reads all columns\n",
        "\n",
        "   ✓ Fast:\n",
        "   SELECT user_id, amount\n",
        "   FROM read_parquet('s3://bucket/wide_table.parquet',\n",
        "                     columns=['user_id', 'amount']);\n",
        "\n",
        "3. Parallel Reads\n",
        "   ───────────────\n",
        "   ✓ Fast:\n",
        "   SELECT * FROM 's3://bucket/data/*.parquet';  -- Reads in parallel\n",
        "\n",
        "   Configure threads:\n",
        "   SET threads = 8;\n",
        "\n",
        "4. Compression\n",
        "   ────────────\n",
        "   ✓ Best for network I/O:\n",
        "   - zstd: Best compression ratio\n",
        "   - snappy: Fast compression/decompression\n",
        "   - gzip: Good balance\n",
        "\n",
        "   COPY (...) TO 's3://bucket/data.parquet'\n",
        "   (FORMAT PARQUET, COMPRESSION 'zstd');\n",
        "\n",
        "5. File Sizing\n",
        "   ────────────\n",
        "   ✓ Optimal: 256MB - 1GB per file\n",
        "   ❌ Too small: < 10MB (overhead)\n",
        "   ❌ Too large: > 5GB (memory pressure)\n",
        "\n",
        "6. Statistics\n",
        "   ───────────\n",
        "   ✓ Parquet row group statistics enable skipping\n",
        "\n",
        "7. Caching\n",
        "   ────────\n",
        "   -- Cache hot data\n",
        "   CREATE TEMP TABLE hot_data AS\n",
        "   SELECT * FROM 's3://bucket/frequently_accessed.parquet';\n",
        "\n",
        "8. Batch Operations\n",
        "   ─────────────────\n",
        "   ❌ Slow:\n",
        "   FOR user IN users:\n",
        "       SELECT * FROM orders WHERE user_id = user;\n",
        "\n",
        "   ✓ Fast:\n",
        "   SELECT * FROM orders WHERE user_id IN (SELECT user_id FROM users);\n",
        "\"\"\"\n",
        "\n",
        "print(optimizations)\n",
        "\n",
        "# Benchmark framework\n",
        "benchmark = \"\"\"\n",
        "-- Benchmark framework\n",
        "CREATE TABLE query_benchmarks (\n",
        "    query_id VARCHAR,\n",
        "    query_text VARCHAR,\n",
        "    execution_time_ms BIGINT,\n",
        "    rows_processed BIGINT,\n",
        "    timestamp TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Measure query\n",
        "CREATE OR REPLACE MACRO benchmark(query_id VARCHAR, query_text VARCHAR) AS (\n",
        "    -- Execute and measure\n",
        "    -- (DuckDB auto-timing in EXPLAIN ANALYZE)\n",
        "    EXPLAIN ANALYZE query_text\n",
        ");\n",
        "\n",
        "-- Example\n",
        "SELECT benchmark(\n",
        "    'query_1',\n",
        "    'SELECT * FROM s3://bucket/data.parquet WHERE date >= 2024-01-01'\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nBenchmarking:\")\n",
        "print(benchmark)\n",
        "\n",
        "con = duckdb.connect()\n",
        "print(\"\"\"\n",
        "Monitoring Metrics:\n",
        "───────────────────\n",
        "✓ Query execution time\n",
        "✓ Data scanned (GB)\n",
        "✓ Network throughput\n",
        "✓ Memory usage\n",
        "✓ CPU utilization\n",
        "✓ Cache hit rate\n",
        "✓ Cost per query\n",
        "\"\"\")\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 8\n",
        "print(\"\"\"\n",
        "Troubleshooting Guide:\n",
        "════════════════════════════════════════════════════════\n",
        "\n",
        "1. Authentication Errors\n",
        "   ─────────────────────\n",
        "   Error: \"Access Denied\" ou \"Authentication failed\"\n",
        "\n",
        "   Checklist:\n",
        "   ☐ Verify credentials are correct\n",
        "   ☐ Check secret is created: SELECT * FROM duckdb_secrets()\n",
        "   ☐ Verify SCOPE matches URL\n",
        "   ☐ Use which_secret() to debug\n",
        "   ☐ Check IAM permissions (AWS)\n",
        "   ☐ Verify service account (GCP)\n",
        "   ☐ Check Azure RBAC roles\n",
        "\n",
        "   Debug:\n",
        "   SELECT * FROM which_secret('s3://bucket/file.parquet', 's3');\n",
        "\n",
        "2. SSL/TLS Errors\n",
        "   ───────────────\n",
        "   Error: \"SSL verification failed\"\n",
        "\n",
        "   Solutions:\n",
        "   ☐ Verify USE_SSL = true for production\n",
        "   ☐ Check SSLMODE setting (postgres)\n",
        "   ☐ Verify certificate paths\n",
        "   ☐ Check certificate expiration\n",
        "   ☐ Validate hostname matches certificate\n",
        "\n",
        "   Debug:\n",
        "   -- Test sem SSL primeiro (apenas dev!)\n",
        "   CREATE SECRET test (\n",
        "       TYPE s3,\n",
        "       KEY_ID 'key',\n",
        "       SECRET 'secret',\n",
        "       USE_SSL false\n",
        "   );\n",
        "\n",
        "3. Extension Not Loaded\n",
        "   ─────────────────────\n",
        "   Error: \"Extension not loaded\"\n",
        "\n",
        "   Solutions:\n",
        "   ☐ INSTALL extension\n",
        "   ☐ LOAD extension\n",
        "   ☐ Check extension name (httpfs vs http_fs)\n",
        "   ☐ Verify installation succeeded\n",
        "\n",
        "   Debug:\n",
        "   SELECT * FROM duckdb_extensions();\n",
        "\n",
        "4. Timeout Errors\n",
        "   ───────────────\n",
        "   Error: \"Connection timeout\" ou \"Read timeout\"\n",
        "\n",
        "   Solutions:\n",
        "   ☐ Increase TIMEOUT parameter\n",
        "   ☐ Check network connectivity\n",
        "   ☐ Verify firewall rules\n",
        "   ☐ Check server load\n",
        "   ☐ Reduce query complexity\n",
        "\n",
        "   Debug:\n",
        "   CREATE SECRET s3_longer_timeout (\n",
        "       TYPE s3,\n",
        "       KEY_ID 'key',\n",
        "       SECRET 'secret',\n",
        "       TIMEOUT 120000  -- 2 minutos\n",
        "   );\n",
        "\n",
        "5. File Not Found\n",
        "   ───────────────\n",
        "   Error: \"File not found\" ou \"No such key\"\n",
        "\n",
        "   Checklist:\n",
        "   ☐ Verify file path is correct\n",
        "   ☐ Check bucket/container name\n",
        "   ☐ Verify permissions (read access)\n",
        "   ☐ Check file actually exists\n",
        "   ☐ Case sensitivity (S3 is case-sensitive)\n",
        "\n",
        "   Debug:\n",
        "   -- List files\n",
        "   SELECT * FROM read_parquet('s3://bucket/path/*.parquet');\n",
        "\n",
        "6. Memory Errors\n",
        "   ──────────────\n",
        "   Error: \"Out of memory\"\n",
        "\n",
        "   Solutions:\n",
        "   ☐ Reduce data volume (add filters)\n",
        "   ☐ Use streaming: read_parquet_auto()\n",
        "   ☐ Process in batches\n",
        "   ☐ Increase available memory\n",
        "   ☐ Use projection pushdown\n",
        "\n",
        "   Debug:\n",
        "   SET memory_limit = '8GB';\n",
        "   SET temp_directory = '/path/to/large/disk';\n",
        "\n",
        "7. Credential Chain Failures\n",
        "   ──────────────────────────\n",
        "   Error: \"No credentials found in chain\"\n",
        "\n",
        "   Checklist:\n",
        "   ☐ Verify environment variables set\n",
        "   ☐ Check ~/.aws/credentials (AWS)\n",
        "   ☐ Verify gcloud auth (GCP)\n",
        "   ☐ Check Azure CLI auth\n",
        "   ☐ Verify IAM role attached (EC2)\n",
        "\n",
        "   Debug:\n",
        "   -- Test each chain method individually\n",
        "   CREATE SECRET s3_env_only (\n",
        "       TYPE s3,\n",
        "       PROVIDER credential_chain,\n",
        "       CHAIN 'env'\n",
        "   );\n",
        "\n",
        "8. Persistent Secret Not Loading\n",
        "   ──────────────────────────────\n",
        "   Error: \"Secret not found after restart\"\n",
        "\n",
        "   Checklist:\n",
        "   ☐ Verify used CREATE PERSISTENT SECRET\n",
        "   ☐ Check secret_directory setting\n",
        "   ☐ Verify file permissions\n",
        "   ☐ Check disk space\n",
        "   ☐ Verify database path\n",
        "\n",
        "   Debug:\n",
        "   SELECT name, persistent, storage\n",
        "   FROM duckdb_secrets();\n",
        "\n",
        "9. Slow Queries\n",
        "   ─────────────\n",
        "   Issue: Query takes too long\n",
        "\n",
        "   Solutions:\n",
        "   ☐ Use EXPLAIN ANALYZE\n",
        "   ☐ Check partitioning\n",
        "   ☐ Verify predicate pushdown\n",
        "   ☐ Add WHERE filters early\n",
        "   ☐ Use covering projections\n",
        "   ☐ Check file sizes\n",
        "   ☐ Verify compression\n",
        "\n",
        "   Debug:\n",
        "   EXPLAIN ANALYZE\n",
        "   SELECT * FROM 's3://bucket/data.parquet'\n",
        "   WHERE date >= '2024-01-01';\n",
        "\n",
        "10. Cross-Database Join Issues\n",
        "    ───────────────────────────\n",
        "    Issue: Joins between databases slow/failing\n",
        "\n",
        "    Solutions:\n",
        "    ☐ Materialize smaller table locally\n",
        "    ☐ Use temp tables for intermediate results\n",
        "    ☐ Optimize join order\n",
        "    ☐ Add indexes on source databases\n",
        "    ☐ Consider denormalization\n",
        "\n",
        "    Debug:\n",
        "    CREATE TEMP TABLE small_table AS\n",
        "    SELECT * FROM mysql_db.small_dimension;\n",
        "\n",
        "    SELECT *\n",
        "    FROM large_s3_fact f\n",
        "    JOIN small_table d ON f.id = d.id;\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 9\n",
        "import duckdb\n",
        "\n",
        "print(\"\"\"\n",
        "Diagnostic Queries:\n",
        "════════════════════════════════════════════════════════\n",
        "\"\"\")\n",
        "\n",
        "con = duckdb.connect()\n",
        "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
        "\n",
        "diagnostics = \"\"\"\n",
        "-- 1. List all secrets\n",
        "SELECT\n",
        "    name,\n",
        "    type,\n",
        "    provider,\n",
        "    scope,\n",
        "    persistent,\n",
        "    storage\n",
        "FROM duckdb_secrets()\n",
        "ORDER BY type, name;\n",
        "\n",
        "-- 2. Check which secret will be used\n",
        "SELECT *\n",
        "FROM which_secret('s3://my-bucket/file.parquet', 's3');\n",
        "\n",
        "-- 3. List installed extensions\n",
        "SELECT\n",
        "    extension_name,\n",
        "    installed,\n",
        "    loaded\n",
        "FROM duckdb_extensions()\n",
        "WHERE extension_name IN ('httpfs', 'azure', 'mysql_scanner', 'postgres_scanner');\n",
        "\n",
        "-- 4. Check DuckDB settings\n",
        "SELECT *\n",
        "FROM duckdb_settings()\n",
        "WHERE name IN (\n",
        "    'secret_directory',\n",
        "    'threads',\n",
        "    'memory_limit',\n",
        "    'temp_directory'\n",
        ");\n",
        "\n",
        "-- 5. View active connections (attached databases)\n",
        "SELECT *\n",
        "FROM duckdb_databases();\n",
        "\n",
        "-- 6. Memory usage\n",
        "SELECT *\n",
        "FROM pragma_database_size();\n",
        "\n",
        "-- 7. Table information\n",
        "SELECT *\n",
        "FROM information_schema.tables;\n",
        "\"\"\"\n",
        "\n",
        "print(diagnostics)\n",
        "\n",
        "print(\"\"\"\n",
        "Logging e Monitoring:\n",
        "─────────────────────\n",
        "\n",
        "Python logging:\n",
        "\"\"\")\n",
        "\n",
        "logging_example = \"\"\"\n",
        "import logging\n",
        "import duckdb\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    filename='duckdb_secrets.log'\n",
        ")\n",
        "\n",
        "logger = logging.getLogger('duckdb_secrets')\n",
        "\n",
        "try:\n",
        "    con = duckdb.connect()\n",
        "    logger.info(\"Connection established\")\n",
        "\n",
        "    con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
        "    logger.info(\"httpfs extension loaded\")\n",
        "\n",
        "    con.execute(\\\"\\\"\\\"\n",
        "        CREATE SECRET s3_prod (\n",
        "            TYPE s3,\n",
        "            KEY_ID 'key',\n",
        "            SECRET 'secret'\n",
        "        )\n",
        "    \\\"\\\"\\\")\n",
        "    logger.info(\"Secret 's3_prod' created\")\n",
        "\n",
        "    result = con.execute(\"SELECT * FROM 's3://bucket/file.parquet'\").df()\n",
        "    logger.info(f\"Query completed: {len(result)} rows\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error: {e}\", exc_info=True)\n",
        "finally:\n",
        "    con.close()\n",
        "    logger.info(\"Connection closed\")\n",
        "\"\"\"\n",
        "\n",
        "print(logging_example)\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 10\n",
        "print(\"\"\"\n",
        "Docker Deployment:\n",
        "════════════════════════════════════════════════════════\n",
        "\"\"\")\n",
        "\n",
        "dockerfile = \"\"\"\n",
        "# Dockerfile\n",
        "FROM python:3.11-slim\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Install DuckDB\n",
        "RUN pip install --no-cache-dir duckdb\n",
        "\n",
        "# Install cloud SDKs\n",
        "RUN pip install boto3 azure-identity azure-storage-blob google-cloud-storage\n",
        "\n",
        "# Copy application\n",
        "COPY app/ /app/\n",
        "\n",
        "# Environment variables (overridden at runtime)\n",
        "ENV AWS_ACCESS_KEY_ID=\"\"\n",
        "ENV AWS_SECRET_ACCESS_KEY=\"\"\n",
        "ENV AZURE_TENANT_ID=\"\"\n",
        "ENV AZURE_CLIENT_ID=\"\"\n",
        "ENV AZURE_CLIENT_SECRET=\"\"\n",
        "\n",
        "# Run\n",
        "CMD [\"python\", \"main.py\"]\n",
        "\"\"\"\n",
        "\n",
        "docker_compose = \"\"\"\n",
        "# docker-compose.yml\n",
        "version: '3.8'\n",
        "\n",
        "services:\n",
        "  duckdb-app:\n",
        "    build: .\n",
        "    environment:\n",
        "      # Load from .env file\n",
        "      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n",
        "      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n",
        "      - GCP_PROJECT_ID=${GCP_PROJECT_ID}\n",
        "      - AZURE_TENANT_ID=${AZURE_TENANT_ID}\n",
        "      - AZURE_CLIENT_ID=${AZURE_CLIENT_ID}\n",
        "      - AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}\n",
        "    volumes:\n",
        "      # Persistent secrets\n",
        "      - ./secrets:/app/secrets:ro\n",
        "      # DuckDB data\n",
        "      - ./data:/app/data\n",
        "    restart: unless-stopped\n",
        "\"\"\"\n",
        "\n",
        "print(\"Dockerfile:\")\n",
        "print(dockerfile)\n",
        "\n",
        "print(\"\\ndocker-compose.yml:\")\n",
        "print(docker_compose)\n",
        "\n",
        "kubernetes = \"\"\"\n",
        "# Kubernetes Deployment\n",
        "apiVersion: v1\n",
        "kind: Secret\n",
        "metadata:\n",
        "  name: duckdb-secrets\n",
        "type: Opaque\n",
        "stringData:\n",
        "  aws-access-key-id: \"AKIAIOSFODNN7EXAMPLE\"\n",
        "  aws-secret-access-key: \"wJalrXUtnFEMI...\"\n",
        "\n",
        "---\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: duckdb-app\n",
        "spec:\n",
        "  replicas: 3\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: duckdb-app\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: duckdb-app\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: duckdb-app\n",
        "        image: mycompany/duckdb-app:latest\n",
        "        env:\n",
        "        - name: AWS_ACCESS_KEY_ID\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: duckdb-secrets\n",
        "              key: aws-access-key-id\n",
        "        - name: AWS_SECRET_ACCESS_KEY\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: duckdb-secrets\n",
        "              key: aws-secret-access-key\n",
        "        resources:\n",
        "          limits:\n",
        "            memory: \"4Gi\"\n",
        "            cpu: \"2000m\"\n",
        "        volumeMounts:\n",
        "        - name: data\n",
        "          mountPath: /app/data\n",
        "      volumes:\n",
        "      - name: data\n",
        "        persistentVolumeClaim:\n",
        "          claimName: duckdb-data-pvc\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nKubernetes:\")\n",
        "print(kubernetes)\n",
        "\n",
        "print(\"\"\"\n",
        "Best Practices:\n",
        "───────────────\n",
        "✓ Use secrets managers (não env vars em prod)\n",
        "✓ Read-only volume mounts para secrets\n",
        "✓ Health checks\n",
        "✓ Resource limits\n",
        "✓ Logging to stdout/stderr\n",
        "✓ Graceful shutdown\n",
        "✓ Horizontal scaling considerations\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 11\n",
        "print(\"\"\"\n",
        "════════════════════════════════════════════════════════\n",
        "            CURSO COMPLETO - RESUMO FINAL\n",
        "════════════════════════════════════════════════════════\n",
        "\n",
        "Capítulo 1: Introdução\n",
        "───────────────────────\n",
        "✓ O que são secrets\n",
        "✓ CREATE/DROP SECRET\n",
        "✓ duckdb_secrets()\n",
        "✓ Temporary vs Persistent\n",
        "\n",
        "Capítulo 2: Tipos de Secrets\n",
        "─────────────────────────────\n",
        "✓ S3, R2, GCS, Azure\n",
        "✓ MySQL, PostgreSQL\n",
        "✓ HTTP, Hugging Face\n",
        "\n",
        "Capítulo 3: Cloud Storage\n",
        "──────────────────────────\n",
        "✓ S3 completo (URL styles, regions)\n",
        "✓ Azure (connection string, managed identity)\n",
        "✓ GCS (service accounts, ADC)\n",
        "✓ R2 (Cloudflare)\n",
        "\n",
        "Capítulo 4: Database Secrets\n",
        "─────────────────────────────\n",
        "✓ MySQL (SSL, connection strings)\n",
        "✓ PostgreSQL (ATTACH, SSL)\n",
        "✓ Cross-database queries\n",
        "✓ ETL patterns\n",
        "\n",
        "Capítulo 5: Persistent Secrets\n",
        "───────────────────────────────\n",
        "✓ Temporary vs Persistent\n",
        "✓ secret_directory\n",
        "✓ Backup e restore\n",
        "✓ File permissions\n",
        "\n",
        "Capítulo 6: Providers\n",
        "─────────────────────\n",
        "✓ config, credential_chain\n",
        "✓ managed_identity\n",
        "✓ service_principal\n",
        "✓ Por ambiente\n",
        "\n",
        "Capítulo 7: SCOPE e Named Secrets\n",
        "──────────────────────────────────\n",
        "✓ SCOPE hierarchy\n",
        "✓ which_secret()\n",
        "✓ Múltiplas credenciais\n",
        "✓ Naming conventions\n",
        "\n",
        "Capítulo 8: Extensions\n",
        "──────────────────────\n",
        "✓ httpfs (S3, GCS, HTTP)\n",
        "✓ azure (Blob Storage)\n",
        "✓ mysql_scanner, postgres_scanner\n",
        "✓ Extension management\n",
        "\n",
        "Capítulo 9: Segurança\n",
        "─────────────────────\n",
        "✓ Evitar exposição de credenciais\n",
        "✓ Rotação de secrets\n",
        "✓ SSL/TLS configuration\n",
        "✓ Auditoria e logging\n",
        "✓ Compliance\n",
        "\n",
        "Capítulo 10: Casos de Uso Avançados\n",
        "────────────────────────────────────\n",
        "✓ ETL multi-cloud\n",
        "✓ Medallion architecture\n",
        "✓ Database federation\n",
        "✓ CDC patterns\n",
        "✓ Performance optimization\n",
        "✓ Troubleshooting\n",
        "✓ Production deployment\n",
        "\n",
        "Próximos Passos:\n",
        "────────────────\n",
        "1. Implementar secrets em seus projetos\n",
        "2. Configurar CI/CD com secrets\n",
        "3. Estabelecer rotação policy\n",
        "4. Setup monitoring e alerting\n",
        "5. Documentar sua arquitetura\n",
        "6. Treinar seu time\n",
        "7. Regular security reviews\n",
        "\n",
        "Recursos Adicionais:\n",
        "────────────────────\n",
        "- DuckDB Documentation: https://duckdb.org/docs\n",
        "- GitHub Issues: https://github.com/duckdb/duckdb\n",
        "- Discord Community: https://discord.duckdb.org\n",
        "- Stack Overflow: tag [duckdb]\n",
        "\n",
        "Lembre-se:\n",
        "──────────\n",
        "\"Security is not a product, but a process\" - Bruce Schneier\n",
        "\n",
        "✓ Secrets são poderosos, mas requerem cuidado\n",
        "✓ Sempre seguir security best practices\n",
        "✓ Documentar decisões e configurações\n",
        "✓ Automatizar quando possível\n",
        "✓ Monitorar continuamente\n",
        "✓ Estar preparado para incidentes\n",
        "\n",
        "Boa sorte com seus projetos DuckDB! 🦆\n",
        "════════════════════════════════════════════════════════\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo/Bloco 12\n",
        "\"\"\"\n",
        "Projeto Final: Multi-Cloud Data Platform\n",
        "═══════════════════════════════════════════════════════\n",
        "\n",
        "Objetivo:\n",
        "─────────\n",
        "Implementar plataforma completa de dados usando DuckDB Secrets\n",
        "\n",
        "Requisitos:\n",
        "───────────\n",
        "1. Multi-Cloud Setup\n",
        "   - AWS S3 (raw data)\n",
        "   - GCS (processed data)\n",
        "   - Azure (analytics)\n",
        "\n",
        "2. Database Integration\n",
        "   - PostgreSQL (users)\n",
        "   - MySQL (transactions)\n",
        "\n",
        "3. Security\n",
        "   - Credential chain\n",
        "   - SSL/TLS everywhere\n",
        "   - Rotação automática\n",
        "   - Audit logging\n",
        "\n",
        "4. Data Pipeline\n",
        "   - Incremental ETL\n",
        "   - Data quality checks\n",
        "   - Partitioning strategy\n",
        "\n",
        "5. Monitoring\n",
        "   - Query performance\n",
        "   - Secret usage\n",
        "   - Error tracking\n",
        "   - Cost monitoring\n",
        "\n",
        "6. Documentation\n",
        "   - Architecture diagram\n",
        "   - Secret inventory\n",
        "   - Runbooks\n",
        "   - Troubleshooting guide\n",
        "\n",
        "Entregáveis:\n",
        "────────────\n",
        "☐ Código Python completo\n",
        "☐ Configuração de secrets\n",
        "☐ CI/CD pipeline\n",
        "☐ Docker/Kubernetes deployment\n",
        "☐ Monitoring dashboards\n",
        "☐ Documentação completa\n",
        "☐ Testes automatizados\n",
        "\n",
        "Tempo Estimado: 2-3 dias\n",
        "\n",
        "Boa sorte! 🚀\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}