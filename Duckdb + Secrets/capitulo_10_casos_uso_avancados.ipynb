{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InstalaÃ§Ã£o de pacotes necessÃ¡rios\n",
    "!pip install -q duckdb\n",
    "\n",
    "print(\"âœ… DependÃªncias instaladas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b502458",
   "metadata": {},
   "source": [
    "## ğŸ“¦ InstalaÃ§Ã£o de DependÃªncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 10 Casos Uso Avancados\n",
    "\n",
    "Notebook gerado automaticamente a partir do cÃ³digo fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_10_casos_uso_avancados\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_10_casos_uso_avancados\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Caso de Uso: Multi-Cloud ETL Pipeline\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CenÃ¡rio:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Dados source em AWS S3\n",
    "- Processing intermediÃ¡rio em GCP\n",
    "- Resultado final em Azure\n",
    "- Tudo orquestrado via DuckDB\n",
    "\n",
    "Arquitetura:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AWS S3 (Raw) â†’ DuckDB â†’ GCS (Processed) â†’ DuckDB â†’ Azure (Final)\n",
    "\"\"\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(\"INSTALL azure; LOAD azure;\")\n",
    "\n",
    "# Configure secrets\n",
    "print(\"\\n1. Configurando secrets...\")\n",
    "\n",
    "# AWS S3 (source)\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET s3_source (\n",
    "        TYPE s3,\n",
    "        PROVIDER credential_chain,\n",
    "        CHAIN 'env;config',\n",
    "        SCOPE 's3://company-raw-data/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# GCP (intermediate)\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET gcs_processing (\n",
    "        TYPE gcs,\n",
    "        PROVIDER credential_chain,\n",
    "        SCOPE 'gs://company-processing/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Azure (destination)\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET azure_final (\n",
    "        TYPE azure,\n",
    "        PROVIDER managed_identity,\n",
    "        ACCOUNT_NAME 'companyfinal',\n",
    "        SCOPE 'azure://final-data/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ“ Secrets configurados\")\n",
    "\n",
    "# ETL Pipeline\n",
    "etl_pipeline = \"\"\"\n",
    "-- Step 1: Extract from AWS S3\n",
    "CREATE TEMP TABLE raw_events AS\n",
    "SELECT\n",
    "    event_id,\n",
    "    user_id,\n",
    "    event_type,\n",
    "    timestamp,\n",
    "    properties\n",
    "FROM read_parquet('s3://company-raw-data/events/2024/01/*.parquet');\n",
    "\n",
    "-- Step 2: Transform\n",
    "CREATE TEMP TABLE processed_events AS\n",
    "SELECT\n",
    "    event_id,\n",
    "    user_id,\n",
    "    event_type,\n",
    "    DATE_TRUNC('hour', timestamp) as hour,\n",
    "    COUNT(*) OVER (PARTITION BY user_id) as user_event_count,\n",
    "    JSON_EXTRACT_STRING(properties, '$.country') as country,\n",
    "    JSON_EXTRACT_STRING(properties, '$.device') as device\n",
    "FROM raw_events\n",
    "WHERE timestamp >= '2024-01-01'\n",
    "    AND event_type IN ('page_view', 'click', 'purchase');\n",
    "\n",
    "-- Step 3: Load to GCS (intermediate)\n",
    "COPY processed_events\n",
    "TO 'gs://company-processing/events/processed.parquet'\n",
    "(FORMAT PARQUET, PARTITION_BY (country, hour));\n",
    "\n",
    "-- Step 4: Aggregations\n",
    "CREATE TEMP TABLE aggregated_metrics AS\n",
    "SELECT\n",
    "    hour,\n",
    "    country,\n",
    "    device,\n",
    "    event_type,\n",
    "    COUNT(*) as event_count,\n",
    "    COUNT(DISTINCT user_id) as unique_users\n",
    "FROM processed_events\n",
    "GROUP BY 1, 2, 3, 4;\n",
    "\n",
    "-- Step 5: Load to Azure (final)\n",
    "COPY aggregated_metrics\n",
    "TO 'azure://final-data/metrics/daily_metrics.parquet'\n",
    "(FORMAT PARQUET, COMPRESSION 'zstd');\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n2. ETL Pipeline:\")\n",
    "print(etl_pipeline)\n",
    "\n",
    "print(\"\"\"\n",
    "Vantagens Multi-Cloud:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Vendor lock-in mitigation\n",
    "âœ“ Cost optimization por workload\n",
    "âœ“ Geographic data residency\n",
    "âœ“ Best-of-breed services\n",
    "âœ“ Disaster recovery\n",
    "\n",
    "Desafios:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ— Egress costs (exceto R2)\n",
    "âœ— Complexidade de gestÃ£o\n",
    "âœ— Multiple authentication systems\n",
    "âœ— Data consistency\n",
    "âœ— LatÃªncia cross-cloud\n",
    "\n",
    "Best Practices:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Minimize data movement\n",
    "âœ“ Use compression (zstd)\n",
    "âœ“ Partition appropriately\n",
    "âœ“ Monitor costs\n",
    "âœ“ Implement retry logic\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 2\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"\"\"\n",
    "Incremental Multi-Cloud Sync:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "def incremental_multicloud_sync(source_secret, dest_secret, source_path, dest_path):\n",
    "    \"\"\"\n",
    "    Sync incremental entre clouds\n",
    "    \"\"\"\n",
    "    con = duckdb.connect('sync.duckdb')\n",
    "    con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Incremental Sync\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Source: {source_path}\")\n",
    "    print(f\"Dest: {dest_path}\")\n",
    "\n",
    "    # 1. Obter Ãºltimo sync\n",
    "    try:\n",
    "        last_sync = con.execute(\"\"\"\n",
    "            SELECT MAX(sync_timestamp) as last_sync\n",
    "            FROM sync_metadata\n",
    "            WHERE source_path = ?\n",
    "        \"\"\", [source_path]).fetchone()[0]\n",
    "    except:\n",
    "        # Primeira sync\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS sync_metadata (\n",
    "                source_path VARCHAR,\n",
    "                dest_path VARCHAR,\n",
    "                sync_timestamp TIMESTAMP,\n",
    "                records_synced BIGINT\n",
    "            )\n",
    "        \"\"\")\n",
    "        last_sync = datetime(2020, 1, 1)\n",
    "\n",
    "    print(f\"\\n1. Last sync: {last_sync}\")\n",
    "\n",
    "    # 2. Read incremental data\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM read_parquet('{source_path}')\n",
    "        WHERE updated_at > '{last_sync}'\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"2. Reading incremental data...\")\n",
    "    incremental_data = con.execute(query)\n",
    "\n",
    "    # 3. Write to destination\n",
    "    print(f\"3. Writing to {dest_path}...\")\n",
    "    con.execute(f\"\"\"\n",
    "        COPY (\n",
    "            {query}\n",
    "        ) TO '{dest_path}'\n",
    "        (FORMAT PARQUET, COMPRESSION 'snappy', APPEND true)\n",
    "    \"\"\")\n",
    "\n",
    "    # 4. Update metadata\n",
    "    records_count = con.execute(f\"SELECT COUNT(*) FROM ({query})\").fetchone()[0]\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO sync_metadata\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    \"\"\", [source_path, dest_path, datetime.now(), records_count])\n",
    "\n",
    "    print(f\"4. Synced {records_count} records\")\n",
    "    print(f\"âœ“ Sync completed!\")\n",
    "\n",
    "    con.close()\n",
    "\n",
    "# DocumentaÃ§Ã£o\n",
    "print(\"\"\"\n",
    "ImplementaÃ§Ã£o:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Schedule com cron ou Airflow\n",
    "0 * * * * python incremental_sync.py  # A cada hora\n",
    "\n",
    "# Ou via Airflow DAG\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(\n",
    "    'multicloud_sync',\n",
    "    schedule_interval='@hourly',\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "sync_task = PythonOperator(\n",
    "    task_id='incremental_sync',\n",
    "    python_callable=incremental_multicloud_sync,\n",
    "    op_kwargs={\n",
    "        'source_secret': 's3_source',\n",
    "        'dest_secret': 'azure_dest',\n",
    "        'source_path': 's3://source/data/*.parquet',\n",
    "        'dest_path': 'azure://dest/data/'\n",
    "    },\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "Monitoramento:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Records synced per run\n",
    "- Sync duration\n",
    "- Error rate\n",
    "- Data lag\n",
    "- Cost per sync\n",
    "\"\"\")\n",
    "\n",
    "# Exemplo/Bloco 3\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Medallion Architecture com DuckDB:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Bronze â†’ Silver â†’ Gold\n",
    "\n",
    "Bronze: Raw data (S3)\n",
    "Silver: Cleaned, validated (GCS)\n",
    "Gold: Business aggregates (Azure)\n",
    "\"\"\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(\"INSTALL azure; LOAD azure;\")\n",
    "\n",
    "# Configure secrets por layer\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET bronze_s3 (\n",
    "        TYPE s3,\n",
    "        PROVIDER credential_chain,\n",
    "        SCOPE 's3://datalake-bronze/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET silver_gcs (\n",
    "        TYPE gcs,\n",
    "        PROVIDER credential_chain,\n",
    "        SCOPE 'gs://datalake-silver/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET gold_azure (\n",
    "        TYPE azure,\n",
    "        PROVIDER service_principal,\n",
    "        TENANT_ID 'tenant-id',\n",
    "        CLIENT_ID 'client-id',\n",
    "        CLIENT_SECRET 'client-secret',\n",
    "        ACCOUNT_NAME 'datalakegold',\n",
    "        SCOPE 'azure://gold/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Bronze â†’ Silver\n",
    "bronze_to_silver = \"\"\"\n",
    "-- Bronze Layer: Raw events from S3\n",
    "CREATE OR REPLACE TEMP TABLE bronze_events AS\n",
    "SELECT *\n",
    "FROM read_parquet('s3://datalake-bronze/events/year=*/month=*/day=*/*.parquet');\n",
    "\n",
    "-- Silver Layer: Cleaned and validated\n",
    "CREATE OR REPLACE TEMP TABLE silver_events AS\n",
    "SELECT\n",
    "    event_id,\n",
    "    user_id,\n",
    "    -- Data quality: remove nulls\n",
    "    COALESCE(event_type, 'unknown') as event_type,\n",
    "    -- Data quality: valid timestamps only\n",
    "    CASE\n",
    "        WHEN timestamp >= '2020-01-01'\n",
    "         AND timestamp <= CURRENT_TIMESTAMP\n",
    "        THEN timestamp\n",
    "        ELSE NULL\n",
    "    END as timestamp,\n",
    "    -- Data quality: validate JSON\n",
    "    CASE\n",
    "        WHEN JSON_VALID(properties)\n",
    "        THEN properties\n",
    "        ELSE NULL\n",
    "    END as properties,\n",
    "    CURRENT_TIMESTAMP as processed_at\n",
    "FROM bronze_events\n",
    "WHERE user_id IS NOT NULL  -- Data quality rule\n",
    "    AND event_id IS NOT NULL;\n",
    "\n",
    "-- Write to Silver (GCS)\n",
    "COPY silver_events\n",
    "TO 'gs://datalake-silver/events/'\n",
    "(FORMAT PARQUET, PARTITION_BY (DATE_TRUNC('day', timestamp)));\n",
    "\"\"\"\n",
    "\n",
    "# Silver â†’ Gold\n",
    "silver_to_gold = \"\"\"\n",
    "-- Silver Layer: Read cleaned data\n",
    "CREATE OR REPLACE TEMP TABLE silver_events AS\n",
    "SELECT *\n",
    "FROM read_parquet('gs://datalake-silver/events/year=*/month=*/day=*/*.parquet');\n",
    "\n",
    "-- Gold Layer: Business metrics\n",
    "CREATE OR REPLACE TEMP TABLE gold_user_metrics AS\n",
    "SELECT\n",
    "    user_id,\n",
    "    DATE_TRUNC('day', timestamp) as date,\n",
    "    COUNT(*) as total_events,\n",
    "    COUNT(DISTINCT event_type) as unique_event_types,\n",
    "    MIN(timestamp) as first_event,\n",
    "    MAX(timestamp) as last_event,\n",
    "    COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) as purchases,\n",
    "    SUM(CAST(JSON_EXTRACT(properties, '$.amount') AS DOUBLE)) as total_amount\n",
    "FROM silver_events\n",
    "WHERE timestamp IS NOT NULL\n",
    "GROUP BY user_id, DATE_TRUNC('day', timestamp);\n",
    "\n",
    "-- Write to Gold (Azure)\n",
    "COPY gold_user_metrics\n",
    "TO 'azure://gold/metrics/user_daily_metrics/'\n",
    "(FORMAT PARQUET, PARTITION_BY (date));\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n1. Bronze â†’ Silver:\")\n",
    "print(bronze_to_silver)\n",
    "\n",
    "print(\"\\n2. Silver â†’ Gold:\")\n",
    "print(silver_to_gold)\n",
    "\n",
    "print(\"\"\"\n",
    "Data Quality Rules:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Bronze:\n",
    "âœ“ Schema enforcement\n",
    "âœ“ Duplicate detection\n",
    "âœ“ Format validation\n",
    "\n",
    "Silver:\n",
    "âœ“ Null handling\n",
    "âœ“ Type validation\n",
    "âœ“ Referential integrity\n",
    "âœ“ Business rule validation\n",
    "âœ“ Deduplication\n",
    "\n",
    "Gold:\n",
    "âœ“ Aggregation accuracy\n",
    "âœ“ Metric definitions\n",
    "âœ“ SLA compliance\n",
    "âœ“ Data freshness\n",
    "\n",
    "Orchestration:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Bronze ingestion: Real-time ou batch\n",
    "2. Silver processing: Hourly\n",
    "3. Gold aggregation: Daily\n",
    "4. Retention: Bronze (30d), Silver (1y), Gold (7y)\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 4\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Lambda Architecture com DuckDB:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Batch Layer + Speed Layer â†’ Serving Layer\n",
    "\n",
    "Batch Layer: Complete historical data (S3)\n",
    "Speed Layer: Recent data, incremental (GCS)\n",
    "Serving Layer: Merged views (Azure)\n",
    "\"\"\")\n",
    "\n",
    "lambda_arch = \"\"\"\n",
    "-- Batch Layer: Historical aggregations\n",
    "CREATE OR REPLACE VIEW batch_metrics AS\n",
    "SELECT\n",
    "    user_id,\n",
    "    DATE_TRUNC('day', timestamp) as date,\n",
    "    COUNT(*) as event_count,\n",
    "    SUM(amount) as total_amount\n",
    "FROM read_parquet('s3://datalake/batch/events/**/*.parquet')\n",
    "WHERE timestamp < CURRENT_DATE\n",
    "GROUP BY 1, 2;\n",
    "\n",
    "-- Speed Layer: Real-time incremental\n",
    "CREATE OR REPLACE VIEW speed_metrics AS\n",
    "SELECT\n",
    "    user_id,\n",
    "    DATE_TRUNC('day', timestamp) as date,\n",
    "    COUNT(*) as event_count,\n",
    "    SUM(amount) as total_amount\n",
    "FROM read_parquet('gs://datalake/speed/events/**/*.parquet')\n",
    "WHERE timestamp >= CURRENT_DATE\n",
    "GROUP BY 1, 2;\n",
    "\n",
    "-- Serving Layer: Merged view\n",
    "CREATE OR REPLACE VIEW serving_metrics AS\n",
    "SELECT\n",
    "    COALESCE(b.user_id, s.user_id) as user_id,\n",
    "    COALESCE(b.date, s.date) as date,\n",
    "    COALESCE(b.event_count, 0) + COALESCE(s.event_count, 0) as event_count,\n",
    "    COALESCE(b.total_amount, 0) + COALESCE(s.total_amount, 0) as total_amount\n",
    "FROM batch_metrics b\n",
    "FULL OUTER JOIN speed_metrics s\n",
    "    ON b.user_id = s.user_id\n",
    "    AND b.date = s.date;\n",
    "\n",
    "-- Materialize to serving layer\n",
    "COPY serving_metrics\n",
    "TO 'azure://serving/metrics/user_metrics.parquet'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\"\n",
    "\n",
    "print(lambda_arch)\n",
    "\n",
    "print(\"\"\"\n",
    "Vantagens Lambda:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Real-time + historical data\n",
    "âœ“ Fault tolerance\n",
    "âœ“ Scalability\n",
    "âœ“ Reprocessing capability\n",
    "\n",
    "DuckDB Role:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Query engine para batch layer\n",
    "âœ“ Merge de batch + speed layers\n",
    "âœ“ Serving layer queries\n",
    "âœ“ Data quality checks\n",
    "\n",
    "Alternativa: Kappa Architecture\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Use apenas streaming (sem batch layer)\n",
    "DuckDB queries real-time stream storage\n",
    "\"\"\")\n",
    "\n",
    "# Exemplo/Bloco 5\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Database Federation com DuckDB:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CenÃ¡rio:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Users em PostgreSQL\n",
    "- Orders em MySQL\n",
    "- Products em S3 Parquet\n",
    "- Analytics em DuckDB\n",
    "\"\"\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(\"INSTALL postgres_scanner; LOAD postgres_scanner;\")\n",
    "con.execute(\"INSTALL mysql_scanner; LOAD mysql_scanner;\")\n",
    "\n",
    "# Configure secrets\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET postgres_users (\n",
    "        TYPE postgres,\n",
    "        HOST 'postgres.example.com',\n",
    "        DATABASE 'users_db',\n",
    "        USER 'readonly',\n",
    "        PASSWORD 'password',\n",
    "        SSLMODE 'verify-full'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET mysql_orders (\n",
    "        TYPE mysql,\n",
    "        HOST 'mysql.example.com',\n",
    "        DATABASE 'orders_db',\n",
    "        USER 'readonly',\n",
    "        PASSWORD 'password',\n",
    "        SSL_MODE 'REQUIRED'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE SECRET s3_products (\n",
    "        TYPE s3,\n",
    "        PROVIDER credential_chain,\n",
    "        SCOPE 's3://company-data/products/'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# ATTACH databases\n",
    "con.execute(\"\"\"\n",
    "    ATTACH 'postgres:users_db' AS pg_users (\n",
    "        TYPE postgres,\n",
    "        SECRET postgres_users\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    ATTACH 'mysql:orders_db' AS mysql_orders (\n",
    "        TYPE mysql,\n",
    "        SECRET mysql_orders\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Federation query\n",
    "federation_query = \"\"\"\n",
    "-- Customer 360 View: Data de 3 fontes diferentes\n",
    "CREATE OR REPLACE VIEW customer_360 AS\n",
    "SELECT\n",
    "    -- PostgreSQL: User data\n",
    "    u.user_id,\n",
    "    u.email,\n",
    "    u.name,\n",
    "    u.country,\n",
    "    u.created_at as user_since,\n",
    "\n",
    "    -- MySQL: Order data\n",
    "    o.order_stats.total_orders,\n",
    "    o.order_stats.total_spent,\n",
    "    o.order_stats.last_order_date,\n",
    "    o.order_stats.avg_order_value,\n",
    "\n",
    "    -- S3: Product preferences\n",
    "    p.product_prefs.favorite_category,\n",
    "    p.product_prefs.categories_purchased,\n",
    "    p.product_prefs.product_diversity_score\n",
    "\n",
    "FROM pg_users.users u\n",
    "\n",
    "-- Join com MySQL orders\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        STRUCT_PACK(\n",
    "            total_orders := COUNT(*),\n",
    "            total_spent := SUM(amount),\n",
    "            last_order_date := MAX(order_date),\n",
    "            avg_order_value := AVG(amount)\n",
    "        ) as order_stats\n",
    "    FROM mysql_orders.orders\n",
    "    GROUP BY customer_id\n",
    ") o ON u.user_id = o.customer_id\n",
    "\n",
    "-- Join com S3 products\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        STRUCT_PACK(\n",
    "            favorite_category := MODE(category),\n",
    "            categories_purchased := COUNT(DISTINCT category),\n",
    "            product_diversity_score := COUNT(DISTINCT product_id) / COUNT(*)\n",
    "        ) as product_prefs\n",
    "    FROM read_parquet('s3://company-data/products/user_products.parquet')\n",
    "    GROUP BY user_id\n",
    ") p ON u.user_id = p.user_id\n",
    "\n",
    "WHERE u.status = 'active';\n",
    "\n",
    "-- Analytics query\n",
    "SELECT\n",
    "    country,\n",
    "    COUNT(*) as customers,\n",
    "    AVG(order_stats.total_orders) as avg_orders_per_customer,\n",
    "    AVG(order_stats.total_spent) as avg_lifetime_value,\n",
    "    AVG(product_prefs.product_diversity_score) as avg_diversity\n",
    "FROM customer_360\n",
    "GROUP BY country\n",
    "ORDER BY customers DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nFederation Query:\")\n",
    "print(federation_query)\n",
    "\n",
    "print(\"\"\"\n",
    "Performance Optimization:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Push-down predicates to source databases\n",
    "âœ“ Filter early (WHERE clauses)\n",
    "âœ“ Aggregate at source when possible\n",
    "âœ“ Use covering indexes on source databases\n",
    "âœ“ Materialize frequently-used joins\n",
    "âœ“ Partition S3 data appropriately\n",
    "\n",
    "Monitoring:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Query execution time\n",
    "- Data volume transferred\n",
    "- Source database load\n",
    "- Cache hit rate\n",
    "- Cost per query\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 6\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\"\"\n",
    "CDC Pattern com DuckDB:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Tracking changes across multiple databases\n",
    "\"\"\")\n",
    "\n",
    "cdc_pattern = \"\"\"\n",
    "-- CDC tracking table\n",
    "CREATE TABLE IF NOT EXISTS cdc_watermarks (\n",
    "    source_database VARCHAR,\n",
    "    source_table VARCHAR,\n",
    "    last_sync_timestamp TIMESTAMP,\n",
    "    last_sync_id BIGINT,\n",
    "    records_synced BIGINT\n",
    ");\n",
    "\n",
    "-- CDC extraction from PostgreSQL\n",
    "CREATE OR REPLACE TEMP TABLE pg_changes AS\n",
    "SELECT\n",
    "    'postgres' as source_db,\n",
    "    'users' as source_table,\n",
    "    *\n",
    "FROM pg_users.users\n",
    "WHERE updated_at > (\n",
    "    SELECT COALESCE(MAX(last_sync_timestamp), '1970-01-01'::TIMESTAMP)\n",
    "    FROM cdc_watermarks\n",
    "    WHERE source_database = 'postgres'\n",
    "        AND source_table = 'users'\n",
    ");\n",
    "\n",
    "-- CDC extraction from MySQL\n",
    "CREATE OR REPLACE TEMP TABLE mysql_changes AS\n",
    "SELECT\n",
    "    'mysql' as source_db,\n",
    "    'orders' as source_table,\n",
    "    *\n",
    "FROM mysql_orders.orders\n",
    "WHERE updated_at > (\n",
    "    SELECT COALESCE(MAX(last_sync_timestamp), '1970-01-01'::TIMESTAMP)\n",
    "    FROM cdc_watermarks\n",
    "    WHERE source_database = 'mysql'\n",
    "        AND source_table = 'orders'\n",
    ");\n",
    "\n",
    "-- Merge changes to data lake\n",
    "COPY pg_changes\n",
    "TO 's3://datalake/cdc/users/incremental.parquet'\n",
    "(FORMAT PARQUET, APPEND true);\n",
    "\n",
    "COPY mysql_changes\n",
    "TO 's3://datalake/cdc/orders/incremental.parquet'\n",
    "(FORMAT PARQUET, APPEND true);\n",
    "\n",
    "-- Update watermarks\n",
    "INSERT INTO cdc_watermarks\n",
    "SELECT\n",
    "    'postgres' as source_database,\n",
    "    'users' as source_table,\n",
    "    MAX(updated_at) as last_sync_timestamp,\n",
    "    MAX(user_id) as last_sync_id,\n",
    "    COUNT(*) as records_synced\n",
    "FROM pg_changes\n",
    "UNION ALL\n",
    "SELECT\n",
    "    'mysql' as source_database,\n",
    "    'orders' as source_table,\n",
    "    MAX(updated_at) as last_sync_timestamp,\n",
    "    MAX(order_id) as last_sync_id,\n",
    "    COUNT(*) as records_synced\n",
    "FROM mysql_changes;\n",
    "\n",
    "-- Compaction: Merge incremental â†’ full\n",
    "CREATE OR REPLACE TABLE users_full AS\n",
    "SELECT DISTINCT ON (user_id)\n",
    "    *\n",
    "FROM (\n",
    "    SELECT * FROM read_parquet('s3://datalake/cdc/users/*.parquet')\n",
    ")\n",
    "ORDER BY user_id, updated_at DESC;\n",
    "\n",
    "-- Export compacted\n",
    "COPY users_full\n",
    "TO 's3://datalake/full/users/snapshot.parquet'\n",
    "(FORMAT PARQUET, COMPRESSION 'zstd');\n",
    "\"\"\"\n",
    "\n",
    "print(cdc_pattern)\n",
    "\n",
    "print(\"\"\"\n",
    "CDC Best Practices:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Use updated_at timestamps\n",
    "âœ“ Implement watermark tracking\n",
    "âœ“ Handle deletes (soft delete ou tombstones)\n",
    "âœ“ Periodic full snapshots\n",
    "âœ“ Compaction strategy\n",
    "âœ“ Idempotent processing\n",
    "âœ“ Monitoring e alerting\n",
    "\n",
    "Scheduling:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "High-frequency: Every 5 minutes\n",
    "Standard: Hourly\n",
    "Low-frequency: Daily\n",
    "Compaction: Weekly\n",
    "\"\"\")\n",
    "\n",
    "# Exemplo/Bloco 7\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Performance Optimization Strategies:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Optimization examples\n",
    "optimizations = \"\"\"\n",
    "1. Predicate Pushdown\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âŒ Slow:\n",
    "   SELECT * FROM 's3://bucket/data/*.parquet'\n",
    "   WHERE date >= '2024-01-01';\n",
    "\n",
    "   âœ“ Fast (Hive Partitioning):\n",
    "   SELECT * FROM 's3://bucket/data/year=2024/month=01/*.parquet';\n",
    "\n",
    "2. Projection Pushdown\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âŒ Slow:\n",
    "   SELECT user_id, amount\n",
    "   FROM 's3://bucket/wide_table.parquet';  -- Reads all columns\n",
    "\n",
    "   âœ“ Fast:\n",
    "   SELECT user_id, amount\n",
    "   FROM read_parquet('s3://bucket/wide_table.parquet',\n",
    "                     columns=['user_id', 'amount']);\n",
    "\n",
    "3. Parallel Reads\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âœ“ Fast:\n",
    "   SELECT * FROM 's3://bucket/data/*.parquet';  -- Reads in parallel\n",
    "\n",
    "   Configure threads:\n",
    "   SET threads = 8;\n",
    "\n",
    "4. Compression\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âœ“ Best for network I/O:\n",
    "   - zstd: Best compression ratio\n",
    "   - snappy: Fast compression/decompression\n",
    "   - gzip: Good balance\n",
    "\n",
    "   COPY (...) TO 's3://bucket/data.parquet'\n",
    "   (FORMAT PARQUET, COMPRESSION 'zstd');\n",
    "\n",
    "5. File Sizing\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âœ“ Optimal: 256MB - 1GB per file\n",
    "   âŒ Too small: < 10MB (overhead)\n",
    "   âŒ Too large: > 5GB (memory pressure)\n",
    "\n",
    "6. Statistics\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âœ“ Parquet row group statistics enable skipping\n",
    "\n",
    "7. Caching\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   -- Cache hot data\n",
    "   CREATE TEMP TABLE hot_data AS\n",
    "   SELECT * FROM 's3://bucket/frequently_accessed.parquet';\n",
    "\n",
    "8. Batch Operations\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âŒ Slow:\n",
    "   FOR user IN users:\n",
    "       SELECT * FROM orders WHERE user_id = user;\n",
    "\n",
    "   âœ“ Fast:\n",
    "   SELECT * FROM orders WHERE user_id IN (SELECT user_id FROM users);\n",
    "\"\"\"\n",
    "\n",
    "print(optimizations)\n",
    "\n",
    "# Benchmark framework\n",
    "benchmark = \"\"\"\n",
    "-- Benchmark framework\n",
    "CREATE TABLE query_benchmarks (\n",
    "    query_id VARCHAR,\n",
    "    query_text VARCHAR,\n",
    "    execution_time_ms BIGINT,\n",
    "    rows_processed BIGINT,\n",
    "    timestamp TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Measure query\n",
    "CREATE OR REPLACE MACRO benchmark(query_id VARCHAR, query_text VARCHAR) AS (\n",
    "    -- Execute and measure\n",
    "    -- (DuckDB auto-timing in EXPLAIN ANALYZE)\n",
    "    EXPLAIN ANALYZE query_text\n",
    ");\n",
    "\n",
    "-- Example\n",
    "SELECT benchmark(\n",
    "    'query_1',\n",
    "    'SELECT * FROM s3://bucket/data.parquet WHERE date >= 2024-01-01'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nBenchmarking:\")\n",
    "print(benchmark)\n",
    "\n",
    "con = duckdb.connect()\n",
    "print(\"\"\"\n",
    "Monitoring Metrics:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Query execution time\n",
    "âœ“ Data scanned (GB)\n",
    "âœ“ Network throughput\n",
    "âœ“ Memory usage\n",
    "âœ“ CPU utilization\n",
    "âœ“ Cache hit rate\n",
    "âœ“ Cost per query\n",
    "\"\"\")\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 8\n",
    "print(\"\"\"\n",
    "Troubleshooting Guide:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. Authentication Errors\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"Access Denied\" ou \"Authentication failed\"\n",
    "\n",
    "   Checklist:\n",
    "   â˜ Verify credentials are correct\n",
    "   â˜ Check secret is created: SELECT * FROM duckdb_secrets()\n",
    "   â˜ Verify SCOPE matches URL\n",
    "   â˜ Use which_secret() to debug\n",
    "   â˜ Check IAM permissions (AWS)\n",
    "   â˜ Verify service account (GCP)\n",
    "   â˜ Check Azure RBAC roles\n",
    "\n",
    "   Debug:\n",
    "   SELECT * FROM which_secret('s3://bucket/file.parquet', 's3');\n",
    "\n",
    "2. SSL/TLS Errors\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"SSL verification failed\"\n",
    "\n",
    "   Solutions:\n",
    "   â˜ Verify USE_SSL = true for production\n",
    "   â˜ Check SSLMODE setting (postgres)\n",
    "   â˜ Verify certificate paths\n",
    "   â˜ Check certificate expiration\n",
    "   â˜ Validate hostname matches certificate\n",
    "\n",
    "   Debug:\n",
    "   -- Test sem SSL primeiro (apenas dev!)\n",
    "   CREATE SECRET test (\n",
    "       TYPE s3,\n",
    "       KEY_ID 'key',\n",
    "       SECRET 'secret',\n",
    "       USE_SSL false\n",
    "   );\n",
    "\n",
    "3. Extension Not Loaded\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"Extension not loaded\"\n",
    "\n",
    "   Solutions:\n",
    "   â˜ INSTALL extension\n",
    "   â˜ LOAD extension\n",
    "   â˜ Check extension name (httpfs vs http_fs)\n",
    "   â˜ Verify installation succeeded\n",
    "\n",
    "   Debug:\n",
    "   SELECT * FROM duckdb_extensions();\n",
    "\n",
    "4. Timeout Errors\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"Connection timeout\" ou \"Read timeout\"\n",
    "\n",
    "   Solutions:\n",
    "   â˜ Increase TIMEOUT parameter\n",
    "   â˜ Check network connectivity\n",
    "   â˜ Verify firewall rules\n",
    "   â˜ Check server load\n",
    "   â˜ Reduce query complexity\n",
    "\n",
    "   Debug:\n",
    "   CREATE SECRET s3_longer_timeout (\n",
    "       TYPE s3,\n",
    "       KEY_ID 'key',\n",
    "       SECRET 'secret',\n",
    "       TIMEOUT 120000  -- 2 minutos\n",
    "   );\n",
    "\n",
    "5. File Not Found\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"File not found\" ou \"No such key\"\n",
    "\n",
    "   Checklist:\n",
    "   â˜ Verify file path is correct\n",
    "   â˜ Check bucket/container name\n",
    "   â˜ Verify permissions (read access)\n",
    "   â˜ Check file actually exists\n",
    "   â˜ Case sensitivity (S3 is case-sensitive)\n",
    "\n",
    "   Debug:\n",
    "   -- List files\n",
    "   SELECT * FROM read_parquet('s3://bucket/path/*.parquet');\n",
    "\n",
    "6. Memory Errors\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"Out of memory\"\n",
    "\n",
    "   Solutions:\n",
    "   â˜ Reduce data volume (add filters)\n",
    "   â˜ Use streaming: read_parquet_auto()\n",
    "   â˜ Process in batches\n",
    "   â˜ Increase available memory\n",
    "   â˜ Use projection pushdown\n",
    "\n",
    "   Debug:\n",
    "   SET memory_limit = '8GB';\n",
    "   SET temp_directory = '/path/to/large/disk';\n",
    "\n",
    "7. Credential Chain Failures\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"No credentials found in chain\"\n",
    "\n",
    "   Checklist:\n",
    "   â˜ Verify environment variables set\n",
    "   â˜ Check ~/.aws/credentials (AWS)\n",
    "   â˜ Verify gcloud auth (GCP)\n",
    "   â˜ Check Azure CLI auth\n",
    "   â˜ Verify IAM role attached (EC2)\n",
    "\n",
    "   Debug:\n",
    "   -- Test each chain method individually\n",
    "   CREATE SECRET s3_env_only (\n",
    "       TYPE s3,\n",
    "       PROVIDER credential_chain,\n",
    "       CHAIN 'env'\n",
    "   );\n",
    "\n",
    "8. Persistent Secret Not Loading\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Error: \"Secret not found after restart\"\n",
    "\n",
    "   Checklist:\n",
    "   â˜ Verify used CREATE PERSISTENT SECRET\n",
    "   â˜ Check secret_directory setting\n",
    "   â˜ Verify file permissions\n",
    "   â˜ Check disk space\n",
    "   â˜ Verify database path\n",
    "\n",
    "   Debug:\n",
    "   SELECT name, persistent, storage\n",
    "   FROM duckdb_secrets();\n",
    "\n",
    "9. Slow Queries\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   Issue: Query takes too long\n",
    "\n",
    "   Solutions:\n",
    "   â˜ Use EXPLAIN ANALYZE\n",
    "   â˜ Check partitioning\n",
    "   â˜ Verify predicate pushdown\n",
    "   â˜ Add WHERE filters early\n",
    "   â˜ Use covering projections\n",
    "   â˜ Check file sizes\n",
    "   â˜ Verify compression\n",
    "\n",
    "   Debug:\n",
    "   EXPLAIN ANALYZE\n",
    "   SELECT * FROM 's3://bucket/data.parquet'\n",
    "   WHERE date >= '2024-01-01';\n",
    "\n",
    "10. Cross-Database Join Issues\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Issue: Joins between databases slow/failing\n",
    "\n",
    "    Solutions:\n",
    "    â˜ Materialize smaller table locally\n",
    "    â˜ Use temp tables for intermediate results\n",
    "    â˜ Optimize join order\n",
    "    â˜ Add indexes on source databases\n",
    "    â˜ Consider denormalization\n",
    "\n",
    "    Debug:\n",
    "    CREATE TEMP TABLE small_table AS\n",
    "    SELECT * FROM mysql_db.small_dimension;\n",
    "\n",
    "    SELECT *\n",
    "    FROM large_s3_fact f\n",
    "    JOIN small_table d ON f.id = d.id;\n",
    "\"\"\")\n",
    "\n",
    "# Exemplo/Bloco 9\n",
    "import duckdb\n",
    "\n",
    "print(\"\"\"\n",
    "Diagnostic Queries:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "\n",
    "diagnostics = \"\"\"\n",
    "-- 1. List all secrets\n",
    "SELECT\n",
    "    name,\n",
    "    type,\n",
    "    provider,\n",
    "    scope,\n",
    "    persistent,\n",
    "    storage\n",
    "FROM duckdb_secrets()\n",
    "ORDER BY type, name;\n",
    "\n",
    "-- 2. Check which secret will be used\n",
    "SELECT *\n",
    "FROM which_secret('s3://my-bucket/file.parquet', 's3');\n",
    "\n",
    "-- 3. List installed extensions\n",
    "SELECT\n",
    "    extension_name,\n",
    "    installed,\n",
    "    loaded\n",
    "FROM duckdb_extensions()\n",
    "WHERE extension_name IN ('httpfs', 'azure', 'mysql_scanner', 'postgres_scanner');\n",
    "\n",
    "-- 4. Check DuckDB settings\n",
    "SELECT *\n",
    "FROM duckdb_settings()\n",
    "WHERE name IN (\n",
    "    'secret_directory',\n",
    "    'threads',\n",
    "    'memory_limit',\n",
    "    'temp_directory'\n",
    ");\n",
    "\n",
    "-- 5. View active connections (attached databases)\n",
    "SELECT *\n",
    "FROM duckdb_databases();\n",
    "\n",
    "-- 6. Memory usage\n",
    "SELECT *\n",
    "FROM pragma_database_size();\n",
    "\n",
    "-- 7. Table information\n",
    "SELECT *\n",
    "FROM information_schema.tables;\n",
    "\"\"\"\n",
    "\n",
    "print(diagnostics)\n",
    "\n",
    "print(\"\"\"\n",
    "Logging e Monitoring:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Python logging:\n",
    "\"\"\")\n",
    "\n",
    "logging_example = \"\"\"\n",
    "import logging\n",
    "import duckdb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    filename='duckdb_secrets.log'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('duckdb_secrets')\n",
    "\n",
    "try:\n",
    "    con = duckdb.connect()\n",
    "    logger.info(\"Connection established\")\n",
    "\n",
    "    con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    logger.info(\"httpfs extension loaded\")\n",
    "\n",
    "    con.execute(\\\"\\\"\\\"\n",
    "        CREATE SECRET s3_prod (\n",
    "            TYPE s3,\n",
    "            KEY_ID 'key',\n",
    "            SECRET 'secret'\n",
    "        )\n",
    "    \\\"\\\"\\\")\n",
    "    logger.info(\"Secret 's3_prod' created\")\n",
    "\n",
    "    result = con.execute(\"SELECT * FROM 's3://bucket/file.parquet'\").df()\n",
    "    logger.info(f\"Query completed: {len(result)} rows\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {e}\", exc_info=True)\n",
    "finally:\n",
    "    con.close()\n",
    "    logger.info(\"Connection closed\")\n",
    "\"\"\"\n",
    "\n",
    "print(logging_example)\n",
    "\n",
    "con.close()\n",
    "\n",
    "# Exemplo/Bloco 10\n",
    "print(\"\"\"\n",
    "Docker Deployment:\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "dockerfile = \"\"\"\n",
    "# Dockerfile\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install DuckDB\n",
    "RUN pip install --no-cache-dir duckdb\n",
    "\n",
    "# Install cloud SDKs\n",
    "RUN pip install boto3 azure-identity azure-storage-blob google-cloud-storage\n",
    "\n",
    "# Copy application\n",
    "COPY app/ /app/\n",
    "\n",
    "# Environment variables (overridden at runtime)\n",
    "ENV AWS_ACCESS_KEY_ID=\"\"\n",
    "ENV AWS_SECRET_ACCESS_KEY=\"\"\n",
    "ENV AZURE_TENANT_ID=\"\"\n",
    "ENV AZURE_CLIENT_ID=\"\"\n",
    "ENV AZURE_CLIENT_SECRET=\"\"\n",
    "\n",
    "# Run\n",
    "CMD [\"python\", \"main.py\"]\n",
    "\"\"\"\n",
    "\n",
    "docker_compose = \"\"\"\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  duckdb-app:\n",
    "    build: .\n",
    "    environment:\n",
    "      # Load from .env file\n",
    "      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n",
    "      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n",
    "      - GCP_PROJECT_ID=${GCP_PROJECT_ID}\n",
    "      - AZURE_TENANT_ID=${AZURE_TENANT_ID}\n",
    "      - AZURE_CLIENT_ID=${AZURE_CLIENT_ID}\n",
    "      - AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}\n",
    "    volumes:\n",
    "      # Persistent secrets\n",
    "      - ./secrets:/app/secrets:ro\n",
    "      # DuckDB data\n",
    "      - ./data:/app/data\n",
    "    restart: unless-stopped\n",
    "\"\"\"\n",
    "\n",
    "print(\"Dockerfile:\")\n",
    "print(dockerfile)\n",
    "\n",
    "print(\"\\ndocker-compose.yml:\")\n",
    "print(docker_compose)\n",
    "\n",
    "kubernetes = \"\"\"\n",
    "# Kubernetes Deployment\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: duckdb-secrets\n",
    "type: Opaque\n",
    "stringData:\n",
    "  aws-access-key-id: \"AKIAIOSFODNN7EXAMPLE\"\n",
    "  aws-secret-access-key: \"wJalrXUtnFEMI...\"\n",
    "\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: duckdb-app\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: duckdb-app\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: duckdb-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: duckdb-app\n",
    "        image: mycompany/duckdb-app:latest\n",
    "        env:\n",
    "        - name: AWS_ACCESS_KEY_ID\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: duckdb-secrets\n",
    "              key: aws-access-key-id\n",
    "        - name: AWS_SECRET_ACCESS_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: duckdb-secrets\n",
    "              key: aws-secret-access-key\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "        volumeMounts:\n",
    "        - name: data\n",
    "          mountPath: /app/data\n",
    "      volumes:\n",
    "      - name: data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: duckdb-data-pvc\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nKubernetes:\")\n",
    "print(kubernetes)\n",
    "\n",
    "print(\"\"\"\n",
    "Best Practices:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Use secrets managers (nÃ£o env vars em prod)\n",
    "âœ“ Read-only volume mounts para secrets\n",
    "âœ“ Health checks\n",
    "âœ“ Resource limits\n",
    "âœ“ Logging to stdout/stderr\n",
    "âœ“ Graceful shutdown\n",
    "âœ“ Horizontal scaling considerations\n",
    "\"\"\")\n",
    "\n",
    "# Exemplo/Bloco 11\n",
    "print(\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "            CURSO COMPLETO - RESUMO FINAL\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CapÃ­tulo 1: IntroduÃ§Ã£o\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ O que sÃ£o secrets\n",
    "âœ“ CREATE/DROP SECRET\n",
    "âœ“ duckdb_secrets()\n",
    "âœ“ Temporary vs Persistent\n",
    "\n",
    "CapÃ­tulo 2: Tipos de Secrets\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ S3, R2, GCS, Azure\n",
    "âœ“ MySQL, PostgreSQL\n",
    "âœ“ HTTP, Hugging Face\n",
    "\n",
    "CapÃ­tulo 3: Cloud Storage\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ S3 completo (URL styles, regions)\n",
    "âœ“ Azure (connection string, managed identity)\n",
    "âœ“ GCS (service accounts, ADC)\n",
    "âœ“ R2 (Cloudflare)\n",
    "\n",
    "CapÃ­tulo 4: Database Secrets\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ MySQL (SSL, connection strings)\n",
    "âœ“ PostgreSQL (ATTACH, SSL)\n",
    "âœ“ Cross-database queries\n",
    "âœ“ ETL patterns\n",
    "\n",
    "CapÃ­tulo 5: Persistent Secrets\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Temporary vs Persistent\n",
    "âœ“ secret_directory\n",
    "âœ“ Backup e restore\n",
    "âœ“ File permissions\n",
    "\n",
    "CapÃ­tulo 6: Providers\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ config, credential_chain\n",
    "âœ“ managed_identity\n",
    "âœ“ service_principal\n",
    "âœ“ Por ambiente\n",
    "\n",
    "CapÃ­tulo 7: SCOPE e Named Secrets\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ SCOPE hierarchy\n",
    "âœ“ which_secret()\n",
    "âœ“ MÃºltiplas credenciais\n",
    "âœ“ Naming conventions\n",
    "\n",
    "CapÃ­tulo 8: Extensions\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ httpfs (S3, GCS, HTTP)\n",
    "âœ“ azure (Blob Storage)\n",
    "âœ“ mysql_scanner, postgres_scanner\n",
    "âœ“ Extension management\n",
    "\n",
    "CapÃ­tulo 9: SeguranÃ§a\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ Evitar exposiÃ§Ã£o de credenciais\n",
    "âœ“ RotaÃ§Ã£o de secrets\n",
    "âœ“ SSL/TLS configuration\n",
    "âœ“ Auditoria e logging\n",
    "âœ“ Compliance\n",
    "\n",
    "CapÃ­tulo 10: Casos de Uso AvanÃ§ados\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ“ ETL multi-cloud\n",
    "âœ“ Medallion architecture\n",
    "âœ“ Database federation\n",
    "âœ“ CDC patterns\n",
    "âœ“ Performance optimization\n",
    "âœ“ Troubleshooting\n",
    "âœ“ Production deployment\n",
    "\n",
    "PrÃ³ximos Passos:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Implementar secrets em seus projetos\n",
    "2. Configurar CI/CD com secrets\n",
    "3. Estabelecer rotaÃ§Ã£o policy\n",
    "4. Setup monitoring e alerting\n",
    "5. Documentar sua arquitetura\n",
    "6. Treinar seu time\n",
    "7. Regular security reviews\n",
    "\n",
    "Recursos Adicionais:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- DuckDB Documentation: https://duckdb.org/docs\n",
    "- GitHub Issues: https://github.com/duckdb/duckdb\n",
    "- Discord Community: https://discord.duckdb.org\n",
    "- Stack Overflow: tag [duckdb]\n",
    "\n",
    "Lembre-se:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"Security is not a product, but a process\" - Bruce Schneier\n",
    "\n",
    "âœ“ Secrets sÃ£o poderosos, mas requerem cuidado\n",
    "âœ“ Sempre seguir security best practices\n",
    "âœ“ Documentar decisÃµes e configuraÃ§Ãµes\n",
    "âœ“ Automatizar quando possÃ­vel\n",
    "âœ“ Monitorar continuamente\n",
    "âœ“ Estar preparado para incidentes\n",
    "\n",
    "Boa sorte com seus projetos DuckDB! ğŸ¦†\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Exemplo/Bloco 12\n",
    "\"\"\"\n",
    "Projeto Final: Multi-Cloud Data Platform\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Objetivo:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Implementar plataforma completa de dados usando DuckDB Secrets\n",
    "\n",
    "Requisitos:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Multi-Cloud Setup\n",
    "   - AWS S3 (raw data)\n",
    "   - GCS (processed data)\n",
    "   - Azure (analytics)\n",
    "\n",
    "2. Database Integration\n",
    "   - PostgreSQL (users)\n",
    "   - MySQL (transactions)\n",
    "\n",
    "3. Security\n",
    "   - Credential chain\n",
    "   - SSL/TLS everywhere\n",
    "   - RotaÃ§Ã£o automÃ¡tica\n",
    "   - Audit logging\n",
    "\n",
    "4. Data Pipeline\n",
    "   - Incremental ETL\n",
    "   - Data quality checks\n",
    "   - Partitioning strategy\n",
    "\n",
    "5. Monitoring\n",
    "   - Query performance\n",
    "   - Secret usage\n",
    "   - Error tracking\n",
    "   - Cost monitoring\n",
    "\n",
    "6. Documentation\n",
    "   - Architecture diagram\n",
    "   - Secret inventory\n",
    "   - Runbooks\n",
    "   - Troubleshooting guide\n",
    "\n",
    "EntregÃ¡veis:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â˜ CÃ³digo Python completo\n",
    "â˜ ConfiguraÃ§Ã£o de secrets\n",
    "â˜ CI/CD pipeline\n",
    "â˜ Docker/Kubernetes deployment\n",
    "â˜ Monitoring dashboards\n",
    "â˜ DocumentaÃ§Ã£o completa\n",
    "â˜ Testes automatizados\n",
    "\n",
    "Tempo Estimado: 2-3 dias\n",
    "\n",
    "Boa sorte! ğŸš€\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
