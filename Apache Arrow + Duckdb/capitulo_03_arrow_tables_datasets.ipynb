{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 03 Arrow Tables Datasets\n",
    "\n",
    "Notebook gerado automaticamente a partir do c√≥digo fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de pacotes necess√°rios\n",
    "!pip install pyarrow duckdb pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5609b3f",
   "metadata": {},
   "source": [
    "## üìö Introdu√ß√£o\n",
    "\n",
    "Este notebook aborda os conceitos de Arrow Tables e Datasets:\n",
    "- Cria√ß√£o de Tables\n",
    "- Schemas e tipos\n",
    "- Datasets particionados\n",
    "- Leitura de Parquet\n",
    "- Filtros e projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8550516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Cap√≠tulo 03: Arrow Tables e Datasets\n",
    "Curso: Apache Arrow + DuckDB\n",
    "Nota: UTF-8 √© configurado automaticamente em notebooks Jupyter\n",
    "\"\"\"\n",
    "\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"CAP√çTULO 03: ARROW TABLES E DATASETS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be8a3f",
   "metadata": {},
   "source": [
    "## üîß Prepara√ß√£o dos Dados\n",
    "\n",
    "Cria√ß√£o de dados de exemplo e conex√£o com DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo globais\n",
    "try:\n",
    "    print(\"\\nGerando dados de exemplo...\")\n",
    "    data = pa.table({\n",
    "        'id': range(1000),\n",
    "        'valor': np.random.randn(1000),\n",
    "        'categoria': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "    })\n",
    "    print(f\"Tabela PyArrow criada: {data.num_rows} linhas\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao criar dados: {e}\")\n",
    "\n",
    "# Conex√£o DuckDB\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b735569",
   "metadata": {},
   "source": [
    "## üìã T√≥pico 1: Cria√ß√£o de Tables\n",
    "\n",
    "Exemplos pr√°ticos de como criar tabelas Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9efb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Cria√ß√£o de Tables'.upper()} ---\")\n",
    "\n",
    "# 3.1.1 Anatomia de uma Arrow Table\n",
    "print(\"\\n1. Anatomia de uma Arrow Table:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar Arrow table\n",
    "table = pa.table({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Carol', 'David', 'Eve'],\n",
    "    'age': [30, 25, 35, 28, 32],\n",
    "    'salary': [75000.00, 65000.00, 85000.00, 70000.00, 80000.00]\n",
    "})\n",
    "\n",
    "print(\"Arrow Table:\")\n",
    "print(table)\n",
    "print(f\"\\nN√∫mero de linhas: {table.num_rows}\")\n",
    "print(f\"N√∫mero de colunas: {table.num_columns}\")\n",
    "print(f\"Nome das colunas: {table.column_names}\")\n",
    "print(f\"Schema: {table.schema}\")\n",
    "\n",
    "# Tamanho em mem√≥ria\n",
    "print(f\"\\nTamanho total: {table.nbytes:,} bytes\")\n",
    "\n",
    "# 3.1.2 Acessar Dados da Table\n",
    "print(\"\\n2. Acessar Dados da Table:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "table_sales = pa.table({\n",
    "    'product': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'sales': [100, 200, 150, 300, 250],\n",
    "    'region': ['North', 'South', 'North', 'East', 'South']\n",
    "})\n",
    "\n",
    "# Acessar coluna por nome\n",
    "sales_column = table_sales['sales']\n",
    "print(f\"Coluna 'sales': {sales_column}\")\n",
    "print(f\"Tipo: {type(sales_column)}\")  # pyarrow.lib.ChunkedArray\n",
    "\n",
    "# Acessar coluna por √≠ndice\n",
    "first_column = table_sales.column(0)\n",
    "print(f\"\\nPrimeira coluna: {first_column}\")\n",
    "\n",
    "# Converter para Python list\n",
    "sales_list = table_sales['sales'].to_pylist()\n",
    "print(f\"\\nSales como lista: {sales_list}\")\n",
    "\n",
    "# Slice (fatiar) table\n",
    "subset = table_sales.slice(1, 3)  # Linhas 1-3\n",
    "print(f\"\\nSubset (linhas 1-3):\")\n",
    "print(subset)\n",
    "\n",
    "# Filtrar com DuckDB\n",
    "filtered = con.execute(\"\"\"\n",
    "    SELECT * FROM table_sales\n",
    "    WHERE sales > 150\n",
    "    ORDER BY sales DESC\n",
    "\"\"\").arrow()\n",
    "\n",
    "print(f\"\\nFiltrado (sales > 150):\")\n",
    "print(filtered)\n",
    "\n",
    "# 3.1.3 Opera√ß√µes com Colunas\n",
    "print(\"\\n3. Opera√ß√µes com Colunas:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "table_ops = pa.table({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'value': [10, 20, 30, 40, 50]\n",
    "})\n",
    "\n",
    "# Adicionar coluna\n",
    "doubled = pc.multiply(table_ops['value'], 2)\n",
    "table_with_doubled = table_ops.append_column('value_doubled', doubled)\n",
    "\n",
    "print(\"Table com nova coluna:\")\n",
    "print(table_with_doubled)\n",
    "\n",
    "# Remover coluna\n",
    "table_without_id = table_ops.remove_column(0)\n",
    "print(\"\\nTable sem coluna 'id':\")\n",
    "print(table_without_id)\n",
    "\n",
    "# Renomear colunas\n",
    "renamed = table_ops.rename_columns(['product_id', 'quantity'])\n",
    "print(\"\\nTable com colunas renomeadas:\")\n",
    "print(renamed)\n",
    "\n",
    "# Selecionar colunas\n",
    "selected = table_ops.select(['value'])\n",
    "print(\"\\nApenas coluna 'value':\")\n",
    "print(selected)\n",
    "\n",
    "# 3.1.4 Concatenar Tables\n",
    "print(\"\\n4. Concatenar Tables:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar m√∫ltiplas tables\n",
    "table1 = pa.table({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Carol']\n",
    "})\n",
    "\n",
    "table2 = pa.table({\n",
    "    'id': [4, 5, 6],\n",
    "    'name': ['David', 'Eve', 'Frank']\n",
    "})\n",
    "\n",
    "table3 = pa.table({\n",
    "    'id': [7, 8, 9],\n",
    "    'name': ['Grace', 'Henry', 'Ivy']\n",
    "})\n",
    "\n",
    "# Concatenar verticalmente (empilhar linhas)\n",
    "combined = pa.concat_tables([table1, table2, table3])\n",
    "print(\"Tables concatenadas:\")\n",
    "print(combined)\n",
    "\n",
    "# Verificar com DuckDB\n",
    "result = con.execute(\"SELECT count(*) FROM combined\").fetchone()\n",
    "print(f\"\\nTotal de linhas: {result[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8e77f",
   "metadata": {},
   "source": [
    "## üî§ T√≥pico 2: Schemas e tipos\n",
    "\n",
    "Trabalhando com schemas e diferentes tipos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Schemas e tipos'.upper()} ---\")\n",
    "\n",
    "# 3.3.1 Nested Types (Struct)\n",
    "print(\"\\n1. Tipos Aninhados (Struct):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar schema com struct (tipo aninhado)\n",
    "schema = pa.schema([\n",
    "    ('id', pa.int64()),\n",
    "    ('name', pa.string()),\n",
    "    ('address', pa.struct([\n",
    "        ('street', pa.string()),\n",
    "        ('city', pa.string()),\n",
    "        ('zip', pa.string())\n",
    "    ])),\n",
    "    ('phone_numbers', pa.list_(pa.string()))\n",
    "])\n",
    "\n",
    "# Criar dados\n",
    "data_arrays = [\n",
    "    pa.array([1, 2, 3], type=pa.int64()),\n",
    "    pa.array(['Alice', 'Bob', 'Carol'], type=pa.string()),\n",
    "    pa.StructArray.from_arrays(\n",
    "        [\n",
    "            pa.array(['123 Main St', '456 Oak Ave', '789 Pine Rd']),\n",
    "            pa.array(['New York', 'Los Angeles', 'Chicago']),\n",
    "            pa.array(['10001', '90001', '60601'])\n",
    "        ],\n",
    "        names=['street', 'city', 'zip']\n",
    "    ),\n",
    "    pa.array([\n",
    "        ['555-1234', '555-5678'],\n",
    "        ['555-9012'],\n",
    "        ['555-3456', '555-7890', '555-1111']\n",
    "    ])\n",
    "]\n",
    "\n",
    "table_nested = pa.Table.from_arrays(data_arrays, schema=schema)\n",
    "\n",
    "print(\"Table com tipos aninhados:\")\n",
    "print(table_nested)\n",
    "print(f\"\\nSchema:\\n{table_nested.schema}\")\n",
    "\n",
    "# Query com DuckDB (acessa campos nested)\n",
    "result_nested = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        name,\n",
    "        address.city as city,\n",
    "        address.zip as zip,\n",
    "        len(phone_numbers) as phone_count\n",
    "    FROM table_nested\n",
    "\"\"\").arrow()\n",
    "\n",
    "print(\"\\nResultado (campos extra√≠dos):\")\n",
    "print(result_nested)\n",
    "\n",
    "# 3.3.2 List Types\n",
    "print(\"\\n2. Tipos de Lista:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar table com arrays\n",
    "table_lists = pa.table({\n",
    "    'customer_id': [1, 2, 3],\n",
    "    'customer_name': ['Alice', 'Bob', 'Carol'],\n",
    "    'order_ids': [\n",
    "        [101, 102, 103],\n",
    "        [201],\n",
    "        [301, 302]\n",
    "    ],\n",
    "    'order_amounts': [\n",
    "        [50.00, 75.50, 100.00],\n",
    "        [200.00],\n",
    "        [30.00, 45.00]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Table com listas:\")\n",
    "print(table_lists)\n",
    "\n",
    "# Query com DuckDB (unnest arrays)\n",
    "# Explodir arrays\n",
    "result_unnest = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        customer_name,\n",
    "        unnest(order_ids) as order_id,\n",
    "        unnest(order_amounts) as amount\n",
    "    FROM table_lists\n",
    "\"\"\").arrow()\n",
    "\n",
    "print(\"\\nDados expandidos:\")\n",
    "print(result_unnest)\n",
    "\n",
    "# Agrega√ß√µes\n",
    "summary = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        customer_name,\n",
    "        len(order_ids) as order_count,\n",
    "        list_sum(order_amounts) as total_spent\n",
    "    FROM table_lists\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nResumo por cliente:\")\n",
    "print(summary)\n",
    "\n",
    "# 3.3.3 Tipos Primitivos\n",
    "print(\"\\n3. Tipos Primitivos Diversos:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from decimal import Decimal\n",
    "\n",
    "# Criar table com v√°rios tipos\n",
    "table_types = pa.table({\n",
    "    'int8_col': pa.array([1, 2, 3], type=pa.int8()),\n",
    "    'int16_col': pa.array([100, 200, 300], type=pa.int16()),\n",
    "    'int32_col': pa.array([10000, 20000, 30000], type=pa.int32()),\n",
    "    'int64_col': pa.array([1000000, 2000000, 3000000], type=pa.int64()),\n",
    "    'float32_col': pa.array([1.1, 2.2, 3.3], type=pa.float32()),\n",
    "    'float64_col': pa.array([1.111, 2.222, 3.333], type=pa.float64()),\n",
    "    'bool_col': pa.array([True, False, True], type=pa.bool_()),\n",
    "    'string_col': pa.array(['a', 'b', 'c'], type=pa.string()),\n",
    "    'binary_col': pa.array([b'x', b'y', b'z'], type=pa.binary()),\n",
    "    'date_col': pa.array([date(2024, 1, 1), date(2024, 1, 2), date(2024, 1, 3)], type=pa.date32()),\n",
    "    'timestamp_col': pa.array([\n",
    "        datetime(2024, 1, 1, 10, 30),\n",
    "        datetime(2024, 1, 2, 11, 30),\n",
    "        datetime(2024, 1, 3, 12, 30)\n",
    "    ], type=pa.timestamp('s')),\n",
    "    'decimal_col': pa.array([\n",
    "        Decimal('123.45'),\n",
    "        Decimal('678.90'),\n",
    "        Decimal('111.22')\n",
    "    ], type=pa.decimal128(10, 2))\n",
    "})\n",
    "\n",
    "print(\"Table com tipos diversos:\")\n",
    "print(table_types)\n",
    "\n",
    "print(\"\\nSchema detalhado:\")\n",
    "for field in table_types.schema:\n",
    "    print(f\"  {field.name}: {field.type}\")\n",
    "\n",
    "# Query com DuckDB\n",
    "result_types = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        int32_col,\n",
    "        float64_col,\n",
    "        bool_col,\n",
    "        string_col,\n",
    "        date_col,\n",
    "        decimal_col\n",
    "    FROM table_types\n",
    "\"\"\").arrow()\n",
    "\n",
    "print(\"\\nResultado da query:\")\n",
    "print(result_types)\n",
    "\n",
    "# 3.3.4 Tipos Nullable vs Non-Nullable\n",
    "print(\"\\n4. Tipos Nullable vs Non-Nullable:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar schema com nullable e non-nullable\n",
    "schema_nullable = pa.schema([\n",
    "    pa.field('id', pa.int32(), nullable=False),\n",
    "    pa.field('name', pa.string(), nullable=False),\n",
    "    pa.field('email', pa.string(), nullable=True),\n",
    "    pa.field('age', pa.int32(), nullable=True)\n",
    "])\n",
    "\n",
    "# Criar table com valores nulos\n",
    "table_nullable = pa.table({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Carol'],\n",
    "    'email': ['alice@example.com', None, 'carol@example.com'],\n",
    "    'age': [30, None, 35]\n",
    "}, schema=schema_nullable)\n",
    "\n",
    "print(\"Schema com nullable:\")\n",
    "for field in schema_nullable:\n",
    "    print(f\"  {field.name}: {field.type} (nullable={field.nullable})\")\n",
    "\n",
    "print(\"\\nTable com valores nulos:\")\n",
    "print(table_nullable)\n",
    "\n",
    "# Query filtrando nulos\n",
    "result_nulls = con.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM table_nullable\n",
    "    WHERE email IS NOT NULL AND age IS NOT NULL\n",
    "\"\"\").arrow()\n",
    "\n",
    "print(\"\\nAp√≥s filtrar nulos:\")\n",
    "print(result_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac3cf3",
   "metadata": {},
   "source": [
    "## üìÇ T√≥pico 3: Datasets particionados\n",
    "\n",
    "Trabalhando com datasets particionados para melhor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Datasets particionados'.upper()} ---\")\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 3.2.1 Criar Dataset Simples (Multi-arquivo)\n",
    "print(\"\\n1. Dataset Simples (Multi-arquivo Parquet):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar diret√≥rio para dados\n",
    "data_dir = 'data_sales'\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.makedirs(f'{data_dir}', exist_ok=True)\n",
    "\n",
    "# Criar m√∫ltiplos arquivos Parquet (simulando dados mensais)\n",
    "for month in range(1, 4):\n",
    "    table = pa.table({\n",
    "        'date': pa.array([f'2024-{month:02d}-{day:02d}' for day in range(1, 11)]),\n",
    "        'sales': pa.array([100 * month + day for day in range(1, 11)], type=pa.int32()),\n",
    "        'region': pa.array(['North' if day % 2 == 0 else 'South' for day in range(1, 11)])\n",
    "    })\n",
    "    \n",
    "    # Escrever arquivo\n",
    "    pq.write_table(table, f'{data_dir}/month_{month}.parquet')\n",
    "\n",
    "print(\"Arquivos criados:\")\n",
    "for f in os.listdir(data_dir):\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Criar Dataset apontando para o diret√≥rio\n",
    "dataset = ds.dataset(data_dir, format='parquet')\n",
    "\n",
    "print(f\"\\nDataset criado\")\n",
    "print(f\"Schema: {dataset.schema}\")\n",
    "print(f\"Arquivos: {len(list(dataset.get_fragments()))}\")\n",
    "\n",
    "# Query com DuckDB\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        region,\n",
    "        count(*) as count,\n",
    "        sum(sales) as total_sales\n",
    "    FROM dataset\n",
    "    GROUP BY region\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nResultado da query:\")\n",
    "print(result)\n",
    "\n",
    "# 3.2.2 Dataset Particionado (Hive Partitioning)\n",
    "print(\"\\n2. Dataset Particionado (Hive Partitioning):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar diret√≥rio particionado\n",
    "partition_dir = 'data_partitioned'\n",
    "if os.path.exists(partition_dir):\n",
    "    shutil.rmtree(partition_dir)\n",
    "\n",
    "# Criar dados particionados por regi√£o e ano\n",
    "regions = ['North', 'South', 'East']\n",
    "years = [2023, 2024]\n",
    "\n",
    "for region in regions:\n",
    "    for year in years:\n",
    "        # Criar dados com tipos expl√≠citos para evitar inconsist√™ncia\n",
    "        table_part = pa.table({\n",
    "            'date': pa.array([f'{year}-01-{day:02d}' for day in range(1, 21)]),\n",
    "            'product': pa.array([f'Product_{i%3}' for i in range(20)]),\n",
    "            'sales': pa.array([100 + i * 10 for i in range(20)], type=pa.int32()),\n",
    "            'region': pa.array([region] * 20),\n",
    "            'year': pa.array([year] * 20, type=pa.int32())\n",
    "        })\n",
    "        \n",
    "        # Criar diret√≥rio particionado (Hive style: region=X/year=Y)\n",
    "        partition_path = f'{partition_dir}/region={region}/year={year}'\n",
    "        os.makedirs(partition_path, exist_ok=True)\n",
    "        \n",
    "        # Escrever arquivo\n",
    "        pq.write_table(table_part, f'{partition_path}/data.parquet')\n",
    "\n",
    "print(\"Dataset particionado criado com estrutura Hive:\")\n",
    "for root, dirs, files in os.walk(partition_dir):\n",
    "    level = root.replace(partition_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f'{subindent}{file}')\n",
    "\n",
    "# Ler dataset com parti√ß√µes autom√°ticas\n",
    "dataset_hive = ds.dataset(partition_dir, partitioning='hive')\n",
    "\n",
    "print(f\"\\nSchema (com colunas de parti√ß√£o):\")\n",
    "print(dataset_hive.schema)\n",
    "\n",
    "# Query filtrando por parti√ß√£o (muito eficiente!)\n",
    "result_partition = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        product,\n",
    "        sum(sales) as total_sales\n",
    "    FROM dataset_hive\n",
    "    WHERE region = 'North' AND year = 2024\n",
    "    GROUP BY product\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\").arrow()\n",
    "\n",
    "print(\"\\nVendas em North (2024):\")\n",
    "print(result_partition)\n",
    "\n",
    "# 3.2.3 Scanear Dataset Incrementalmente (Com Filtros)\n",
    "print(\"\\n3. Scanear Dataset com Filtros e Proje√ß√µes:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar scanner com filtros\n",
    "scanner = dataset_hive.scanner(\n",
    "    columns=['product', 'sales', 'region'],\n",
    "    filter=ds.field('year') == 2024\n",
    ")\n",
    "\n",
    "print(\"Scanner criado com filtro year=2024\")\n",
    "print(f\"Colunas projetadas: {scanner.projected_schema.names}\")\n",
    "\n",
    "# Converter para Arrow table\n",
    "filtered_table = scanner.to_table()\n",
    "print(f\"Linhas filtradas: {filtered_table.num_rows}\")\n",
    "\n",
    "# Query com DuckDB\n",
    "result_scan = con.execute(\"\"\"\n",
    "    SELECT region, count(*) as count\n",
    "    FROM filtered_table\n",
    "    GROUP BY region\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nContagem por regi√£o (2024):\")\n",
    "print(result_scan)\n",
    "\n",
    "# 3.2.4 Compara√ß√£o de Performance\n",
    "print(\"\\n4. Compara√ß√£o: Arquivo vs Dataset Particionado:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "import time\n",
    "\n",
    "# Teste 1: Ler arquivo √∫nico\n",
    "print(\"\\nTeste 1: Arquivo √∫nico (month_1.parquet)\")\n",
    "start = time.time()\n",
    "single_file = pq.read_table(f'{data_dir}/month_1.parquet')\n",
    "query_single = con.execute(\"\"\"\n",
    "    SELECT sum(sales) FROM single_file\n",
    "\"\"\").fetchone()\n",
    "time_single = time.time() - start\n",
    "print(f\"  Tempo: {time_single:.4f}s\")\n",
    "\n",
    "# Teste 2: Ler dataset multi-arquivo (sem parti√ß√£o)\n",
    "print(\"\\nTeste 2: Dataset multi-arquivo\")\n",
    "start = time.time()\n",
    "result_multi = con.execute(\"\"\"\n",
    "    SELECT sum(sales) FROM dataset\n",
    "\"\"\").fetchone()\n",
    "time_multi = time.time() - start\n",
    "print(f\"  Tempo: {time_multi:.4f}s\")\n",
    "\n",
    "# Teste 3: Ler dataset particionado com filtro\n",
    "print(\"\\nTeste 3: Dataset particionado com filtro (region='North')\")\n",
    "start = time.time()\n",
    "result_filtered = con.execute(\"\"\"\n",
    "    SELECT sum(sales) FROM dataset_hive\n",
    "    WHERE region = 'North' AND year = 2024\n",
    "\"\"\").fetchone()\n",
    "time_filtered = time.time() - start\n",
    "print(f\"  Tempo: {time_filtered:.4f}s\")\n",
    "\n",
    "print(f\"\\nOs datasets particionados permitem pruning de arquivos,\")\n",
    "print(f\"evitando ler dados desnecess√°rios!\")\n",
    "\n",
    "# Limpeza\n",
    "shutil.rmtree(data_dir, ignore_errors=True)\n",
    "shutil.rmtree(partition_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d1d77",
   "metadata": {},
   "source": [
    "## üìÑ T√≥pico 4: Leitura de Parquet\n",
    "\n",
    "Carregando e manipulando arquivos Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Leitura de Parquet'.upper()} ---\")\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "# 3.4.1 Preparar arquivo Parquet de exemplo\n",
    "parquet_dir = 'parquet_examples'\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "parquet_file = os.path.join(parquet_dir, 'sample_data.parquet')\n",
    "\n",
    "table_to_save = pa.table({\n",
    "    'id': range(100),\n",
    "    'name': [f'User_{i}' for i in range(100)],\n",
    "    'score': np.random.randint(0, 100, 100),\n",
    "    'active': [i % 2 == 0 for i in range(100)]\n",
    "})\n",
    "\n",
    "pq.write_table(table_to_save, parquet_file)\n",
    "print(f\"Arquivo '{parquet_file}' criado.\")\n",
    "\n",
    "# 3.4.2 Leitura b√°sica\n",
    "print(\"\\n1. Leitura Completa:\")\n",
    "full_table = pq.read_table(parquet_file)\n",
    "print(f\"Linhas lidas: {full_table.num_rows}\")\n",
    "\n",
    "# 3.4.3 Leitura seletiva (Projections)\n",
    "print(\"\\n2. Leitura Seletiva (Apenas 'id' e 'name'):\")\n",
    "partial_table = pq.read_table(parquet_file, columns=['id', 'name'])\n",
    "\n",
    "print(f\"Table lida (colunas selecionadas):\")\n",
    "print(partial_table.slice(0, 5))\n",
    "print(f\"\\nN√∫mero de colunas: {partial_table.num_columns}\")\n",
    "print(f\"Tamanho em mem√≥ria: {partial_table.nbytes:,} bytes\")\n",
    "\n",
    "# 3.4.4 Inspe√ß√£o de Metadados\n",
    "print(\"\\n3. Inspe√ß√£o de Metadados do Arquivo:\")\n",
    "metadata = pq.read_metadata(parquet_file)\n",
    "print(f\"N√∫mero de row groups: {metadata.num_row_groups}\")\n",
    "print(f\"Esquema no arquivo: {metadata.schema.to_arrow_schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0e910",
   "metadata": {},
   "source": [
    "## üîç T√≥pico 5: Filtros e projections\n",
    "\n",
    "Otimizando consultas com filtros e proje√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Filtros e projections'.upper()} ---\")\n",
    "\n",
    "import pyarrow.compute as pc\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Preparar dados para otimiza√ß√£o\n",
    "optimize_dir = 'optimize_data'\n",
    "if os.path.exists(optimize_dir):\n",
    "    shutil.rmtree(optimize_dir)\n",
    "os.makedirs(optimize_dir, exist_ok=True)\n",
    "\n",
    "# Criar tabela grande para demonstra√ß√£o\n",
    "large_table = pa.table({\n",
    "    'id': list(range(1, 10001)),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1, 10001)],\n",
    "    'email': [f'customer{i}@example.com' for i in range(1, 10001)],\n",
    "    'country': np.random.choice(['USA', 'UK', 'Canada', 'Germany', 'France'], 10000),\n",
    "    'age': np.random.randint(18, 85, 10000),\n",
    "    'salary': np.random.uniform(30000, 200000, 10000),\n",
    "    'department': np.random.choice(['Sales', 'Engineering', 'HR', 'Marketing', 'Finance'], 10000),\n",
    "    'hire_date': pa.array([f'2020-{(i%12)+1:02d}-{(i%28)+1:02d}' for i in range(10000)]),\n",
    "    'is_active': np.random.choice([True, False], 10000),\n",
    "    'last_login': pa.array([f'2024-{(i%12)+1:02d}-{(i%28)+1:02d}' for i in range(10000)])\n",
    "})\n",
    "\n",
    "# Escrever para Parquet\n",
    "parquet_optimize = f'{optimize_dir}/large_dataset.parquet'\n",
    "pq.write_table(large_table, parquet_optimize)\n",
    "\n",
    "print(f\"Dataset criado: {large_table.num_rows} linhas, {large_table.num_columns} colunas\")\n",
    "print(f\"Tamanho em mem√≥ria: {large_table.nbytes:,} bytes\")\n",
    "\n",
    "# 3.5.1 Push-down Filters (Filtrar ao ler)\n",
    "print(\"\\n1. Push-down Filters (Filtros na leitura):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sem filtro - ler tudo\n",
    "print(\"Op√ß√£o A: Ler tudo e depois filtrar em mem√≥ria\")\n",
    "start = time.time()\n",
    "full_table = pq.read_table(parquet_optimize)\n",
    "filtered_memory = full_table.filter(pc.field('salary') > 100000)\n",
    "time_filter_memory = time.time() - start\n",
    "print(f\"  Tempo: {time_filter_memory:.4f}s\")\n",
    "print(f\"  Resultados: {filtered_memory.num_rows} linhas\")\n",
    "\n",
    "# Com push-down filter - PyArrow filtra ao ler (mais eficiente)\n",
    "print(\"\\nOp√ß√£o B: Usar push-down filter (ler com filtro)\")\n",
    "start = time.time()\n",
    "filters = [('salary', '>', 100000)]\n",
    "filtered_pushdown = pq.read_table(parquet_optimize, filters=filters)\n",
    "time_filter_pushdown = time.time() - start\n",
    "print(f\"  Tempo: {time_filter_pushdown:.4f}s\")\n",
    "print(f\"  Resultados: {filtered_pushdown.num_rows} linhas\")\n",
    "\n",
    "print(f\"\\nMelhoria: {((time_filter_memory - time_filter_pushdown) / time_filter_memory * 100):.1f}% mais r√°pido com push-down\")\n",
    "\n",
    "# Exemplo com filtro complexo\n",
    "print(\"\\nOp√ß√£o C: Filtros complexos (m√∫ltiplas condi√ß√µes)\")\n",
    "complex_filters = [\n",
    "    [('department', '==', 'Engineering'), ('salary', '>', 80000)],\n",
    "    [('department', '==', 'Finance'), ('age', '>', 40)]\n",
    "]\n",
    "filtered_complex = pq.read_table(parquet_optimize, filters=complex_filters)\n",
    "print(f\"  Resultados: {filtered_complex.num_rows} linhas\")\n",
    "print(f\"  Colunas: {filtered_complex.column_names}\")\n",
    "\n",
    "# 3.5.2 Column Projection (Selecionar colunas √∫teis)\n",
    "print(\"\\n2. Column Projection (Proje√ß√£o de colunas):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sem proje√ß√£o - ler todas as colunas\n",
    "print(\"Op√ß√£o A: Ler todas as colunas (10 colunas)\")\n",
    "start = time.time()\n",
    "all_cols = pq.read_table(parquet_optimize)\n",
    "time_all_cols = time.time() - start\n",
    "memory_all_cols = all_cols.nbytes\n",
    "print(f\"  Tempo: {time_all_cols:.4f}s\")\n",
    "print(f\"  Mem√≥ria: {memory_all_cols:,} bytes\")\n",
    "\n",
    "# Com proje√ß√£o - ler apenas colunas necess√°rias\n",
    "print(\"\\nOp√ß√£o B: Projetar apenas 3 colunas (id, salary, department)\")\n",
    "needed_cols = ['id', 'salary', 'department']\n",
    "start = time.time()\n",
    "projected = pq.read_table(parquet_optimize, columns=needed_cols)\n",
    "time_projected = time.time() - start\n",
    "memory_projected = projected.nbytes\n",
    "print(f\"  Tempo: {time_projected:.4f}s\")\n",
    "print(f\"  Mem√≥ria: {memory_projected:,} bytes\")\n",
    "\n",
    "economy = (1 - memory_projected / memory_all_cols) * 100\n",
    "print(f\"\\nEconomia de mem√≥ria: {economy:.1f}%\")\n",
    "print(f\"Redu√ß√£o de tempo: {((time_all_cols - time_projected) / time_all_cols * 100):.1f}%\")\n",
    "\n",
    "# 3.5.3 Filtros + Projections (Otimiza√ß√£o combinada)\n",
    "print(\"\\n3. Filtros + Projections (Otimiza√ß√£o combinada):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Cen√°rio: Filtrar por pa√≠s e selecionar apenas dados relevantes\")\n",
    "\n",
    "# Sem otimiza√ß√£o\n",
    "print(\"\\nAbordagem 1: Ler tudo, depois filtrar e projetar\")\n",
    "start = time.time()\n",
    "step1 = pq.read_table(parquet_optimize)\n",
    "step2 = step1.filter(pc.field('country') == 'USA')\n",
    "step3 = step2.select(['id', 'customer_name', 'salary', 'department'])\n",
    "time_naive = time.time() - start\n",
    "print(f\"  Tempo: {time_naive:.4f}s\")\n",
    "print(f\"  Resultado: {step3.num_rows} linhas, {step3.num_columns} colunas\")\n",
    "\n",
    "# Com otimiza√ß√£o\n",
    "print(\"\\nAbordagem 2: Projetar + Filtrar na leitura (otimizado)\")\n",
    "start = time.time()\n",
    "filters_opt = [('country', '==', 'USA')]\n",
    "optimized = pq.read_table(\n",
    "    parquet_optimize,\n",
    "    columns=['id', 'customer_name', 'salary', 'department', 'country'],\n",
    "    filters=filters_opt\n",
    ")\n",
    "time_optimized = time.time() - start\n",
    "print(f\"  Tempo: {time_optimized:.4f}s\")\n",
    "print(f\"  Resultado: {optimized.num_rows} linhas, {optimized.num_columns} colunas\")\n",
    "\n",
    "improvement = ((time_naive - time_optimized) / time_naive * 100)\n",
    "print(f\"\\nMelhoria de performance: {improvement:.1f}% mais r√°pido\")\n",
    "\n",
    "# 3.5.4 Benchmark Completo com Dataset Scanner\n",
    "print(\"\\n4. Benchmark: Diferentes estrat√©gias de acesso\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "dataset = ds.dataset(parquet_optimize, format='parquet')\n",
    "\n",
    "# Estrat√©gia 1: Leitura padr√£o sem otimiza√ß√£o\n",
    "print(\"\\nEstrat√©gia 1: Scanner b√°sico (sem otimiza√ß√µes)\")\n",
    "start = time.time()\n",
    "scanner_basic = dataset.scanner()\n",
    "result_basic = scanner_basic.to_table()\n",
    "time_basic = time.time() - start\n",
    "print(f\"  Tempo: {time_basic:.4f}s\")\n",
    "print(f\"  Linhas: {result_basic.num_rows}\")\n",
    "\n",
    "# Estrat√©gia 2: Scanner com proje√ß√£o\n",
    "print(\"\\nEstrat√©gia 2: Scanner com proje√ß√£o de 3 colunas\")\n",
    "start = time.time()\n",
    "scanner_proj = dataset.scanner(columns=['id', 'salary', 'department'])\n",
    "result_proj = scanner_proj.to_table()\n",
    "time_proj = time.time() - start\n",
    "print(f\"  Tempo: {time_proj:.4f}s\")\n",
    "print(f\"  Linhas: {result_proj.num_rows}\")\n",
    "\n",
    "# Estrat√©gia 3: Scanner com filtro\n",
    "print(\"\\nEstrat√©gia 3: Scanner com filtro (salary > 120000)\")\n",
    "start = time.time()\n",
    "scanner_filter = dataset.scanner(\n",
    "    filter=ds.field('salary') > 120000\n",
    ")\n",
    "result_filter = scanner_filter.to_table()\n",
    "time_filter = time.time() - start\n",
    "print(f\"  Tempo: {time_filter:.4f}s\")\n",
    "print(f\"  Linhas: {result_filter.num_rows}\")\n",
    "\n",
    "# Estrat√©gia 4: Scanner otimizado (filtro + proje√ß√£o)\n",
    "print(\"\\nEstrat√©gia 4: Scanner otimizado (filtro + proje√ß√£o)\")\n",
    "start = time.time()\n",
    "scanner_optimized = dataset.scanner(\n",
    "    columns=['id', 'customer_name', 'salary', 'department'],\n",
    "    filter=(ds.field('salary') > 120000) & (ds.field('country') == 'USA')\n",
    ")\n",
    "result_optimized = scanner_optimized.to_table()\n",
    "time_optimized_scan = time.time() - start\n",
    "print(f\"  Tempo: {time_optimized_scan:.4f}s\")\n",
    "print(f\"  Linhas: {result_optimized.num_rows}\")\n",
    "\n",
    "# Resumo de performance\n",
    "print(\"\\nüìä Resumo de Performance:\")\n",
    "print(\"-\" * 40)\n",
    "results = [\n",
    "    (\"Scanner b√°sico\", time_basic),\n",
    "    (\"Scanner com proje√ß√£o\", time_proj),\n",
    "    (\"Scanner com filtro\", time_filter),\n",
    "    (\"Scanner otimizado\", time_optimized_scan)\n",
    "]\n",
    "results.sort(key=lambda x: x[1])\n",
    "\n",
    "for i, (strategy, elapsed) in enumerate(results, 1):\n",
    "    pct = (elapsed / results[-1][1] - 1) * 100 if i > 1 else 0\n",
    "    faster = f\"({pct:.1f}% mais r√°pido)\" if pct != 0 else \"(refer√™ncia)\"\n",
    "    print(f\"  {i}. {strategy:.<30} {elapsed:.4f}s {faster}\")\n",
    "\n",
    "# An√°lise com DuckDB\n",
    "print(\"\\nAn√°lise com DuckDB (push-down autom√°tico):\")\n",
    "result_duckdb = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        COUNT(CASE WHEN salary > 120000 THEN 1 END) as high_salary,\n",
    "        AVG(salary) as avg_salary,\n",
    "        COUNT(DISTINCT country) as countries\n",
    "    FROM read_parquet('{parquet_optimize}')\n",
    "\"\"\").df()\n",
    "\n",
    "print(result_duckdb)\n",
    "\n",
    "# Limpeza\n",
    "shutil.rmtree(optimize_dir, ignore_errors=True)\n",
    "\n",
    "print(\"\\n‚úÖ Filtros e Projections demonstram o poder das otimiza√ß√µes!\")\n",
    "print(\"   - Push-down filters reduzem dados lidos do disco\")\n",
    "print(\"   - Column projection reduz mem√≥ria utilizada\")\n",
    "print(\"   - Combinadas = m√°xima performance!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
