{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d4d175",
   "metadata": {},
   "source": [
    "# üåä Cap√≠tulo 06: Streaming e Batches\n",
    "## Curso: Apache Arrow + DuckDB\n",
    "\n",
    "Neste cap√≠tulo, exploraremos como processar grandes volumes de dados de forma eficiente utilizando streaming e o conceito de batches (lotes) do Apache Arrow integrados ao DuckDB.\n",
    "\n",
    "### T√≥picos abordados:\n",
    "- **Record Batches**: A unidade fundamental de dados no Arrow.\n",
    "- **RecordBatchReader**: Interface para leitura de fluxos de dados.\n",
    "- **Processamento incremental**: Como evitar carregar tudo na mem√≥ria.\n",
    "- **Iteradores e geradores**: Integra√ß√£o com fluxos Python.\n",
    "- **Controle de mem√≥ria**: Melhores pr√°ticas para efici√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76d5e3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configura√ß√£o para sa√≠da correta no Windows (opcional em Notebooks, mas boa pr√°tica)\n",
    "# sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41058ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando dados de exemplo...\n",
      "Tabela PyArrow criada: 1000 linhas\n",
      "Conex√£o DuckDB estabelecida.\n"
     ]
    }
   ],
   "source": [
    "# Gerando dados de exemplo globais\n",
    "print(\"Gerando dados de exemplo...\")\n",
    "data = pa.table({\n",
    "    'id': range(1000),\n",
    "    'valor': np.random.randn(1000),\n",
    "    'categoria': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "})\n",
    "print(f\"Tabela PyArrow criada: {data.num_rows} linhas\")\n",
    "\n",
    "# Conex√£o DuckDB em mem√≥ria\n",
    "con = duckdb.connect()\n",
    "print(\"Conex√£o DuckDB estabelecida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58013717",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Record Batches\n",
    "\n",
    "O `RecordBatch` √© uma cole√ß√£o de arrays de mesmo tamanho. Enquanto uma `Table` pode ser composta por v√°rios peda√ßos (chunks), um `RecordBatch` √© uma estrutura cont√≠gua na mem√≥ria, ideal para processamento em streaming.\n",
    "\n",
    "### Exemplo Pr√°tico:\n",
    "Vamos converter nossa tabela para uma lista de batches e inspecion√°-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c691139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de batches gerados: 5\n",
      "Tipo do objeto: <class 'pyarrow.lib.RecordBatch'>\n",
      "N√∫mero de linhas no batch: 200\n",
      "\n",
      "Resultado da consulta DuckDB sobre um RecordBatch:\n",
      "   count_star()  avg(valor)\n",
      "0           200   -0.093368\n"
     ]
    }
   ],
   "source": [
    "# Convertendo tabela para batches com tamanho m√°ximo de 200 linhas cada\n",
    "batches = data.to_batches(max_chunksize=200)\n",
    "print(f\"Total de batches gerados: {len(batches)}\")\n",
    "\n",
    "# Inspecionando o primeiro batch\n",
    "batch = batches[0]\n",
    "print(f\"Tipo do objeto: {type(batch)}\")\n",
    "print(f\"N√∫mero de linhas no batch: {batch.num_rows}\")\n",
    "\n",
    "# O DuckDB pode consultar um batch individual como se fosse uma tabela comum\n",
    "result = con.execute(\"SELECT count(*), avg(valor) FROM batch\").df()\n",
    "print(\"\\nResultado da consulta DuckDB sobre um RecordBatch:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90810c1e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ RecordBatchReader\n",
    "\n",
    "O `RecordBatchReader` √© um iterador que permite ler batches um por um sem carregar todo o conjunto na mem√≥ria. Ele √© fundamental para fluxos de dados (streaming).\n",
    "\n",
    "### Exemplo Pr√°tico:\n",
    "Gerando um reader a partir de uma consulta DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81368538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo do reader: <class 'pyarrow.lib.RecordBatchReader'>\n",
      "Batch lido: 150 linhas\n",
      "Linhas restantes lidas de uma vez: 333\n"
     ]
    }
   ],
   "source": [
    "# Criando um RecordBatchReader a partir de uma consulta no DuckDB\n",
    "# O par√¢metro rows_per_batch controla o tamanho ideal do streaming\n",
    "reader = con.execute(\"SELECT * FROM data WHERE valor > 0\").fetch_record_batch(rows_per_batch=150)\n",
    "\n",
    "print(f\"Tipo do reader: {type(reader)}\")\n",
    "\n",
    "# Lendo o pr√≥ximo batch dispon√≠vel\n",
    "try:\n",
    "    batch = reader.read_next_batch()\n",
    "    print(f\"Batch lido: {batch.num_rows} linhas\")\n",
    "except StopIteration:\n",
    "    print(\"Fim do stream\")\n",
    "\n",
    "# Podemos ler o restante como uma tabela Arrow completa se necess√°rio\n",
    "final_table = reader.read_all()\n",
    "print(f\"Linhas restantes lidas de uma vez: {final_table.num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5adfa2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Processamento Incremental\n",
    "\n",
    "Processar dados de forma incremental √© a chave para trabalhar com datasets maiores que a mem√≥ria RAM dispon√≠vel.\n",
    "\n",
    "### Exemplo Pr√°tico:\n",
    "Iterando sobre batches para calcular uma soma acumulada sem carregar tudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03c7a74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando processamento incremental...\n",
      "-> Processado batch de 250 linhas. Soma parcial: -13.53\n",
      "-> Processado batch de 250 linhas. Soma parcial: -11.11\n",
      "-> Processado batch de 250 linhas. Soma parcial: -37.01\n",
      "-> Processado batch de 250 linhas. Soma parcial: -44.17\n",
      "\n",
      "Processamento conclu√≠do!\n",
      "Total de linhas vistas: 1000\n",
      "Soma total calculada: -44.17\n",
      "Valida√ß√£o DuckDB: -44.17\n"
     ]
    }
   ],
   "source": [
    "# Refazendo o reader\n",
    "reader = con.execute(\"SELECT valor FROM data\").fetch_record_batch(rows_per_batch=250)\n",
    "\n",
    "total_rows = 0\n",
    "running_sum = 0.0\n",
    "\n",
    "print(\"Iniciando processamento incremental...\")\n",
    "for batch in reader:\n",
    "    # O batch √© um RecordBatch. Podemos convert√™-lo para pandas ou numpy para processar\n",
    "    # ou usar fun√ß√µes de computa√ß√£o do pr√≥prio Arrow\n",
    "    chunk_sum = np.sum(batch.column('valor').to_numpy())\n",
    "    running_sum += chunk_sum\n",
    "    total_rows += batch.num_rows\n",
    "    print(f\"-> Processado batch de {batch.num_rows} linhas. Soma parcial: {running_sum:.2f}\")\n",
    "\n",
    "print(f\"\\nProcessamento conclu√≠do!\")\n",
    "print(f\"Total de linhas vistas: {total_rows}\")\n",
    "print(f\"Soma total calculada: {running_sum:.2f}\")\n",
    "\n",
    "# Valida√ß√£o com DuckDB simples\n",
    "final_sum = con.execute(\"SELECT sum(valor) FROM data\").fetchone()[0]\n",
    "print(f\"Valida√ß√£o DuckDB: {final_sum:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc86696",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Iteradores e Geradores\n",
    "\n",
    "Podemos integrar o Apache Arrow com geradores Python, permitindo que qualquer fluxo de dados seja exposto como um `RecordBatchReader`.\n",
    "\n",
    "### Exemplo Pr√°tico:\n",
    "Criando um gerador de batches e convertendo-o em um Reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84ca6b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reader criado de gerador: <class 'pyarrow.lib.RecordBatchReader'>\n",
      "\n",
      "Resultado da consulta sobre o gerador:\n",
      "   avg(rand)\n",
      "0   0.489407\n"
     ]
    }
   ],
   "source": [
    "def custom_batch_gen(row_count=1000, batch_size=200):\n",
    "    \"\"\"Gera RecordBatches de dados aleat√≥rios sob demanda.\"\"\"\n",
    "    for i in range(0, row_count, batch_size):\n",
    "        yield pa.RecordBatch.from_pydict({\n",
    "            'seq': range(i, min(i + batch_size, row_count)),\n",
    "            'rand': np.random.rand(min(batch_size, row_count - i))\n",
    "        })\n",
    "\n",
    "# Criando o schema necess√°rio para o reader\n",
    "schema = pa.schema([\n",
    "    ('seq', pa.int64()),\n",
    "    ('rand', pa.float64())\n",
    "])\n",
    "\n",
    "# Criando o RecordBatchReader a partir do gerador\n",
    "gen = custom_batch_gen()\n",
    "reader_from_gen = pa.RecordBatchReader.from_batches(schema, gen)\n",
    "\n",
    "print(f\"Reader criado de gerador: {type(reader_from_gen)}\")\n",
    "\n",
    "# Consumindo via DuckDB (o DuckDB entende RecordBatchReader como fonte)\n",
    "result = con.execute(\"SELECT avg(rand) FROM reader_from_gen\").df()\n",
    "print(\"\\nResultado da consulta sobre o gerador:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059f650",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Controle de Mem√≥ria\n",
    "\n",
    "Gerenciar o tamanho dos batches e liberar refer√™ncias √© crucial no processamento de grandes volumes de dados.\n",
    "\n",
    "### Exemplo Pr√°tico:\n",
    "Aferi√ß√£o de tamanho de mem√≥ria e limpeza de vari√°veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff1f0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de mem√≥ria inicial: 118.88 MB\n",
      "Tamanho estimado da tabela 'data': 20.51 KB\n",
      "Uso de mem√≥ria ap√≥s limpeza: 118.90 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # em MB\n",
    "\n",
    "print(f\"Uso de mem√≥ria inicial: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# Tabelas e Batches ocupam mem√≥ria\n",
    "print(f\"Tamanho estimado da tabela 'data': {data.nbytes / 1024:.2f} KB\")\n",
    "\n",
    "# Para datasets gigantes, evite read_all()\n",
    "# Prefira processar o iterator e descartar o batch o quanto antes\n",
    "\n",
    "# Exemplo de limpeza manual se necess√°rio\n",
    "vars_to_clean = ['data', 'batches', 'final_table']\n",
    "for var in vars_to_clean:\n",
    "    if var in locals():\n",
    "        del locals()[var]\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Uso de mem√≥ria ap√≥s limpeza: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c780325",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Conclus√£o do Cap√≠tulo 06\n",
    "\n",
    "Neste cap√≠tulo aprendemos:\n",
    "1. Trabalhar com **RecordBatches** para processamento cont√≠guo.\n",
    "2. Utilizar o **RecordBatchReader** para ler fluxos de dados sem sobrecarregar a mem√≥ria.\n",
    "3. T√©cnicas de **processamento incremental** integrando DuckDB e Arrow.\n",
    "4. Como expor geradores Python como leitores Arrow.\n",
    "5. A import√¢ncia do controle de mem√≥ria no mundo de Big Data.\n",
    "\n",
    "Pr√≥ximo passo: **Cap√≠tulo 07: IPC e Serializa√ß√£o**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
