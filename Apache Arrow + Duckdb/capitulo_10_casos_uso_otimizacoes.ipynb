{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdbbd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow duckdb pandas numpy -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c36cf",
   "metadata": {},
   "source": [
    "# ðŸ“¦ InstalaÃ§Ã£o de Pacotes\n",
    "\n",
    "Antes de comeÃ§ar, vamos instalar os pacotes necessÃ¡rios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 10 Casos Uso Otimizacoes\n",
    "\n",
    "Notebook gerado automaticamente a partir do cÃ³digo fonte python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154becfd",
   "metadata": {},
   "source": [
    "## ðŸ“š ImportaÃ§Ã£o de Bibliotecas\n",
    "\n",
    "Importando as bibliotecas necessÃ¡rias para o capÃ­tulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8225d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ConfiguraÃ§Ãµes de exibiÃ§Ã£o\n",
    "pd.options.display.max_columns = None\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ead5810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- BEST PRACTICES AVANÃ‡ADAS: PRODUCTION-READY PATTERNS ---\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¾ FASE 1: Gerenciamento de MemÃ³ria e Recursos\n",
      "\n",
      "  âœ“ ConfiguraÃ§Ãµes de produÃ§Ã£o aplicadas:\n",
      "                    name            value\n",
      "            memory_limit        953.6 MiB\n",
      "preserve_insertion_order            false\n",
      "          temp_directory data_output/temp\n",
      "                 threads                4\n",
      "\n",
      "  â†’ Registrando tabelas com Zero-Copy:\n",
      "    âœ“ Tabela registrada em 0.0000s (zero-copy, sem duplicaÃ§Ã£o)\n",
      "\n",
      "âš™ï¸ FASE 2: Processamento em Batches para Grandes Volumes\n",
      "\n",
      "  â†’ Processando 50,000 registros em 5 batches de 10,000\n",
      "  âœ“ Processamento em batches concluÃ­do em 0.0174s\n",
      "\n",
      "  â†’ Resultado Consolidado:\n",
      " total_batches total_registros  preco_medio_geral qtd_total_geral\n",
      "             5           50000             2520.1          500223\n",
      "\n",
      "ðŸ›¡ï¸ FASE 3: Error Handling Robusto e ValidaÃ§Ã£o de Dados\n",
      "\n",
      "  â†’ Testando queries com error handling:\n",
      "  âœ“ Query VÃ¡lida: 5 linhas em 0.0041s\n",
      "  âœ— Query InvÃ¡lida: ERRO - Catalog Error: Table with name tabela_nao_existe does not exist!\n",
      "Did you mean \"pragma_database_list\"\n",
      "  âš ï¸ Query Vazia: Retornou 0 linhas\n",
      "\n",
      "ðŸ“ FASE 4: ValidaÃ§Ã£o de Schema e Tipos\n",
      "\n",
      "  â†’ Validando schema da tabela de vendas:\n",
      "  âš ï¸ Problemas encontrados:\n",
      "    â€¢ Colunas extras: {'desconto_percentual', 'vendedor_id', 'cliente_id', 'rating', 'status', 'regiao'}\n",
      "    â€¢ Coluna 'quantidade': tipo esperado int64, obtido int32\n",
      "    â€¢ Coluna 'data_venda': tipo esperado timestamp[ns], obtido timestamp[us]\n",
      "\n",
      "ðŸ”„ FASE 5: Connection Pooling e Reuso de Recursos\n",
      "\n",
      "  âœ“ Connection pool criado: {'total': 3, 'available': 3, 'in_use': 0}\n",
      "\n",
      "  â†’ Simulando queries concorrentes com pool:\n",
      "    â€¢ Query 1: conexÃ£o #0, resultado: 50,000 registros\n",
      "    â€¢ Query 2: conexÃ£o #1, resultado: 50,000 registros\n",
      "    â€¢ Query 3: conexÃ£o #2, resultado: 50,000 registros\n",
      "    â€¢ Query 4: conexÃ£o #0, resultado: 50,000 registros\n",
      "    â€¢ Query 5: conexÃ£o #1, resultado: 50,000 registros\n",
      "  âœ“ Estado final do pool: {'total': 3, 'available': 3, 'in_use': 0}\n",
      "\n",
      "ðŸ“Š FASE 6: Monitoring e Profiling de Performance\n",
      "\n",
      "  â†’ Executando query com profiling habilitado:\n",
      "    âœ“ Query executada em 0.0133s\n",
      "    âœ“ Resultados: 20 linhas\n",
      "    âœ“ Profile salvo em: data_output/profile.json\n",
      "\n",
      "  â†’ EstatÃ­sticas de MemÃ³ria do DuckDB:\n",
      "db_size  block_size  total_blocks  used_blocks  free_blocks\n",
      "0 bytes           0             0            0            0\n",
      "\n",
      "================================================================================\n",
      "âœ¨ BEST PRACTICES - RESUMO\n",
      "================================================================================\n",
      "  âœ“ Memory Management: ConfiguraÃ§Ãµes otimizadas aplicadas\n",
      "  âœ“ Zero-Copy: Tabelas registradas sem duplicaÃ§Ã£o\n",
      "  âœ“ Batch Processing: 50,000 registros em batches de 10k\n",
      "  âœ“ Error Handling: ValidaÃ§Ã£o robusta implementada\n",
      "  âœ“ Schema Validation: VerificaÃ§Ã£o de tipos e colunas\n",
      "  âœ“ Connection Pooling: Pool de 3 conexÃµes gerenciado\n",
      "  âœ“ Monitoring: Profiling e estatÃ­sticas habilitados\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ FIM DO CAPÃTULO 10: CASOS DE USO E OTIMIZAÃ‡Ã•ES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“š TÃ³picos Cobertos:\n",
      "  1. ETL Pipeline AvanÃ§ado (Extract, Transform, Load)\n",
      "  2. Data Lake Architecture Multi-NÃ­vel\n",
      "  3. Incremental Loading com CDC e Merge/Upsert\n",
      "  4. Query Optimization com Window Functions e JOINs\n",
      "  5. Best Practices para ProduÃ§Ã£o\n",
      "\n",
      "âœ… Todas as implementaÃ§Ãµes foram executadas com sucesso!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"--- {'BEST PRACTICES AVANÃ‡ADAS: Production-Ready Patterns'.upper()} ---\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Melhores prÃ¡ticas para ambientes de produÃ§Ã£o\n",
    "\n",
    "# ==== 1. MEMORY MANAGEMENT ====\n",
    "print(\"ðŸ’¾ FASE 1: Gerenciamento de MemÃ³ria e Recursos\\n\")\n",
    "\n",
    "# 1.1 ConfiguraÃ§Ã£o otimizada do DuckDB\n",
    "con_prod = duckdb.connect(':memory:')  # In-memory para performance\n",
    "con_prod.execute(\"SET threads TO 4\")\n",
    "con_prod.execute(\"SET memory_limit='1GB'\")\n",
    "con_prod.execute(\"SET temp_directory='data_output/temp'\")\n",
    "con_prod.execute(\"SET preserve_insertion_order=false\")  # Mais rÃ¡pido\n",
    "\n",
    "print(\"  âœ“ ConfiguraÃ§Ãµes de produÃ§Ã£o aplicadas:\")\n",
    "configs = con_prod.execute(\"\"\"\n",
    "    SELECT name, value \n",
    "    FROM duckdb_settings() \n",
    "    WHERE name IN ('threads', 'memory_limit', 'temp_directory', 'preserve_insertion_order')\n",
    "\"\"\").fetchdf()\n",
    "print(configs.to_string(index=False))\n",
    "\n",
    "# 1.2 Zero-Copy com Arrow (registrar tabelas sem copiar)\n",
    "print(\"\\n  â†’ Registrando tabelas com Zero-Copy:\")\n",
    "start_register = time.time()\n",
    "con_prod.register('vendas_view', data)\n",
    "register_time = time.time() - start_register\n",
    "print(f\"    âœ“ Tabela registrada em {register_time:.4f}s (zero-copy, sem duplicaÃ§Ã£o)\")\n",
    "\n",
    "# ==== 2. BATCH PROCESSING ====\n",
    "print(f\"\\nâš™ï¸ FASE 2: Processamento em Batches para Grandes Volumes\\n\")\n",
    "\n",
    "def process_in_batches(table, batch_size=10000, operation_func=None):\n",
    "    \"\"\"Processa uma Arrow Table em batches\"\"\"\n",
    "    results = []\n",
    "    num_batches = (table.num_rows + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"  â†’ Processando {table.num_rows:,} registros em {num_batches} batches de {batch_size:,}\")\n",
    "    \n",
    "    for i in range(0, table.num_rows, batch_size):\n",
    "        batch_data = table.slice(i, min(batch_size, table.num_rows - i))\n",
    "        \n",
    "        # Aplicar operaÃ§Ã£o customizada\n",
    "        if operation_func:\n",
    "            result = operation_func(batch_data, i // batch_size + 1)\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Exemplo: processar em batches\n",
    "def batch_aggregation(batch_table, batch_num):\n",
    "    \"\"\"AgregaÃ§Ã£o customizada por batch\"\"\"\n",
    "    con_prod.register(f'batch_{batch_num}', batch_table)\n",
    "    agg = con_prod.execute(f\"\"\"\n",
    "        SELECT \n",
    "            {batch_num} as batch_id,\n",
    "            COUNT(*) as registros,\n",
    "            ROUND(AVG(preco_unitario), 2) as preco_medio,\n",
    "            SUM(quantidade) as qtd_total\n",
    "        FROM batch_{batch_num}\n",
    "    \"\"\").fetch_arrow_table()\n",
    "    con_prod.unregister(f'batch_{batch_num}')  # Liberar memÃ³ria\n",
    "    return agg\n",
    "\n",
    "start_batch = time.time()\n",
    "batch_results = process_in_batches(data, batch_size=10000, operation_func=batch_aggregation)\n",
    "batch_time = time.time() - start_batch\n",
    "\n",
    "# Consolidar resultados\n",
    "con_prod.register('all_batches', pa.concat_tables(batch_results))\n",
    "consolidated = con_prod.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT batch_id) as total_batches,\n",
    "        SUM(registros) as total_registros,\n",
    "        ROUND(AVG(preco_medio), 2) as preco_medio_geral,\n",
    "        SUM(qtd_total) as qtd_total_geral\n",
    "    FROM all_batches\n",
    "\"\"\").fetch_arrow_table()\n",
    "\n",
    "print(f\"  âœ“ Processamento em batches concluÃ­do em {batch_time:.4f}s\")\n",
    "print(\"\\n  â†’ Resultado Consolidado:\")\n",
    "print(consolidated.to_pandas().to_string(index=False))\n",
    "\n",
    "# ==== 3. ERROR HANDLING E VALIDAÃ‡ÃƒO ====\n",
    "print(f\"\\nðŸ›¡ï¸ FASE 3: Error Handling Robusto e ValidaÃ§Ã£o de Dados\\n\")\n",
    "\n",
    "def safe_query_execution(connection, query, query_name=\"Query\"):\n",
    "    \"\"\"ExecuÃ§Ã£o segura com error handling\"\"\"\n",
    "    try:\n",
    "        start = time.time()\n",
    "        result = connection.execute(query).fetch_arrow_table()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # ValidaÃ§Ãµes\n",
    "        if result.num_rows == 0:\n",
    "            print(f\"  âš ï¸ {query_name}: Retornou 0 linhas\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  âœ“ {query_name}: {result.num_rows:,} linhas em {elapsed:.4f}s\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— {query_name}: ERRO - {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "# Testar com queries vÃ¡lidas e invÃ¡lidas\n",
    "print(\"  â†’ Testando queries com error handling:\")\n",
    "\n",
    "# Query vÃ¡lida\n",
    "valid_query = \"SELECT regiao, COUNT(*) as total FROM vendas_view GROUP BY regiao\"\n",
    "result1 = safe_query_execution(con_prod, valid_query, \"Query VÃ¡lida\")\n",
    "\n",
    "# Query com tabela inexistente (gera erro)\n",
    "invalid_query = \"SELECT * FROM tabela_nao_existe\"\n",
    "result2 = safe_query_execution(con_prod, invalid_query, \"Query InvÃ¡lida\")\n",
    "\n",
    "# Query que retorna vazio\n",
    "empty_query = \"SELECT * FROM vendas_view WHERE 1=0\"\n",
    "result3 = safe_query_execution(con_prod, empty_query, \"Query Vazia\")\n",
    "\n",
    "# ==== 4. SCHEMA VALIDATION ====\n",
    "print(f\"\\nðŸ“ FASE 4: ValidaÃ§Ã£o de Schema e Tipos\\n\")\n",
    "\n",
    "def validate_schema(table, expected_schema):\n",
    "    \"\"\"Valida se a tabela tem o schema esperado\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Verificar colunas\n",
    "    expected_cols = set(expected_schema.names)\n",
    "    actual_cols = set(table.schema.names)\n",
    "    \n",
    "    missing = expected_cols - actual_cols\n",
    "    extra = actual_cols - expected_cols\n",
    "    \n",
    "    if missing:\n",
    "        issues.append(f\"Colunas faltando: {missing}\")\n",
    "    if extra:\n",
    "        issues.append(f\"Colunas extras: {extra}\")\n",
    "    \n",
    "    # Verificar tipos das colunas comuns\n",
    "    for col in expected_cols.intersection(actual_cols):\n",
    "        expected_type = expected_schema.field(col).type\n",
    "        actual_type = table.schema.field(col).type\n",
    "        if expected_type != actual_type:\n",
    "            issues.append(f\"Coluna '{col}': tipo esperado {expected_type}, obtido {actual_type}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Definir schema esperado\n",
    "expected_schema = pa.schema([\n",
    "    ('id', pa.int64()),\n",
    "    ('data_venda', pa.timestamp('ns')),\n",
    "    ('produto', pa.string()),\n",
    "    ('quantidade', pa.int64()),\n",
    "    ('preco_unitario', pa.float64())\n",
    "])\n",
    "\n",
    "print(\"  â†’ Validando schema da tabela de vendas:\")\n",
    "validation_issues = validate_schema(data, expected_schema)\n",
    "if validation_issues:\n",
    "    print(\"  âš ï¸ Problemas encontrados:\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"    â€¢ {issue}\")\n",
    "else:\n",
    "    print(\"  âœ“ Schema vÃ¡lido!\")\n",
    "\n",
    "# ==== 5. CONNECTION POOLING E REUSO ====\n",
    "print(f\"\\nðŸ”„ FASE 5: Connection Pooling e Reuso de Recursos\\n\")\n",
    "\n",
    "class DuckDBConnectionPool:\n",
    "    \"\"\"Pool simples de conexÃµes DuckDB\"\"\"\n",
    "    def __init__(self, pool_size=3):\n",
    "        self.pool = [duckdb.connect(':memory:') for _ in range(pool_size)]\n",
    "        self.available = list(range(pool_size))\n",
    "        self.in_use = {}\n",
    "    \n",
    "    def acquire(self):\n",
    "        if self.available:\n",
    "            conn_id = self.available.pop(0)\n",
    "            self.in_use[conn_id] = self.pool[conn_id]\n",
    "            return conn_id, self.pool[conn_id]\n",
    "        return None, None\n",
    "    \n",
    "    def release(self, conn_id):\n",
    "        if conn_id in self.in_use:\n",
    "            del self.in_use[conn_id]\n",
    "            self.available.append(conn_id)\n",
    "    \n",
    "    def stats(self):\n",
    "        return {\n",
    "            'total': len(self.pool),\n",
    "            'available': len(self.available),\n",
    "            'in_use': len(self.in_use)\n",
    "        }\n",
    "\n",
    "# Criar pool\n",
    "pool = DuckDBConnectionPool(pool_size=3)\n",
    "print(f\"  âœ“ Connection pool criado: {pool.stats()}\")\n",
    "\n",
    "# Simular uso do pool\n",
    "print(\"\\n  â†’ Simulando queries concorrentes com pool:\")\n",
    "for i in range(5):\n",
    "    conn_id, conn = pool.acquire()\n",
    "    if conn:\n",
    "        conn.register('data_temp', data)\n",
    "        result = conn.execute(\"SELECT COUNT(*) as total FROM data_temp\").fetchone()\n",
    "        print(f\"    â€¢ Query {i+1}: conexÃ£o #{conn_id}, resultado: {result[0]:,} registros\")\n",
    "        pool.release(conn_id)\n",
    "    else:\n",
    "        print(f\"    â€¢ Query {i+1}: aguardando conexÃ£o disponÃ­vel...\")\n",
    "\n",
    "print(f\"  âœ“ Estado final do pool: {pool.stats()}\")\n",
    "\n",
    "# ==== 6. MONITORING E PROFILING ====\n",
    "print(f\"\\nðŸ“Š FASE 6: Monitoring e Profiling de Performance\\n\")\n",
    "\n",
    "# Habilitar profiling detalhado\n",
    "con_prod.execute(\"PRAGMA enable_profiling='json'\")\n",
    "con_prod.execute(\"PRAGMA profiling_output='data_output/profile.json'\")\n",
    "\n",
    "monitoring_query = \"\"\"\n",
    "    SELECT \n",
    "        regiao,\n",
    "        produto,\n",
    "        COUNT(*) as vendas,\n",
    "        SUM(quantidade * preco_unitario) as receita\n",
    "    FROM vendas_view\n",
    "    WHERE status = 'Completado'\n",
    "    GROUP BY regiao, produto\n",
    "    HAVING vendas > 100\n",
    "    ORDER BY receita DESC\n",
    "    LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "print(\"  â†’ Executando query com profiling habilitado:\")\n",
    "start_profile = time.time()\n",
    "profile_result = con_prod.execute(monitoring_query).fetch_arrow_table()\n",
    "profile_time = time.time() - start_profile\n",
    "\n",
    "print(f\"    âœ“ Query executada em {profile_time:.4f}s\")\n",
    "print(f\"    âœ“ Resultados: {profile_result.num_rows} linhas\")\n",
    "print(f\"    âœ“ Profile salvo em: data_output/profile.json\")\n",
    "\n",
    "# Obter estatÃ­sticas de memÃ³ria\n",
    "memory_stats = con_prod.execute(\"\"\"\n",
    "    SELECT \n",
    "        database_size as db_size,\n",
    "        block_size,\n",
    "        total_blocks,\n",
    "        used_blocks,\n",
    "        free_blocks\n",
    "    FROM pragma_database_size()\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"\\n  â†’ EstatÃ­sticas de MemÃ³ria do DuckDB:\")\n",
    "print(memory_stats.to_string(index=False))\n",
    "\n",
    "# ==== 7. RESUMO FINAL ====\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ¨ BEST PRACTICES - RESUMO\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  âœ“ Memory Management: ConfiguraÃ§Ãµes otimizadas aplicadas\")\n",
    "print(f\"  âœ“ Zero-Copy: Tabelas registradas sem duplicaÃ§Ã£o\")\n",
    "print(f\"  âœ“ Batch Processing: {data.num_rows:,} registros em batches de 10k\")\n",
    "print(f\"  âœ“ Error Handling: ValidaÃ§Ã£o robusta implementada\")\n",
    "print(f\"  âœ“ Schema Validation: VerificaÃ§Ã£o de tipos e colunas\")\n",
    "print(f\"  âœ“ Connection Pooling: Pool de 3 conexÃµes gerenciado\")\n",
    "print(f\"  âœ“ Monitoring: Profiling e estatÃ­sticas habilitados\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ðŸŽ¯ FIM DO CAPÃTULO 10: CASOS DE USO E OTIMIZAÃ‡Ã•ES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“š TÃ³picos Cobertos:\")\n",
    "print(\"  1. ETL Pipeline AvanÃ§ado (Extract, Transform, Load)\")\n",
    "print(\"  2. Data Lake Architecture Multi-NÃ­vel\")\n",
    "print(\"  3. Incremental Loading com CDC e Merge/Upsert\")\n",
    "print(\"  4. Query Optimization com Window Functions e JOINs\")\n",
    "print(\"  5. Best Practices para ProduÃ§Ã£o\")\n",
    "print(\"\\nâœ… Todas as implementaÃ§Ãµes foram executadas com sucesso!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95968bc",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Best Practices\n",
    "\n",
    "Melhores prÃ¡ticas no uso de Arrow e DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d32958c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- OTIMIZAÃ‡ÃƒO AVANÃ‡ADA DE QUERIES ---\n",
      "================================================================================\n",
      "\n",
      "âš™ï¸ FASE 1: Setup de Benchmark\n",
      "  âœ“ Dataset registrado: 50,000 vendas\n",
      "  âœ“ Colunas: 11\n",
      "\n",
      "ðŸ” FASE 2: Comparativo - Query NÃ£o Otimizada vs Otimizada\n",
      "\n",
      "  âŒ Query NÃƒO OTIMIZADA:\n",
      "    â€¢ Tempo: 0.0357s\n",
      "    â€¢ Registros processados: 50,000\n",
      "    â€¢ TÃ©cnica: Full table scan + Python aggregation\n",
      "\n",
      "  âœ… Query OTIMIZADA:\n",
      "    â€¢ Tempo: 0.0111s\n",
      "    â€¢ Speedup: 3.23x mais rÃ¡pido\n",
      "    â€¢ TÃ©cnicas: Filter pushdown + Projection pushdown + SQL aggregation\n",
      "\n",
      "  ðŸ“Š Melhoria de Performance: 69.0%\n",
      "\n",
      "ðŸ“‹ FASE 3: AnÃ¡lise de Planos de ExecuÃ§Ã£o\n",
      "\n",
      "  â†’ EXPLAIN (Plano LÃ³gico):\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚            CTE            â”‚\n",
      "â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚\n",
      "â”‚         CTE Name:         â”‚\n",
      "â”‚    vendas_por_vendedor    â”‚\n",
      "â”‚                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚       Table Index: 0      â”‚              â”‚\n",
      "â”‚                           â”‚              â”‚\n",
      "â”‚           ~1 row          â”‚              â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚       HASH_GROUP_BY       â”‚â”‚          ORDER_BY         â”‚\n",
      "â”‚    â”€â”€...\n",
      "\n",
      "  â†’ EXPLAIN ANALYZE (Plano FÃ­sico com MÃ©tricas):\n",
      "    â€¢ Tempo total de execuÃ§Ã£o: 0.0208s\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\n",
      "â”‚â”‚    Query Profiling Information    â”‚â”‚\n",
      "â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "EXPLAIN ANALYZE      WITH vendas_por_vendedor AS (         SELECT              vendedor_id,             produto,             regiao,             COUNT(*) as total_vendas,             SUM(quantidade * preco_unitario * (1 - desconto_percentual/100.0)) as receita         FROM vendas         WHERE status = 'Completado'             AND data_venda >= '2023-06-01'         GROUP BY vendedor_id, produto, regiao     ),     ranking_vendedores AS (         SELECT              vendedor_id,             SUM(receita) as receita_total,             COUNT(DISTINCT produto) as produtos_vendidos,             RANK(...\n",
      "\n",
      "  â†’ Resultados: 400 linhas retornadas\n",
      "\n",
      "ðŸªŸ FASE 4: Window Functions AvanÃ§adas\n",
      "\n",
      "  âœ“ Query com Window Functions executada em: 0.0233s\n",
      "  âœ“ Resultados: 720 linhas\n",
      "\n",
      "  â†’ Amostra dos Resultados (Top 10):\n",
      " produto regiao        mes  receita_mensal  media_movel_3m  rank_na_regiao  pct_da_regiao\n",
      " Headset Centro 2024-06-01     329907.3455      1085991.41               1          23.46\n",
      "   Mouse Centro 2024-06-01     237009.3210       781195.51               2          16.85\n",
      "Notebook Centro 2024-06-01     197794.0610       931973.54               3          14.07\n",
      "  Webcam Centro 2024-06-01     162467.6290       915535.55               4          11.55\n",
      " Teclado Centro 2024-06-01     157624.7045       799136.25               5          11.21\n",
      "     RAM Centro 2024-06-01     125002.3040       806973.05               6           8.89\n",
      " Monitor Centro 2024-06-01     103446.5940       771649.35               7           7.36\n",
      "     SSD Centro 2024-06-01      93030.1140       866888.99               8           6.62\n",
      "     RAM  Leste 2024-06-01     297068.2130       935979.75               1          21.76\n",
      " Teclado  Leste 2024-06-01     266163.5730       798655.11               2          19.49\n",
      "\n",
      "ðŸ”— FASE 5: OtimizaÃ§Ã£o de JOINs Complexos\n",
      "\n",
      "  âœ“ JOIN executado em: 0.0148s\n",
      "  âœ“ Tipo: INNER JOIN com agregaÃ§Ãµes\n",
      "\n",
      "  â†’ AnÃ¡lise de Margem por Categoria e RegiÃ£o:\n",
      "   categoria regiao  total_vendas qtd_total  receita_bruta  margem_lucro\n",
      "  AcessÃ³rios    Sul          3505     35940    81459013.07   28468706.35\n",
      "  AcessÃ³rios  Norte          3493     35118    80620328.82   28181463.79\n",
      "  AcessÃ³rios  Oeste          3457     34132    77744069.31   27195105.16\n",
      "  AcessÃ³rios Centro          3445     34473    76484300.76   26740392.63\n",
      "  AcessÃ³rios  Leste          3402     33739    75968250.99   26523049.62\n",
      " Componentes    Sul          1822     18049    42146559.64   11545178.28\n",
      " Componentes  Leste          1736     17718    40462508.68   11115433.74\n",
      " Componentes  Oeste          1760     17382    39980105.06   10983214.03\n",
      " Componentes Centro          1733     17292    39802719.90   10917393.55\n",
      " Componentes  Norte          1744     17450    39036866.97   10749158.61\n",
      " PerifÃ©ricos    Sul           924      9117    20420595.70    4084119.14\n",
      " PerifÃ©ricos  Oeste           893      9110    20285887.87    4057177.57\n",
      " PerifÃ©ricos Centro           888      8812    20262333.05    4052466.61\n",
      " PerifÃ©ricos  Norte           855      8302    19207157.46    3841431.49\n",
      " PerifÃ©ricos  Leste           820      8068    17687221.27    3537444.25\n",
      "Computadores  Leste           925      9130    20495987.52    3074398.13\n",
      "Computadores Centro           853      8830    20085635.23    3012845.29\n",
      "Computadores  Oeste           921      8902    20077198.82    3011579.82\n",
      "Computadores    Sul           863      8720    20004467.62    3000670.14\n",
      "Computadores  Norte           862      8527    19720843.04    2958126.46\n",
      "\n",
      "================================================================================\n",
      "âš¡ RESUMO DE PERFORMANCE - OTIMIZAÃ‡Ã•ES\n",
      "================================================================================\n",
      "  Query NÃ£o Otimizada: 0.0357s\n",
      "  Query Otimizada: 0.0111s\n",
      "  Speedup: 3.23x\n",
      "  Query Complexa (CTEs + Window): 0.0208s\n",
      "  Window Functions: 0.0233s\n",
      "  JOIN com AgregaÃ§Ãµes: 0.0148s\n",
      "  Total de registros processados: 50,000\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"--- {'OTIMIZAÃ‡ÃƒO AVANÃ‡ADA DE QUERIES'.upper()} ---\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# TÃ©cnicas avanÃ§adas de otimizaÃ§Ã£o com comparativos de performance\n",
    "\n",
    "# ==== 1. BENCHMARK SETUP ====\n",
    "print(\"âš™ï¸ FASE 1: Setup de Benchmark\")\n",
    "\n",
    "# Registrar dados para queries\n",
    "con.register('vendas', data)\n",
    "print(f\"  âœ“ Dataset registrado: {data.num_rows:,} vendas\")\n",
    "print(f\"  âœ“ Colunas: {data.num_columns}\")\n",
    "\n",
    "# ==== 2. QUERY NÃƒO OTIMIZADA VS OTIMIZADA ====\n",
    "print(f\"\\nðŸ” FASE 2: Comparativo - Query NÃ£o Otimizada vs Otimizada\\n\")\n",
    "\n",
    "# Query NÃ£o Otimizada (sem Ã­ndices, sem filtros early, sem projection pushdown)\n",
    "print(\"  âŒ Query NÃƒO OTIMIZADA:\")\n",
    "query_nao_otimizada = \"\"\"\n",
    "    SELECT * \n",
    "    FROM vendas\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "result_full = con.execute(query_nao_otimizada).fetch_arrow_table()\n",
    "# Aplicar filtros e agregaÃ§Ãµes em Python (ineficiente)\n",
    "df_temp = result_full.to_pandas()\n",
    "df_filtered = df_temp[df_temp['status'] == 'Completado']\n",
    "resultado_nao_opt = df_filtered.groupby('produto').agg({\n",
    "    'quantidade': 'sum',\n",
    "    'preco_unitario': 'mean'\n",
    "}).reset_index()\n",
    "time_nao_otimizada = time.time() - start\n",
    "print(f\"    â€¢ Tempo: {time_nao_otimizada:.4f}s\")\n",
    "print(f\"    â€¢ Registros processados: {result_full.num_rows:,}\")\n",
    "print(f\"    â€¢ TÃ©cnica: Full table scan + Python aggregation\")\n",
    "\n",
    "# Query Otimizada (com projection, filter pushdown, agregaÃ§Ã£o no DuckDB)\n",
    "print(\"\\n  âœ… Query OTIMIZADA:\")\n",
    "query_otimizada = \"\"\"\n",
    "    SELECT \n",
    "        produto,\n",
    "        SUM(quantidade) as quantidade,\n",
    "        ROUND(AVG(preco_unitario), 2) as preco_unitario\n",
    "    FROM vendas\n",
    "    WHERE status = 'Completado'\n",
    "    GROUP BY produto\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "resultado_otimizado = con.execute(query_otimizada).fetch_arrow_table()\n",
    "time_otimizada = time.time() - start\n",
    "print(f\"    â€¢ Tempo: {time_otimizada:.4f}s\")\n",
    "print(f\"    â€¢ Speedup: {time_nao_otimizada/time_otimizada:.2f}x mais rÃ¡pido\")\n",
    "print(f\"    â€¢ TÃ©cnicas: Filter pushdown + Projection pushdown + SQL aggregation\")\n",
    "\n",
    "print(f\"\\n  ðŸ“Š Melhoria de Performance: {((time_nao_otimizada - time_otimizada) / time_nao_otimizada * 100):.1f}%\")\n",
    "\n",
    "# ==== 3. EXPLAIN PLANS DETALHADOS ====\n",
    "print(f\"\\nðŸ“‹ FASE 3: AnÃ¡lise de Planos de ExecuÃ§Ã£o\\n\")\n",
    "\n",
    "# Query complexa para anÃ¡lise\n",
    "complex_query = \"\"\"\n",
    "    WITH vendas_por_vendedor AS (\n",
    "        SELECT \n",
    "            vendedor_id,\n",
    "            produto,\n",
    "            regiao,\n",
    "            COUNT(*) as total_vendas,\n",
    "            SUM(quantidade * preco_unitario * (1 - desconto_percentual/100.0)) as receita\n",
    "        FROM vendas\n",
    "        WHERE status = 'Completado'\n",
    "            AND data_venda >= '2023-06-01'\n",
    "        GROUP BY vendedor_id, produto, regiao\n",
    "    ),\n",
    "    ranking_vendedores AS (\n",
    "        SELECT \n",
    "            vendedor_id,\n",
    "            SUM(receita) as receita_total,\n",
    "            COUNT(DISTINCT produto) as produtos_vendidos,\n",
    "            RANK() OVER (ORDER BY SUM(receita) DESC) as rank_receita\n",
    "        FROM vendas_por_vendedor\n",
    "        GROUP BY vendedor_id\n",
    "    )\n",
    "    SELECT \n",
    "        v.vendedor_id,\n",
    "        v.produto,\n",
    "        v.regiao,\n",
    "        v.total_vendas,\n",
    "        ROUND(v.receita, 2) as receita,\n",
    "        r.receita_total,\n",
    "        r.rank_receita\n",
    "    FROM vendas_por_vendedor v\n",
    "    JOIN ranking_vendedores r ON v.vendedor_id = r.vendedor_id\n",
    "    WHERE r.rank_receita <= 10\n",
    "    ORDER BY r.rank_receita, v.receita DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"  â†’ EXPLAIN (Plano LÃ³gico):\")\n",
    "explain_result = con.execute(f\"EXPLAIN {complex_query}\").fetchdf()\n",
    "print(explain_result['explain_value'].iloc[0][:500] + \"...\")\n",
    "\n",
    "print(\"\\n  â†’ EXPLAIN ANALYZE (Plano FÃ­sico com MÃ©tricas):\")\n",
    "start_analyze = time.time()\n",
    "analyze_result = con.execute(f\"EXPLAIN ANALYZE {complex_query}\").fetchdf()\n",
    "analyze_time = time.time() - start_analyze\n",
    "print(f\"    â€¢ Tempo total de execuÃ§Ã£o: {analyze_time:.4f}s\")\n",
    "print(\"\\n\" + analyze_result['explain_value'].iloc[0][:800] + \"...\")\n",
    "\n",
    "# Executar a query para ver resultados\n",
    "actual_result = con.execute(complex_query).fetch_arrow_table()\n",
    "print(f\"\\n  â†’ Resultados: {actual_result.num_rows} linhas retornadas\")\n",
    "\n",
    "# ==== 4. WINDOW FUNCTIONS E OTIMIZAÃ‡Ã•ES ====\n",
    "print(f\"\\nðŸªŸ FASE 4: Window Functions AvanÃ§adas\\n\")\n",
    "\n",
    "window_query = \"\"\"\n",
    "    SELECT \n",
    "        produto,\n",
    "        regiao,\n",
    "        DATE_TRUNC('month', data_venda) as mes,\n",
    "        SUM(quantidade * preco_unitario * (1 - desconto_percentual/100.0)) as receita_mensal,\n",
    "        -- Moving average (mÃ©dia mÃ³vel de 3 meses)\n",
    "        ROUND(AVG(SUM(quantidade * preco_unitario * (1 - desconto_percentual/100.0))) \n",
    "            OVER (PARTITION BY produto, regiao \n",
    "                  ORDER BY DATE_TRUNC('month', data_venda) \n",
    "                  ROWS BETWEEN 2 PRECEDING AND CURRENT ROW), 2) as media_movel_3m,\n",
    "        -- Rank dentro de cada regiÃ£o\n",
    "        RANK() OVER (PARTITION BY regiao, DATE_TRUNC('month', data_venda) \n",
    "                     ORDER BY SUM(quantidade * preco_unitario * (1 - desconto_percentual/100.0)) DESC) as rank_na_regiao,\n",
    "        -- Percentual do total da regiÃ£o\n",
    "        ROUND(SUM(quantidade * preco_unitario * (1 - desconto_percentual/100.0)) * 100.0 / \n",
    "              SUM(SUM(quantidade * preco_unitario * (1 - desconto_percentual/100.0))) \n",
    "                OVER (PARTITION BY regiao, DATE_TRUNC('month', data_venda)), 2) as pct_da_regiao\n",
    "    FROM vendas\n",
    "    WHERE status = 'Completado'\n",
    "    GROUP BY produto, regiao, DATE_TRUNC('month', data_venda)\n",
    "    ORDER BY mes DESC, regiao, receita_mensal DESC\n",
    "\"\"\"\n",
    "\n",
    "start_window = time.time()\n",
    "window_result = con.execute(window_query).fetch_arrow_table()\n",
    "window_time = time.time() - start_window\n",
    "\n",
    "print(f\"  âœ“ Query com Window Functions executada em: {window_time:.4f}s\")\n",
    "print(f\"  âœ“ Resultados: {window_result.num_rows:,} linhas\")\n",
    "print(\"\\n  â†’ Amostra dos Resultados (Top 10):\")\n",
    "print(window_result.to_pandas().head(10).to_string(index=False))\n",
    "\n",
    "# ==== 5. JOINS COMPLEXOS E OTIMIZAÃ‡ÃƒO ====\n",
    "print(f\"\\nðŸ”— FASE 5: OtimizaÃ§Ã£o de JOINs Complexos\\n\")\n",
    "\n",
    "# Criar dimensÃ£o de produtos para JOIN\n",
    "dim_produtos = pa.table({\n",
    "    'produto': ['Notebook', 'Mouse', 'Teclado', 'Monitor', 'Webcam', 'Headset', 'SSD', 'RAM'],\n",
    "    'categoria': ['Computadores', 'AcessÃ³rios', 'AcessÃ³rios', 'PerifÃ©ricos', \n",
    "                  'AcessÃ³rios', 'AcessÃ³rios', 'Componentes', 'Componentes'],\n",
    "    'margem_percentual': [15.0, 40.0, 35.0, 20.0, 30.0, 35.0, 25.0, 30.0]\n",
    "})\n",
    "con.register('dim_produtos', dim_produtos)\n",
    "\n",
    "# JOIN Otimizado\n",
    "join_query = \"\"\"\n",
    "    SELECT \n",
    "        p.categoria,\n",
    "        v.regiao,\n",
    "        COUNT(*) as total_vendas,\n",
    "        SUM(v.quantidade) as qtd_total,\n",
    "        ROUND(SUM(v.quantidade * v.preco_unitario * (1 - v.desconto_percentual/100.0)), 2) as receita_bruta,\n",
    "        ROUND(SUM(v.quantidade * v.preco_unitario * (1 - v.desconto_percentual/100.0) * p.margem_percentual / 100.0), 2) as margem_lucro\n",
    "    FROM vendas v\n",
    "    INNER JOIN dim_produtos p ON v.produto = p.produto\n",
    "    WHERE v.status = 'Completado'\n",
    "    GROUP BY p.categoria, v.regiao\n",
    "    ORDER BY margem_lucro DESC\n",
    "\"\"\"\n",
    "\n",
    "start_join = time.time()\n",
    "join_result = con.execute(join_query).fetch_arrow_table()\n",
    "join_time = time.time() - start_join\n",
    "\n",
    "print(f\"  âœ“ JOIN executado em: {join_time:.4f}s\")\n",
    "print(f\"  âœ“ Tipo: INNER JOIN com agregaÃ§Ãµes\")\n",
    "print(\"\\n  â†’ AnÃ¡lise de Margem por Categoria e RegiÃ£o:\")\n",
    "print(join_result.to_pandas().to_string(index=False))\n",
    "\n",
    "# ==== 6. PERFORMANCE SUMMARY ====\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âš¡ RESUMO DE PERFORMANCE - OTIMIZAÃ‡Ã•ES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Query NÃ£o Otimizada: {time_nao_otimizada:.4f}s\")\n",
    "print(f\"  Query Otimizada: {time_otimizada:.4f}s\")\n",
    "print(f\"  Speedup: {time_nao_otimizada/time_otimizada:.2f}x\")\n",
    "print(f\"  Query Complexa (CTEs + Window): {analyze_time:.4f}s\")\n",
    "print(f\"  Window Functions: {window_time:.4f}s\")\n",
    "print(f\"  JOIN com AgregaÃ§Ãµes: {join_time:.4f}s\")\n",
    "print(f\"  Total de registros processados: {data.num_rows:,}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc356c",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Query Optimization\n",
    "\n",
    "Otimizando queries para melhor desempenho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1795a009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- INCREMENTAL LOADING AVANÃ‡ADO: CDC & MERGE/UPSERT ---\n",
      "================================================================================\n",
      "\n",
      "ðŸ—„ï¸ FASE 1: Setup - Estado Inicial do Data Warehouse\n",
      "  â†’ Criando snapshot inicial (T0)...\n",
      "    âœ“ Snapshot inicial salvo: 8 clientes\n",
      "    âœ“ Path: data_output/incremental_warehouse/clientes.parquet\n",
      "\n",
      "ðŸ”„ FASE 2: Change Data Capture - Detectando AlteraÃ§Ãµes\n",
      "  â†’ Simulando batch incremental (T1)...\n",
      "    âœ“ CDC capturado: 6 alteraÃ§Ãµes\n",
      "    âœ“ Tipo de operaÃ§Ãµes: 3 UPDATEs + 2 INSERTs + 1 SOFT DELETE\n",
      "\n",
      "ðŸ”€ FASE 3: Executando Merge/Upsert com DuckDB\n",
      "  âœ“ Merge concluÃ­do em 0.005s\n",
      "  âœ“ Total de clientes apÃ³s merge: 10\n",
      "  âœ“ Resultado salvo em: data_output/incremental_warehouse/clientes_merged.parquet\n",
      "\n",
      "ðŸ“Š FASE 4: AnÃ¡lise Detalhada das MudanÃ§as\n",
      "  â†’ Comparativo Antes vs Depois:\n",
      " snapshot  total_clientes  clientes_ativos  saldo_total\n",
      "  Inicial               8                8     19772.75\n",
      "PÃ³s-Merge              10                9     24683.00\n",
      "\n",
      "  â†’ Clientes com MudanÃ§as Detectadas:\n",
      " cliente_id  nome_anterior     nome_atual     email_anterior          email_atual  saldo_anterior  saldo_atual  diferenca_saldo status_atual\n",
      "       1002    Bruno Costa    Bruno Costa    bruno@email.com bruno.novo@email.com         2300.00       2500.0           200.00        Ativo\n",
      "       1004  Daniel Santos  Daniel Santos   daniel@email.com     daniel@email.com         4500.25       5000.0           499.75        Ativo\n",
      "       1006  Fernando Dias  Fernando Dias fernando@email.com   fernando@email.com         3300.50       3300.5             0.00      Inativo\n",
      "       1007 Gabriela Rocha Gabriela Rocha gabriela@email.com   gabriela@email.com          890.00       1200.0           310.00        Ativo\n",
      "\n",
      "  â†’ Novos Clientes Inseridos:\n",
      " cliente_id             nome             email status  saldo ultima_atualizacao  versao\n",
      "       1009 Isabela Ferreira isabela@email.com  Ativo 2100.0         2023-02-01       1\n",
      "       1010      JoÃ£o Mendes    joao@email.com  Ativo 1800.5         2023-02-01       1\n",
      "\n",
      "ðŸ“œ FASE 5: Mantendo HistÃ³rico Completo (SCD Type 2)\n",
      "  âœ“ HistÃ³rico completo salvo: 14 registros\n",
      "  âœ“ Path: data_output/incremental_warehouse/historico_completo.parquet\n",
      "\n",
      "  â†’ HistÃ³rico Completo do Cliente 1002 (Bruno):\n",
      " cliente_id        nome                email status  saldo ultima_atualizacao  versao tipo_registro\n",
      "       1002 Bruno Costa      bruno@email.com  Ativo 2300.0         2023-01-01       1    MODIFICADO\n",
      "       1002 Bruno Costa bruno.novo@email.com  Ativo 2500.0         2023-02-01       2    MODIFICADO\n",
      "\n",
      "================================================================================\n",
      "ðŸ“ˆ MÃ‰TRICAS DO PROCESSO INCREMENTAL\n",
      "================================================================================\n",
      "  Snapshot Inicial: 8 registros\n",
      "  MudanÃ§as Detectadas (CDC): 6 registros\n",
      "  Estado Final: 10 registros\n",
      "  Tempo de Merge: 0.005s\n",
      "  Registros no HistÃ³rico: 14\n",
      "  Throughput: 1,540 registros/segundo\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"--- {'INCREMENTAL LOADING AVANÃ‡ADO: CDC & MERGE/UPSERT'.upper()} ---\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# ImplementaÃ§Ã£o de Carregamento Incremental com Change Data Capture (CDC) e Merge/Upsert\n",
    "\n",
    "# ==== 1. SETUP: ESTADO INICIAL DO DATA WAREHOUSE ====\n",
    "print(\"ðŸ—„ï¸ FASE 1: Setup - Estado Inicial do Data Warehouse\")\n",
    "incremental_base = \"data_output/incremental_warehouse\"\n",
    "if os.path.exists(incremental_base):\n",
    "    import shutil\n",
    "    shutil.rmtree(incremental_base)\n",
    "os.makedirs(incremental_base, exist_ok=True)\n",
    "\n",
    "# Carga inicial (Snapshot T0)\n",
    "print(\"  â†’ Criando snapshot inicial (T0)...\")\n",
    "initial_snapshot = pa.table({\n",
    "    'cliente_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008],\n",
    "    'nome': ['Ana Silva', 'Bruno Costa', 'Carla Lima', 'Daniel Santos', \n",
    "             'Elena Martins', 'Fernando Dias', 'Gabriela Rocha', 'Hugo Pinto'],\n",
    "    'email': ['ana@email.com', 'bruno@email.com', 'carla@email.com', 'daniel@email.com',\n",
    "              'elena@email.com', 'fernando@email.com', 'gabriela@email.com', 'hugo@email.com'],\n",
    "    'status': ['Ativo', 'Ativo', 'Ativo', 'Ativo', 'Ativo', 'Ativo', 'Ativo', 'Ativo'],\n",
    "    'saldo': [1500.50, 2300.00, 980.75, 4500.25, 1200.00, 3300.50, 890.00, 5100.75],\n",
    "    'ultima_atualizacao': pd.to_datetime(['2023-01-01']*8),\n",
    "    'versao': [1]*8\n",
    "})\n",
    "\n",
    "snapshot_path = f\"{incremental_base}/clientes.parquet\"\n",
    "pq.write_table(initial_snapshot, snapshot_path)\n",
    "print(f\"    âœ“ Snapshot inicial salvo: {initial_snapshot.num_rows} clientes\")\n",
    "print(f\"    âœ“ Path: {snapshot_path}\\n\")\n",
    "\n",
    "# ==== 2. SIMULAÃ‡ÃƒO DE MUDANÃ‡AS (CDC) ====\n",
    "print(\"ðŸ”„ FASE 2: Change Data Capture - Detectando AlteraÃ§Ãµes\")\n",
    "\n",
    "# Simular mudanÃ§as incrementais (T1)\n",
    "# - Updates: clientes 1002, 1004, 1007\n",
    "# - Inserts: novos clientes 1009, 1010\n",
    "# - Deletes: cliente 1006 (soft delete)\n",
    "print(\"  â†’ Simulando batch incremental (T1)...\")\n",
    "incremental_changes = pa.table({\n",
    "    'cliente_id': [1002, 1004, 1006, 1007, 1009, 1010],\n",
    "    'nome': ['Bruno Costa', 'Daniel Santos', 'Fernando Dias', 'Gabriela Rocha', \n",
    "             'Isabela Ferreira', 'JoÃ£o Mendes'],\n",
    "    'email': ['bruno.novo@email.com', 'daniel@email.com', 'fernando@email.com', 'gabriela@email.com',\n",
    "              'isabela@email.com', 'joao@email.com'],\n",
    "    'status': ['Ativo', 'Ativo', 'Inativo', 'Ativo', 'Ativo', 'Ativo'],  # 1006 soft deleted\n",
    "    'saldo': [2500.00, 5000.00, 3300.50, 1200.00, 2100.00, 1800.50],  # saldos atualizados/novos\n",
    "    'ultima_atualizacao': pd.to_datetime(['2023-02-01']*6),\n",
    "    'versao': [2, 2, 2, 2, 1, 1]\n",
    "})\n",
    "\n",
    "changes_path = f\"{incremental_base}/changes_t1.parquet\"\n",
    "pq.write_table(incremental_changes, changes_path)\n",
    "print(f\"    âœ“ CDC capturado: {incremental_changes.num_rows} alteraÃ§Ãµes\")\n",
    "print(f\"    âœ“ Tipo de operaÃ§Ãµes: 3 UPDATEs + 2 INSERTs + 1 SOFT DELETE\\n\")\n",
    "\n",
    "# ==== 3. MERGE/UPSERT OPERATION ====\n",
    "print(\"ðŸ”€ FASE 3: Executando Merge/Upsert com DuckDB\")\n",
    "\n",
    "con.register('tabela_atual', pq.read_table(snapshot_path))\n",
    "con.register('mudancas', incremental_changes)\n",
    "\n",
    "# EstratÃ©gia de Merge: \n",
    "# 1. Identificar registros novos (INSERTs)\n",
    "# 2. Atualizar registros existentes (UPDATEs)\n",
    "# 3. Manter histÃ³rico de versÃµes\n",
    "start_merge = time.time()\n",
    "\n",
    "merged_result = con.execute(\"\"\"\n",
    "    -- CTE para identificar Ãºltima versÃ£o de cada cliente\n",
    "    WITH ultima_versao AS (\n",
    "        -- Combinar dados atuais e mudanÃ§as\n",
    "        SELECT * FROM tabela_atual\n",
    "        UNION ALL\n",
    "        SELECT * FROM mudancas\n",
    "    ),\n",
    "    ranked AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY cliente_id ORDER BY ultima_atualizacao DESC, versao DESC) as rn\n",
    "        FROM ultima_versao\n",
    "    )\n",
    "    SELECT \n",
    "        cliente_id,\n",
    "        nome,\n",
    "        email,\n",
    "        status,\n",
    "        saldo,\n",
    "        ultima_atualizacao,\n",
    "        versao\n",
    "    FROM ranked\n",
    "    WHERE rn = 1  -- Pegar apenas a versÃ£o mais recente\n",
    "    ORDER BY cliente_id\n",
    "\"\"\").fetch_arrow_table()\n",
    "\n",
    "merge_time = time.time() - start_merge\n",
    "\n",
    "# Salvar resultado do merge\n",
    "merged_path = f\"{incremental_base}/clientes_merged.parquet\"\n",
    "pq.write_table(merged_result, merged_path)\n",
    "print(f\"  âœ“ Merge concluÃ­do em {merge_time:.3f}s\")\n",
    "print(f\"  âœ“ Total de clientes apÃ³s merge: {merged_result.num_rows}\")\n",
    "print(f\"  âœ“ Resultado salvo em: {merged_path}\\n\")\n",
    "\n",
    "# ==== 4. ANÃLISE DE MUDANÃ‡AS ====\n",
    "print(\"ðŸ“Š FASE 4: AnÃ¡lise Detalhada das MudanÃ§as\")\n",
    "\n",
    "# Comparar antes e depois\n",
    "con.register('antes', initial_snapshot)\n",
    "con.register('depois', merged_result)\n",
    "\n",
    "analise = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        'Inicial' as snapshot,\n",
    "        COUNT(*) as total_clientes,\n",
    "        COUNT(CASE WHEN status = 'Ativo' THEN 1 END) as clientes_ativos,\n",
    "        ROUND(SUM(saldo), 2) as saldo_total\n",
    "    FROM antes\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'PÃ³s-Merge' as snapshot,\n",
    "        COUNT(*) as total_clientes,\n",
    "        COUNT(CASE WHEN status = 'Ativo' THEN 1 END) as clientes_ativos,\n",
    "        ROUND(SUM(saldo), 2) as saldo_total\n",
    "    FROM depois\n",
    "\"\"\").fetch_arrow_table()\n",
    "\n",
    "print(\"  â†’ Comparativo Antes vs Depois:\")\n",
    "print(analise.to_pandas().to_string(index=False))\n",
    "\n",
    "# Detalhar mudanÃ§as especÃ­ficas\n",
    "print(\"\\n  â†’ Clientes com MudanÃ§as Detectadas:\")\n",
    "mudancas_detalhadas = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        d.cliente_id,\n",
    "        a.nome as nome_anterior,\n",
    "        d.nome as nome_atual,\n",
    "        a.email as email_anterior,\n",
    "        d.email as email_atual,\n",
    "        a.saldo as saldo_anterior,\n",
    "        d.saldo as saldo_atual,\n",
    "        ROUND(d.saldo - a.saldo, 2) as diferenca_saldo,\n",
    "        d.status as status_atual\n",
    "    FROM antes a\n",
    "    INNER JOIN depois d ON a.cliente_id = d.cliente_id\n",
    "    WHERE a.email != d.email \n",
    "       OR a.saldo != d.saldo \n",
    "       OR a.status != d.status\n",
    "\"\"\").fetch_arrow_table()\n",
    "print(mudancas_detalhadas.to_pandas().to_string(index=False))\n",
    "\n",
    "# Novos clientes\n",
    "print(\"\\n  â†’ Novos Clientes Inseridos:\")\n",
    "novos = con.execute(\"\"\"\n",
    "    SELECT d.*\n",
    "    FROM depois d\n",
    "    LEFT JOIN antes a ON d.cliente_id = a.cliente_id\n",
    "    WHERE a.cliente_id IS NULL\n",
    "\"\"\").fetch_arrow_table()\n",
    "print(novos.to_pandas().to_string(index=False))\n",
    "\n",
    "# ==== 5. HISTÃ“RICO E AUDITORIA ====\n",
    "print(f\"\\nðŸ“œ FASE 5: Mantendo HistÃ³rico Completo (SCD Type 2)\")\n",
    "\n",
    "# Salvando histÃ³rico completo com todas as versÃµes\n",
    "historico_path = f\"{incremental_base}/historico_completo.parquet\"\n",
    "historico_completo = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        CASE \n",
    "            WHEN cliente_id IN (SELECT cliente_id FROM mudancas) THEN 'MODIFICADO'\n",
    "            ELSE 'INALTERADO'\n",
    "        END as tipo_registro\n",
    "    FROM (\n",
    "        SELECT * FROM tabela_atual\n",
    "        UNION ALL\n",
    "        SELECT * FROM mudancas\n",
    "    )\n",
    "    ORDER BY cliente_id, versao\n",
    "\"\"\").fetch_arrow_table()\n",
    "\n",
    "pq.write_table(historico_completo, historico_path)\n",
    "print(f\"  âœ“ HistÃ³rico completo salvo: {historico_completo.num_rows} registros\")\n",
    "print(f\"  âœ“ Path: {historico_path}\")\n",
    "\n",
    "# Visualizar histÃ³rico de um cliente especÃ­fico\n",
    "print(\"\\n  â†’ HistÃ³rico Completo do Cliente 1002 (Bruno):\")\n",
    "historico_bruno = con.execute(\"\"\"\n",
    "    SELECT * FROM historico_completo\n",
    "    WHERE cliente_id = 1002\n",
    "    ORDER BY versao\n",
    "\"\"\").fetch_arrow_table()\n",
    "print(historico_bruno.to_pandas().to_string(index=False))\n",
    "\n",
    "# ==== 6. MÃ‰TRICAS FINAIS ====\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“ˆ MÃ‰TRICAS DO PROCESSO INCREMENTAL\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Snapshot Inicial: {initial_snapshot.num_rows} registros\")\n",
    "print(f\"  MudanÃ§as Detectadas (CDC): {incremental_changes.num_rows} registros\")\n",
    "print(f\"  Estado Final: {merged_result.num_rows} registros\")\n",
    "print(f\"  Tempo de Merge: {merge_time:.3f}s\")\n",
    "print(f\"  Registros no HistÃ³rico: {historico_completo.num_rows}\")\n",
    "print(f\"  Throughput: {initial_snapshot.num_rows / merge_time:,.0f} registros/segundo\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a356b",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Incremental Loading\n",
    "\n",
    "TÃ©cnicas de carregamento incremental de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ef145e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- DATA LAKE ARCHITECTURE AVANÃ‡ADA ---\n",
      "================================================================================\n",
      "\n",
      "ðŸ—ï¸ FASE 1: Construindo Data Lake com Particionamento Multi-NÃ­vel\n",
      "  âœ“ Data Lake criado em: data_output/data_lake_advanced\n",
      "  âœ“ Tempo de particionamento: 0.069s\n",
      "\n",
      "ðŸ“ Estrutura do Data Lake (primeiras 20 pastas):\n",
      "â””â”€ data_lake_advanced/\n",
      "  â””â”€ ano=2023/\n",
      "    â””â”€ mes=1/\n",
      "      â””â”€ regiao=Centro/\n",
      "      â””â”€ regiao=Leste/\n",
      "      â””â”€ regiao=Norte/\n",
      "      â””â”€ regiao=Oeste/\n",
      "      â””â”€ regiao=Sul/\n",
      "    â””â”€ mes=10/\n",
      "      â””â”€ regiao=Centro/\n",
      "      â””â”€ regiao=Leste/\n",
      "      â””â”€ regiao=Norte/\n",
      "      â””â”€ regiao=Oeste/\n",
      "      â””â”€ regiao=Sul/\n",
      "    â””â”€ mes=11/\n",
      "      â””â”€ regiao=Centro/\n",
      "      â””â”€ regiao=Leste/\n",
      "      â””â”€ regiao=Norte/\n",
      "      â””â”€ regiao=Oeste/\n",
      "      â””â”€ regiao=Sul/\n",
      "  ... (mais pastas)\n",
      "\n",
      "ðŸ“Š FASE 2: Analisando Metadata do Data Lake\n",
      "  âœ“ Total de partiÃ§Ãµes: 90\n",
      "  âœ“ Colunas disponÃ­veis: 14\n",
      "  âœ“ Schema: id, data_venda, produto, quantidade, preco_unitario, desconto_percentual, status, cliente_id, vendedor_id, rating...\n",
      "  âœ“ Total de arquivos: 90\n",
      "  âœ“ Tamanho total: 1.01 MB\n",
      "  âœ“ Tamanho mÃ©dio por arquivo: 11.51 KB\n",
      "\n",
      "ðŸ” FASE 3: Executando Queries AnalÃ­ticas Complexas\n",
      "\n",
      "  â†’ Query 1: Top Produtos por RegiÃ£o (com partition pruning)\n",
      "    âœ“ Tempo: 0.180s | Resultados: 20 linhas\n",
      "regiao produto  total_vendas  receita_total  rating_medio\n",
      "Centro     SSD           401     9471201.05          2.98\n",
      "   Sul     SSD           382     9168382.08          2.92\n",
      " Norte   Mouse           385     9037776.72          3.06\n",
      "   Sul     RAM           367     8995681.48          3.04\n",
      " Norte  Webcam           374     8860264.33          3.04\n",
      "\n",
      "  â†’ Query 2: TendÃªncia de Vendas Mensal com Crescimento\n",
      "    âœ“ Tempo: 0.295s | Resultados: 90 linhas\n",
      " ano  mes regiao  total_vendas    receita  receita_mes_anterior  crescimento_percentual\n",
      "2023    1 Centro           425 9828551.08                   NaN                     NaN\n",
      "2023    2 Centro           360 8182430.11            9828551.08                  -16.75\n",
      "2023    3 Centro           387 8366492.72            8182430.11                    2.25\n",
      "2023    4 Centro           368 8318334.65            8366492.72                   -0.58\n",
      "2023    5 Centro           402 9530003.81            8318334.65                   14.57\n",
      "2023    6 Centro           393 9095403.43            9530003.81                   -4.56\n",
      "2023    7 Centro           429 9506027.30            9095403.43                    4.51\n",
      "2023    8 Centro           449 9878281.45            9506027.30                    3.92\n",
      "2023    9 Centro           435 9844024.23            9878281.45                   -0.35\n",
      "2023   10 Centro           395 8889342.37            9844024.23                   -9.70\n",
      "\n",
      "  â†’ Query 3: AnÃ¡lise de Comportamento de Clientes\n",
      "    âœ“ Tempo: 0.041s | Resultados: 15 linhas\n",
      " cliente_id  produtos_distintos  total_compras  lifetime_value  rating_medio     primeira_compra       ultima_compra  dias_como_cliente\n",
      "       8884                   5              9       388731.49          2.34 2023-02-12 13:00:00 2024-02-26 06:15:00                379\n",
      "       2392                   7             15       366062.55          2.91 2023-05-21 02:15:00 2024-05-11 23:45:00                356\n",
      "       7202                   7              9       359245.55          3.00 2023-01-08 20:45:00 2024-02-28 07:15:00                416\n",
      "       6589                   6              9       354391.50          3.53 2023-01-08 21:45:00 2024-02-06 10:00:00                394\n",
      "       4208                   6             12       351022.49          2.53 2023-01-07 06:45:00 2024-04-07 15:15:00                456\n",
      "       4297                   5              7       343583.09          2.81 2023-02-05 16:15:00 2024-05-16 21:00:00                466\n",
      "       3765                   7             12       339914.17          2.95 2023-01-29 19:45:00 2024-04-12 16:00:00                439\n",
      "       8519                   5             10       337401.47          2.41 2023-05-29 12:15:00 2024-03-15 10:30:00                291\n",
      "       8681                   8             12       336997.75          3.65 2023-02-11 12:30:00 2024-05-25 02:00:00                469\n",
      "       3741                   4             10       335860.43          3.17 2023-01-06 08:15:00 2024-04-02 08:00:00                452\n",
      "       9153                   4              8       334251.43          3.10 2023-02-25 12:45:00 2024-05-19 15:00:00                449\n",
      "       6903                   5              9       332448.09          2.82 2023-01-03 03:00:00 2024-04-02 16:00:00                455\n",
      "       6785                   4              8       330525.32          3.18 2023-02-16 23:00:00 2024-03-04 09:15:00                382\n",
      "       7566                   4              9       324400.55          3.29 2023-02-26 14:00:00 2024-03-29 03:15:00                397\n",
      "       3295                   4              8       323665.82          2.53 2023-03-08 19:00:00 2024-04-07 11:45:00                396\n",
      "\n",
      "================================================================================\n",
      "âš¡ PERFORMANCE SUMMARY - DATA LAKE\n",
      "================================================================================\n",
      "  Registros no Lake: 34,901\n",
      "  Arquivos Parquet: 90\n",
      "  CompressÃ£o: 1.01 MB (ratio: 3.6x)\n",
      "  Query 1 (Partition Pruning): 0.180s\n",
      "  Query 2 (Window Functions): 0.295s\n",
      "  Query 3 (Complex Aggregations): 0.041s\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"--- {'DATA LAKE ARCHITECTURE AVANÃ‡ADA'.upper()} ---\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Arquitetura Moderna de Data Lake com Particionamento Multi-NÃ­vel\n",
    "\n",
    "# ==== 1. CRIAÃ‡ÃƒO DO DATA LAKE COM PARTICIONAMENTO HIERÃRQUICO ====\n",
    "print(\"ðŸ—ï¸ FASE 1: Construindo Data Lake com Particionamento Multi-NÃ­vel\")\n",
    "lake_base = \"data_output/data_lake_advanced\"\n",
    "import shutil\n",
    "if os.path.exists(lake_base):\n",
    "    shutil.rmtree(lake_base)\n",
    "os.makedirs(lake_base, exist_ok=True)\n",
    "\n",
    "# Adicionar colunas de particionamento temporal\n",
    "start_partition = time.time()\n",
    "con.register('vendas_raw', data)\n",
    "vendas_partitioned = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        YEAR(data_venda) as ano,\n",
    "        MONTH(data_venda) as mes,\n",
    "        DAY(data_venda) as dia\n",
    "    FROM vendas_raw\n",
    "    WHERE status = 'Completado'  -- Apenas vendas completadas no lake\n",
    "\"\"\").fetch_arrow_table()\n",
    "\n",
    "# Salvar com particionamento hierÃ¡rquico: ano/mes/regiao\n",
    "# Importante: ordem das colunas de partiÃ§Ã£o afeta performance de queries\n",
    "pq.write_to_dataset(\n",
    "    vendas_partitioned,\n",
    "    root_path=lake_base,\n",
    "    partition_cols=['ano', 'mes', 'regiao'],\n",
    "    basename_template='vendas-{i}.parquet',\n",
    "    compression='zstd',  # Melhor compressÃ£o que snappy\n",
    "    existing_data_behavior='overwrite_or_ignore'\n",
    ")\n",
    "\n",
    "partition_time = time.time() - start_partition\n",
    "print(f\"  âœ“ Data Lake criado em: {lake_base}\")\n",
    "print(f\"  âœ“ Tempo de particionamento: {partition_time:.3f}s\")\n",
    "\n",
    "# Mostrar estrutura do Data Lake\n",
    "print(f\"\\nðŸ“ Estrutura do Data Lake (primeiras 20 pastas):\")\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(lake_base):\n",
    "    if count >= 20:\n",
    "        print(\"  ... (mais pastas)\")\n",
    "        break\n",
    "    level = root.replace(lake_base, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder_name = os.path.basename(root)\n",
    "    if folder_name:\n",
    "        print(f\"{indent}â””â”€ {folder_name}/\")\n",
    "        count += 1\n",
    "\n",
    "# ==== 2. METADATA E ESTATÃSTICAS DO DATA LAKE ====\n",
    "print(f\"\\nðŸ“Š FASE 2: Analisando Metadata do Data Lake\")\n",
    "\n",
    "# Coletar estatÃ­sticas do dataset\n",
    "dataset = ds.dataset(lake_base, format='parquet', partitioning='hive')\n",
    "print(f\"  âœ“ Total de partiÃ§Ãµes: {len(list(dataset.get_fragments()))}\")\n",
    "print(f\"  âœ“ Colunas disponÃ­veis: {len(dataset.schema.names)}\")\n",
    "print(f\"  âœ“ Schema: {', '.join(dataset.schema.names[:10])}...\")\n",
    "\n",
    "# Calcular tamanho total dos arquivos\n",
    "total_size = 0\n",
    "file_count = 0\n",
    "for root, dirs, files in os.walk(lake_base):\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            filepath = os.path.join(root, file)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "            file_count += 1\n",
    "\n",
    "print(f\"  âœ“ Total de arquivos: {file_count}\")\n",
    "print(f\"  âœ“ Tamanho total: {total_size / (1024**2):.2f} MB\")\n",
    "print(f\"  âœ“ Tamanho mÃ©dio por arquivo: {total_size / file_count / 1024:.2f} KB\")\n",
    "\n",
    "# ==== 3. QUERIES ANALÃTICAS COMPLEXAS ====\n",
    "print(f\"\\nðŸ” FASE 3: Executando Queries AnalÃ­ticas Complexas\")\n",
    "\n",
    "# Query 1: AnÃ¡lise de vendas por regiÃ£o com partition pruning\n",
    "print(\"\\n  â†’ Query 1: Top Produtos por RegiÃ£o (com partition pruning)\")\n",
    "start_q1 = time.time()\n",
    "query1 = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        regiao,\n",
    "        produto,\n",
    "        COUNT(*) as total_vendas,\n",
    "        ROUND(SUM(preco_unitario * quantidade * (1 - desconto_percentual/100.0)), 2) as receita_total,\n",
    "        ROUND(AVG(rating), 2) as rating_medio\n",
    "    FROM read_parquet('{lake_base}/**/*.parquet', hive_partitioning=1)\n",
    "    WHERE ano = 2023 AND mes >= 6  -- Partition pruning ativa\n",
    "    GROUP BY regiao, produto\n",
    "    ORDER BY receita_total DESC\n",
    "    LIMIT 20\n",
    "\"\"\").fetch_arrow_table()\n",
    "q1_time = time.time() - start_q1\n",
    "print(f\"    âœ“ Tempo: {q1_time:.3f}s | Resultados: {query1.num_rows} linhas\")\n",
    "print(query1.to_pandas().head(5).to_string(index=False))\n",
    "\n",
    "# Query 2: AnÃ¡lise temporal com window functions\n",
    "print(\"\\n  â†’ Query 2: TendÃªncia de Vendas Mensal com Crescimento\")\n",
    "start_q2 = time.time()\n",
    "query2 = con.execute(f\"\"\"\n",
    "    WITH vendas_mensais AS (\n",
    "        SELECT \n",
    "            ano,\n",
    "            mes,\n",
    "            regiao,\n",
    "            COUNT(*) as total_vendas,\n",
    "            ROUND(SUM(preco_unitario * quantidade * (1 - desconto_percentual/100.0)), 2) as receita\n",
    "        FROM read_parquet('{lake_base}/**/*.parquet', hive_partitioning=1)\n",
    "        GROUP BY ano, mes, regiao\n",
    "    )\n",
    "    SELECT \n",
    "        ano,\n",
    "        mes,\n",
    "        regiao,\n",
    "        total_vendas,\n",
    "        receita,\n",
    "        LAG(receita) OVER (PARTITION BY regiao ORDER BY ano, mes) as receita_mes_anterior,\n",
    "        ROUND(\n",
    "            (receita - LAG(receita) OVER (PARTITION BY regiao ORDER BY ano, mes)) / \n",
    "            NULLIF(LAG(receita) OVER (PARTITION BY regiao ORDER BY ano, mes), 0) * 100, \n",
    "            2\n",
    "        ) as crescimento_percentual\n",
    "    FROM vendas_mensais\n",
    "    ORDER BY regiao, ano, mes\n",
    "\"\"\").fetch_arrow_table()\n",
    "q2_time = time.time() - start_q2\n",
    "print(f\"    âœ“ Tempo: {q2_time:.3f}s | Resultados: {query2.num_rows} linhas\")\n",
    "print(query2.to_pandas().head(10).to_string(index=False))\n",
    "\n",
    "# Query 3: AnÃ¡lise de clientes com mÃºltiplas agregaÃ§Ãµes\n",
    "print(\"\\n  â†’ Query 3: AnÃ¡lise de Comportamento de Clientes\")\n",
    "start_q3 = time.time()\n",
    "query3 = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        cliente_id,\n",
    "        COUNT(DISTINCT produto) as produtos_distintos,\n",
    "        COUNT(*) as total_compras,\n",
    "        ROUND(SUM(preco_unitario * quantidade * (1 - desconto_percentual/100.0)), 2) as lifetime_value,\n",
    "        ROUND(AVG(rating), 2) as rating_medio,\n",
    "        MIN(data_venda) as primeira_compra,\n",
    "        MAX(data_venda) as ultima_compra,\n",
    "        DATEDIFF('day', MIN(data_venda), MAX(data_venda)) as dias_como_cliente\n",
    "    FROM read_parquet('{lake_base}/**/*.parquet', hive_partitioning=1)\n",
    "    GROUP BY cliente_id\n",
    "    HAVING total_compras > 5  -- Apenas clientes recorrentes\n",
    "    ORDER BY lifetime_value DESC\n",
    "    LIMIT 15\n",
    "\"\"\").fetch_arrow_table()\n",
    "q3_time = time.time() - start_q3\n",
    "print(f\"    âœ“ Tempo: {q3_time:.3f}s | Resultados: {query3.num_rows} linhas\")\n",
    "print(query3.to_pandas().to_string(index=False))\n",
    "\n",
    "# ==== 4. PERFORMANCE SUMMARY ====\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âš¡ PERFORMANCE SUMMARY - DATA LAKE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Registros no Lake: {vendas_partitioned.num_rows:,}\")\n",
    "print(f\"  Arquivos Parquet: {file_count}\")\n",
    "print(f\"  CompressÃ£o: {total_size / (1024**2):.2f} MB (ratio: {vendas_partitioned.nbytes / total_size:.1f}x)\")\n",
    "print(f\"  Query 1 (Partition Pruning): {q1_time:.3f}s\")\n",
    "print(f\"  Query 2 (Window Functions): {q2_time:.3f}s\")\n",
    "print(f\"  Query 3 (Complex Aggregations): {q3_time:.3f}s\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e711394",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Data Lake Architecture\n",
    "\n",
    "Arquitetura de data lake com Arrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ec2251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- ETL PIPELINE AVANÃ‡ADO: EXTRACT -> TRANSFORM -> LOAD ---\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¥ FASE 1: EXTRACT\n",
      "  âœ“ Fonte Principal: 50,000 registros extraÃ­dos\n",
      "  âœ“ Metadados de Produtos: 8 registros\n",
      "  â± Tempo de extraÃ§Ã£o: 0.006s\n",
      "\n",
      "ðŸ”„ FASE 2: TRANSFORM\n",
      "  â†’ Etapa 2.1: ValidaÃ§Ã£o e Limpeza de Dados\n",
      "    âœ“ Registros vÃ¡lidos: 44,964 (removidos 5,036)\n",
      "  â†’ Etapa 2.2: Enriquecimento com Metadados\n",
      "    âœ“ Dados enriquecidos: 16 colunas\n",
      "  â†’ Etapa 2.3: AgregaÃ§Ãµes AnalÃ­ticas\n",
      "    âœ“ AgregaÃ§Ãµes geradas: 720 linhas\n",
      "  â± Tempo de transformaÃ§Ã£o: 0.060s\n",
      "\n",
      "ðŸ’¾ FASE 3: LOAD\n",
      "  âœ“ Dados validados: data_output/etl_advanced/vendas_validadas.parquet\n",
      "  âœ“ Dados enriquecidos (particionados): data_output/etl_advanced/vendas_enriquecidas\n",
      "  âœ“ AgregaÃ§Ãµes mensais: data_output/etl_advanced/agregacoes_mensais.parquet\n",
      "  â± Tempo de load: 0.076s\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š MÃ‰TRICAS DO PIPELINE ETL\n",
      "================================================================================\n",
      "  Registros Processados: 50,000\n",
      "  Registros Validados: 44,964 (89.9%)\n",
      "  Registros Agregados: 720\n",
      "  Tempo Total: 0.141s\n",
      "  Taxa de Processamento: 355,495 registros/segundo\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Amostra das AgregaÃ§Ãµes Mensais (Top 10 por Receita):\n",
      "       mes regiao   categoria  produto  total_vendas  receita_total qtd_vendida  rating_medio  clientes_unicos  vendedores_ativos  ticket_medio\n",
      "2024-06-01  Norte    Hardware      SSD            13      418709.58         145          2.76               13                 13      32208.43\n",
      "2024-06-01 Centro PerifÃ©ricos  Headset            15      395839.62         177          3.11               15                 14      26389.31\n",
      "2024-06-01  Norte    Hardware      RAM            14      364852.98         167          3.53               14                 13      26060.93\n",
      "2024-06-01    Sul PerifÃ©ricos    Mouse             9      300421.19         111          2.76                9                  9      33380.13\n",
      "2024-06-01  Leste    Hardware Notebook            10      297114.91         116          3.35               10                 10      29711.49\n",
      "2024-06-01  Leste    Hardware      RAM            13      297068.21         156          2.95               13                 13      22851.40\n",
      "2024-06-01    Sul PerifÃ©ricos  Headset            10      284445.85         107          3.24               10                 10      28444.58\n",
      "2024-06-01  Oeste PerifÃ©ricos   Webcam            14      282517.89         125          3.23               14                 14      20179.85\n",
      "2024-06-01  Oeste    Hardware      RAM            10      278530.55         133          3.58               10                 10      27853.05\n",
      "2024-06-01  Leste PerifÃ©ricos  Teclado             9      266163.57         109          2.83                9                  7      29573.73\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"--- {'ETL PIPELINE AVANÃ‡ADO: Extract -> Transform -> Load'.upper()} ---\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Pipeline ETL Completo e AvanÃ§ado\n",
    "\n",
    "# ==== EXTRACT ====\n",
    "print(\"ðŸ“¥ FASE 1: EXTRACT\")\n",
    "start_extract = time.time()\n",
    "\n",
    "# Simulando extraÃ§Ã£o de mÃºltiplas fontes\n",
    "fonte_principal = data\n",
    "print(f\"  âœ“ Fonte Principal: {fonte_principal.num_rows:,} registros extraÃ­dos\")\n",
    "\n",
    "# Criando tabela de metadados de produtos\n",
    "metadados_produtos = pa.table({\n",
    "    'produto': ['Notebook', 'Mouse', 'Teclado', 'Monitor', 'Webcam', 'Headset', 'SSD', 'RAM'],\n",
    "    'categoria': ['Hardware', 'PerifÃ©ricos', 'PerifÃ©ricos', 'Hardware', 'PerifÃ©ricos', 'PerifÃ©ricos', 'Hardware', 'Hardware'],\n",
    "    'peso_kg': [2.5, 0.1, 0.8, 5.0, 0.3, 0.4, 0.05, 0.02],\n",
    "    'garantia_meses': [24, 12, 12, 36, 12, 12, 60, 60]\n",
    "})\n",
    "print(f\"  âœ“ Metadados de Produtos: {metadados_produtos.num_rows} registros\")\n",
    "\n",
    "extract_time = time.time() - start_extract\n",
    "print(f\"  â± Tempo de extraÃ§Ã£o: {extract_time:.3f}s\\n\")\n",
    "\n",
    "# ==== TRANSFORM ====\n",
    "print(\"ðŸ”„ FASE 2: TRANSFORM\")\n",
    "start_transform = time.time()\n",
    "\n",
    "# Registrar tabelas no DuckDB para transformaÃ§Ãµes complexas\n",
    "con.register('vendas', fonte_principal)\n",
    "con.register('produtos_meta', metadados_produtos)\n",
    "\n",
    "# TransformaÃ§Ã£o 1: ValidaÃ§Ã£o e Limpeza\n",
    "print(\"  â†’ Etapa 2.1: ValidaÃ§Ã£o e Limpeza de Dados\")\n",
    "dados_validados = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        CASE \n",
    "            WHEN preco_unitario <= 0 THEN NULL\n",
    "            WHEN quantidade <= 0 THEN NULL\n",
    "            ELSE preco_unitario * quantidade * (1 - desconto_percentual/100.0)\n",
    "        END as valor_total_venda\n",
    "    FROM vendas\n",
    "    WHERE status != 'Cancelado'  -- Remover vendas canceladas\n",
    "        AND preco_unitario > 0\n",
    "        AND quantidade > 0\n",
    "\"\"\").fetch_arrow_table()\n",
    "print(f\"    âœ“ Registros vÃ¡lidos: {dados_validados.num_rows:,} (removidos {fonte_principal.num_rows - dados_validados.num_rows:,})\")\n",
    "\n",
    "# TransformaÃ§Ã£o 2: Enriquecimento com JOIN\n",
    "print(\"  â†’ Etapa 2.2: Enriquecimento com Metadados\")\n",
    "con.register('vendas_validadas', dados_validados)\n",
    "dados_enriquecidos = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        v.*,\n",
    "        p.categoria,\n",
    "        p.peso_kg,\n",
    "        p.garantia_meses,\n",
    "        v.quantidade * p.peso_kg as peso_total_kg\n",
    "    FROM vendas_validadas v\n",
    "    LEFT JOIN produtos_meta p ON v.produto = p.produto\n",
    "\"\"\").fetch_arrow_table()\n",
    "print(f\"    âœ“ Dados enriquecidos: {dados_enriquecidos.num_columns} colunas\")\n",
    "\n",
    "# TransformaÃ§Ã£o 3: AgregaÃ§Ãµes Complexas\n",
    "print(\"  â†’ Etapa 2.3: AgregaÃ§Ãµes AnalÃ­ticas\")\n",
    "con.register('vendas_enriquecidas', dados_enriquecidos)\n",
    "agregacoes = con.execute(\"\"\"\n",
    "    WITH vendas_por_periodo AS (\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', data_venda) as mes,\n",
    "            regiao,\n",
    "            categoria,\n",
    "            produto,\n",
    "            COUNT(*) as total_vendas,\n",
    "            SUM(valor_total_venda) as receita_total,\n",
    "            SUM(quantidade) as qtd_vendida,\n",
    "            AVG(rating) as rating_medio,\n",
    "            COUNT(DISTINCT cliente_id) as clientes_unicos,\n",
    "            COUNT(DISTINCT vendedor_id) as vendedores_ativos\n",
    "        FROM vendas_enriquecidas\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "    )\n",
    "    SELECT \n",
    "        mes,\n",
    "        regiao,\n",
    "        categoria,\n",
    "        produto,\n",
    "        total_vendas,\n",
    "        ROUND(receita_total, 2) as receita_total,\n",
    "        qtd_vendida,\n",
    "        ROUND(rating_medio, 2) as rating_medio,\n",
    "        clientes_unicos,\n",
    "        vendedores_ativos,\n",
    "        ROUND(receita_total / total_vendas, 2) as ticket_medio\n",
    "    FROM vendas_por_periodo\n",
    "    ORDER BY mes DESC, receita_total DESC\n",
    "\"\"\").fetch_arrow_table()\n",
    "print(f\"    âœ“ AgregaÃ§Ãµes geradas: {agregacoes.num_rows:,} linhas\")\n",
    "\n",
    "transform_time = time.time() - start_transform\n",
    "print(f\"  â± Tempo de transformaÃ§Ã£o: {transform_time:.3f}s\\n\")\n",
    "\n",
    "# ==== LOAD ====\n",
    "print(\"ðŸ’¾ FASE 3: LOAD\")\n",
    "start_load = time.time()\n",
    "\n",
    "# Load em mÃºltiplos formatos e destinos\n",
    "output_base = \"data_output/etl_advanced\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# 3.1: Dados validados em Parquet comprimido\n",
    "validated_path = f\"{output_base}/vendas_validadas.parquet\"\n",
    "pq.write_table(dados_validados, validated_path, compression='snappy')\n",
    "print(f\"  âœ“ Dados validados: {validated_path}\")\n",
    "\n",
    "# 3.2: Dados enriquecidos particionados por regiÃ£o\n",
    "enriched_path = f\"{output_base}/vendas_enriquecidas\"\n",
    "pq.write_to_dataset(\n",
    "    dados_enriquecidos, \n",
    "    root_path=enriched_path,\n",
    "    partition_cols=['regiao', 'categoria'],\n",
    "    compression='zstd'\n",
    ")\n",
    "print(f\"  âœ“ Dados enriquecidos (particionados): {enriched_path}\")\n",
    "\n",
    "# 3.3: AgregaÃ§Ãµes finais\n",
    "agg_path = f\"{output_base}/agregacoes_mensais.parquet\"\n",
    "pq.write_table(agregacoes, agg_path, compression='gzip')\n",
    "print(f\"  âœ“ AgregaÃ§Ãµes mensais: {agg_path}\")\n",
    "\n",
    "load_time = time.time() - start_load\n",
    "print(f\"  â± Tempo de load: {load_time:.3f}s\\n\")\n",
    "\n",
    "# ==== MÃ‰TRICAS FINAIS ====\n",
    "total_time = extract_time + transform_time + load_time\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ðŸ“Š MÃ‰TRICAS DO PIPELINE ETL\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Registros Processados: {fonte_principal.num_rows:,}\")\n",
    "print(f\"  Registros Validados: {dados_validados.num_rows:,} ({dados_validados.num_rows/fonte_principal.num_rows*100:.1f}%)\")\n",
    "print(f\"  Registros Agregados: {agregacoes.num_rows:,}\")\n",
    "print(f\"  Tempo Total: {total_time:.3f}s\")\n",
    "print(f\"  Taxa de Processamento: {fonte_principal.num_rows/total_time:,.0f} registros/segundo\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Visualizar amostra dos resultados\n",
    "print(\"ðŸ“‹ Amostra das AgregaÃ§Ãµes Mensais (Top 10 por Receita):\")\n",
    "print(agregacoes.to_pandas().head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca26e93",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ETL Pipelines\n",
    "\n",
    "Construindo pipelines ETL com Arrow e DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b502c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gerando dados de exemplo complexos...\n",
      "âœ“ Tabela PyArrow criada: 50,000 linhas x 11 colunas\n",
      "âœ“ Schema: id, data_venda, produto, regiao, quantidade, preco_unitario, desconto_percentual, status, cliente_id, vendedor_id, rating\n",
      "âœ“ Tamanho em memÃ³ria: ~4.03 MB\n",
      "âœ“ ConexÃ£o DuckDB estabelecida com configuraÃ§Ãµes otimizadas\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo globais - Dataset Complexo para Casos de Uso AvanÃ§ados\n",
    "import time\n",
    "\n",
    "try:\n",
    "    print(\"\\nGerando dados de exemplo complexos...\")\n",
    "    \n",
    "    # Gerar dados de vendas realistas (50.000 registros)\n",
    "    num_records = 50000\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Datas ao longo de 2 anos\n",
    "    start_date = pd.Timestamp('2023-01-01')\n",
    "    dates = pd.date_range(start_date, periods=num_records, freq='15min')\n",
    "    \n",
    "    # Produtos, RegiÃµes, Status\n",
    "    produtos = ['Notebook', 'Mouse', 'Teclado', 'Monitor', 'Webcam', 'Headset', 'SSD', 'RAM']\n",
    "    regioes = ['Norte', 'Sul', 'Leste', 'Oeste', 'Centro']\n",
    "    status = ['Completado', 'Pendente', 'Cancelado', 'Em Processamento']\n",
    "    \n",
    "    data = pa.table({\n",
    "        'id': range(num_records),\n",
    "        'data_venda': dates,\n",
    "        'produto': np.random.choice(produtos, num_records),\n",
    "        'regiao': np.random.choice(regioes, num_records),\n",
    "        'quantidade': np.random.randint(1, 20, num_records),\n",
    "        'preco_unitario': np.round(np.random.uniform(50.0, 5000.0, num_records), 2),\n",
    "        'desconto_percentual': np.random.choice([0, 5, 10, 15, 20], num_records),\n",
    "        'status': np.random.choice(status, num_records, p=[0.7, 0.15, 0.1, 0.05]),\n",
    "        'cliente_id': np.random.randint(1000, 10000, num_records),\n",
    "        'vendedor_id': np.random.randint(1, 100, num_records),\n",
    "        'rating': np.round(np.random.uniform(1.0, 5.0, num_records), 1)\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ“ Tabela PyArrow criada: {data.num_rows:,} linhas x {data.num_columns} colunas\")\n",
    "    print(f\"âœ“ Schema: {', '.join([f.name for f in data.schema])}\")\n",
    "    print(f\"âœ“ Tamanho em memÃ³ria: ~{data.nbytes / (1024**2):.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Erro ao criar dados: {e}\")\n",
    "\n",
    "# ConexÃ£o DuckDB com configuraÃ§Ãµes otimizadas\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET threads TO 4\")\n",
    "con.execute(\"SET memory_limit='2GB'\")\n",
    "print(\"âœ“ ConexÃ£o DuckDB estabelecida com configuraÃ§Ãµes otimizadas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
