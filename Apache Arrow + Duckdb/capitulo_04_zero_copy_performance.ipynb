{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 04 Zero Copy Performance\n",
    "\n",
    "Notebook gerado automaticamente a partir do c√≥digo fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de pacotes necess√°rios\n",
    "!pip install pyarrow duckdb pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80e39d",
   "metadata": {},
   "source": [
    "## üìö Introdu√ß√£o\n",
    "\n",
    "Este notebook aborda Zero-Copy e Performance:\n",
    "- Fundamentos zero-copy\n",
    "- Processamento vetorizado\n",
    "- Memory mapping\n",
    "- Buffer management\n",
    "- Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Cap√≠tulo 04: Zero-Copy e Performance\n",
    "Curso: Apache Arrow + DuckDB\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.compute as pc\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"CAP√çTULO 04: ZERO-COPY E PERFORMANCE\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0613c9",
   "metadata": {},
   "source": [
    "## üîß Prepara√ß√£o dos Dados\n",
    "\n",
    "Cria√ß√£o de dados de exemplo e conex√£o com DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo globais\n",
    "try:\n",
    "    print(\"\\nGerando dados de exemplo...\")\n",
    "    data = pa.table({\n",
    "        'id': range(1000),\n",
    "        'valor': np.random.randn(1000),\n",
    "        'categoria': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "    })\n",
    "    print(f\"Tabela PyArrow criada: {data.num_rows} linhas\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao criar dados: {e}\")\n",
    "\n",
    "# Conex√£o DuckDB\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb4f6b",
   "metadata": {},
   "source": [
    "## üöÄ T√≥pico 1: Fundamentos zero-copy\n",
    "\n",
    "Entendendo os conceitos de zero-copy e suas vantagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Fundamentos zero-copy'.upper()} ---\")\n",
    "\n",
    "import sys\n",
    "import tracemalloc\n",
    "\n",
    "# 4.1.1 Entender o problema: C√≥pia vs Refer√™ncia\n",
    "print(\"\\n1. Zero-Copy vs Copy: Compara√ß√£o de Mem√≥ria:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar dados em NumPy (dados tradicional Python)\n",
    "numpy_array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10] * 1000)\n",
    "\n",
    "print(f\"Array NumPy:\")\n",
    "print(f\"  Shape: {numpy_array.shape}\")\n",
    "print(f\"  Dtype: {numpy_array.dtype}\")\n",
    "print(f\"  Memory: {numpy_array.nbytes:,} bytes\")\n",
    "\n",
    "# Abordagem 1: C√≥pias (ineficiente)\n",
    "print(\"\\nAbordagem 1: COPY (Ineficiente)\")\n",
    "tracemalloc.start()\n",
    "\n",
    "# C√≥pia 1\n",
    "array_copy1 = numpy_array.copy()\n",
    "current, peak1 = tracemalloc.get_traced_memory()\n",
    "\n",
    "# C√≥pia 2\n",
    "array_copy2 = numpy_array.copy()\n",
    "current, peak2 = tracemalloc.get_traced_memory()\n",
    "\n",
    "# C√≥pia 3\n",
    "array_copy3 = numpy_array.copy()\n",
    "current, peak3 = tracemalloc.get_traced_memory()\n",
    "\n",
    "tracemalloc.stop()\n",
    "\n",
    "total_memory_copy = numpy_array.nbytes * 3\n",
    "print(f\"  3 c√≥pias = {total_memory_copy:,} bytes extras em mem√≥ria\")\n",
    "\n",
    "# Abordagem 2: Refer√™ncias (eficiente)\n",
    "print(\"\\nAbordagem 2: ZERO-COPY (Eficiente)\")\n",
    "tracemalloc.start()\n",
    "\n",
    "# Refer√™ncias (sem c√≥pia)\n",
    "array_ref1 = numpy_array  # Mesma mem√≥ria!\n",
    "array_ref2 = numpy_array  # Mesma mem√≥ria!\n",
    "array_ref3 = numpy_array  # Mesma mem√≥ria!\n",
    "\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"  3 refer√™ncias = 0 bytes extras em mem√≥ria\")\n",
    "print(f\"  Mesmos dados? {array_ref1 is numpy_array}\")\n",
    "\n",
    "# 4.1.2 Arrow: Zero-Copy Between Languages\n",
    "print(\"\\n2. Arrow: Zero-Copy entre linguagens:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar tabela Arrow\n",
    "arrow_table = pa.table({\n",
    "    'id': list(range(100)),\n",
    "    'values': np.random.randn(100),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100)\n",
    "})\n",
    "\n",
    "print(f\"Arrow Table:\")\n",
    "print(f\"  Shape: {arrow_table.num_rows} linhas, {arrow_table.num_columns} colunas\")\n",
    "print(f\"  Memory: {arrow_table.nbytes:,} bytes\")\n",
    "\n",
    "# Converter para Pandas (zero-copy quando poss√≠vel)\n",
    "print(\"\\nConvers√£o Arrow ‚Üí Pandas:\")\n",
    "pandas_df = arrow_table.to_pandas()\n",
    "print(f\"  Pandas DataFrame created (not copied)\")\n",
    "\n",
    "# Converter back para Arrow (zero-copy)\n",
    "print(\"\\nConvers√£o Pandas ‚Üí Arrow:\")\n",
    "arrow_table2 = pa.Table.from_pandas(pandas_df)\n",
    "print(f\"  Arrow Table restored (zero-copy)\")\n",
    "\n",
    "# Verificar igualdade\n",
    "print(f\"\\nTables s√£o iguais? {arrow_table.equals(arrow_table2)}\")\n",
    "\n",
    "# 4.1.3 Buffer Compartilhado\n",
    "print(\"\\n3. Buffer Compartilhado (Shared Memory):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar um buffer Arrow\n",
    "buffer = pa.allocate_buffer(1000)\n",
    "print(f\"Buffer alocado: {buffer.size} bytes\")\n",
    "\n",
    "# Slice do mesmo buffer (zero-copy)\n",
    "slice1 = buffer.slice(0, 500)\n",
    "slice2 = buffer.slice(500, 500)\n",
    "\n",
    "print(f\"  Slice 1: offset 0, size 500\")\n",
    "print(f\"  Slice 2: offset 500, size 500\")\n",
    "\n",
    "# Converter para array compartilhado\n",
    "array_from_buffer = pa.array([1, 2, 3, 4, 5], type=pa.int64())\n",
    "print(f\"\\nArray from buffer: {array_from_buffer}\")\n",
    "\n",
    "# 4.1.4 Demonstra√ß√£o com DuckDB\n",
    "print(\"\\n4. Zero-Copy com DuckDB:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar dados em Arrow\n",
    "large_table = pa.table({\n",
    "    'customer_id': list(range(10000)),\n",
    "    'amount': np.random.uniform(10, 1000, 10000),\n",
    "    'country': np.random.choice(['USA', 'UK', 'Canada', 'Germany'], 10000)\n",
    "})\n",
    "\n",
    "print(f\"Large Table: {large_table.num_rows} linhas\")\n",
    "\n",
    "# Query em DuckDB (zero-copy - acessa buffer Arrow diretamente)\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        COUNT(*) as count,\n",
    "        AVG(amount) as avg_amount,\n",
    "        SUM(amount) as total_amount\n",
    "    FROM large_table\n",
    "    GROUP BY country\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\").arrow()\n",
    "\n",
    "print(\"\\nResultado (processado sem copiar dados):\")\n",
    "print(result)\n",
    "\n",
    "# 4.1.5 Memory Ownership\n",
    "print(\"\\n5. Ownership de Mem√≥ria:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Arrow gerencia mem√≥ria automaticamente\n",
    "arrow_data = pa.array([1, 2, 3, 4, 5])\n",
    "print(f\"Arrow Array: {arrow_data}\")\n",
    "print(f\"  Type: {arrow_data.type}\")\n",
    "print(f\"  Length: {len(arrow_data)}\")\n",
    "\n",
    "# Convers√£o mant√©m refer√™ncia\n",
    "numpy_from_arrow = arrow_data.to_numpy()\n",
    "print(f\"\\nNumPy array from Arrow:\")\n",
    "print(f\"  Data: {numpy_from_arrow}\")\n",
    "print(f\"  Dtype: {numpy_from_arrow.dtype}\")\n",
    "\n",
    "# Liberar mem√≥ria (Arrow gerencia)\n",
    "del arrow_data\n",
    "print(f\"\\nArrow array deletado (mem√≥ria liberada automaticamente)\")\n",
    "\n",
    "# NumPy array ainda existe\n",
    "print(f\"NumPy array ainda existe: {numpy_from_arrow}\")\n",
    "\n",
    "print(\"\\n‚úÖ Zero-Copy permite compartilhar buffers de mem√≥ria\")\n",
    "print(\"   entre diferentes estruturas de dados sem c√≥pias!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5d964",
   "metadata": {},
   "source": [
    "## ‚ö° T√≥pico 2: Processamento vetorizado\n",
    "\n",
    "Otimizando opera√ß√µes com processamento vetorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b71079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Processamento vetorizado'.upper()} ---\")\n",
    "\n",
    "# 4.2.1 Opera√ß√µes Escalares vs Vetorizadas\n",
    "print(\"\\n1. Opera√ß√µes Escalares vs Vetorizadas:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar dataset grande\n",
    "n = 100000\n",
    "data = pa.table({\n",
    "    'a': np.random.randint(1, 100, n),\n",
    "    'b': np.random.randint(1, 100, n),\n",
    "    'c': np.random.uniform(10, 1000, n)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {data.num_rows:,} linhas\")\n",
    "\n",
    "# Estrat√©gia 1: Loop Escalar (LENTO)\n",
    "print(\"\\nEstrat√©gia 1: Loop Escalar (Python puro)\")\n",
    "a_list = data['a'].to_pylist()\n",
    "b_list = data['b'].to_pylist()\n",
    "\n",
    "start = time.perf_counter()\n",
    "result_scalar = []\n",
    "for i in range(len(a_list)):\n",
    "    result_scalar.append(a_list[i] * 2 + b_list[i])\n",
    "time_scalar = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_scalar:.4f}s\")\n",
    "print(f\"  Primeiros resultados: {result_scalar[:5]}\")\n",
    "\n",
    "# Estrat√©gia 2: NumPy Vetorizado (R√ÅPIDO)\n",
    "print(\"\\nEstrat√©gia 2: NumPy Vetorizado\")\n",
    "a_numpy = data['a'].to_numpy()\n",
    "b_numpy = data['b'].to_numpy()\n",
    "\n",
    "start = time.perf_counter()\n",
    "result_numpy = a_numpy * 2 + b_numpy\n",
    "time_numpy = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_numpy:.6f}s\")\n",
    "# Preven√ß√£o de divis√£o por zero\n",
    "speedup_np = time_scalar / max(time_numpy, 1e-9)\n",
    "print(f\"  Speedup: {speedup_np:.1f}x mais r√°pido\")\n",
    "\n",
    "# Estrat√©gia 3: Arrow Compute (MAIS R√ÅPIDO)\n",
    "print(\"\\nEstrat√©gia 3: Arrow Compute (Vetorizado)\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "a_col = data['a']\n",
    "b_col = data['b']\n",
    "result_arrow = pc.add(pc.multiply(a_col, 2), b_col)\n",
    "time_arrow = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_arrow:.6f}s\")\n",
    "speedup_arrow_scalar = time_scalar / max(time_arrow, 1e-9)\n",
    "speedup_arrow_numpy = time_numpy / max(time_arrow, 1e-9)\n",
    "print(f\"  Speedup vs Escalar: {speedup_arrow_scalar:.1f}x\")\n",
    "print(f\"  Speedup vs NumPy: {speedup_arrow_numpy:.1f}x\")\n",
    "\n",
    "# 4.2.2 Opera√ß√µes Complexas Vetorizadas\n",
    "print(\"\\n2. Opera√ß√µes Complexas Vetorizadas:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Dataset para an√°lise\n",
    "sales_data = pa.table({\n",
    "    'product': np.random.choice(['A', 'B', 'C', 'D'], 50000),\n",
    "    'price': np.random.uniform(10, 1000, 50000),\n",
    "    'quantity': np.random.randint(1, 100, 50000),\n",
    "    'discount': np.random.uniform(0, 0.5, 50000)\n",
    "})\n",
    "\n",
    "print(f\"Sales Data: {sales_data.num_rows:,} registros\")\n",
    "\n",
    "# C√°lculos Vetorizados com Arrow Compute\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Calcular valores com desconto\n",
    "total_price = pc.multiply(sales_data['price'], sales_data['quantity'])\n",
    "discounted_price = pc.multiply(total_price, pc.subtract(1, sales_data['discount']))\n",
    "\n",
    "# Filtrar valores maiores que 500\n",
    "mask = pc.greater(discounted_price, 500)\n",
    "filtered = sales_data.filter(mask)\n",
    "\n",
    "# Agrega√ß√µes\n",
    "avg_discounted = pc.mean(discounted_price)\n",
    "max_discounted = pc.max(discounted_price)\n",
    "count = pc.count(discounted_price)\n",
    "\n",
    "time_vectorized = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nResultados (tempo: {time_vectorized:.4f}s):\")\n",
    "print(f\"  Quantidade de registros: {count.as_py()}\")\n",
    "print(f\"  Pre√ßo m√©dio (com desconto): {avg_discounted.as_py():.2f}\")\n",
    "print(f\"  Pre√ßo m√°ximo (com desconto): {max_discounted.as_py():.2f}\")\n",
    "\n",
    "# 4.2.3 Opera√ß√µes com DuckDB (Vetorizado)\n",
    "print(\"\\n3. DuckDB: Processamento Vetorizado Autom√°tico:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Query SQL executada vetorizadamente\n",
    "start = time.perf_counter()\n",
    "\n",
    "result_duckdb = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        product,\n",
    "        COUNT(*) as count,\n",
    "        AVG(price * quantity * (1 - discount)) as avg_sale,\n",
    "        SUM(price * quantity * (1 - discount)) as total_sale,\n",
    "        MAX(price * quantity * (1 - discount)) as max_sale\n",
    "    FROM sales_data\n",
    "    WHERE price * quantity * (1 - discount) > 500\n",
    "    GROUP BY product\n",
    "    ORDER BY total_sale DESC\n",
    "\"\"\").df()\n",
    "\n",
    "time_duckdb = time.perf_counter() - start\n",
    "\n",
    "print(f\"Tempo DuckDB: {time_duckdb:.4f}s\")\n",
    "print(\"\\nResultados por produto:\")\n",
    "print(result_duckdb)\n",
    "\n",
    "# 4.2.4 Benchmark Completo\n",
    "print(\"\\n4. Benchmark: Compara√ß√£o de Estrat√©gias\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Opera√ß√£o: Calcular m√©dia de vendas filtradas\n",
    "operation_data = pa.table({\n",
    "    'sales': np.random.uniform(100, 10000, 1000000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], 1000000)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {operation_data.num_rows:,} vendas\")\n",
    "\n",
    "# Estrat√©gia 1: Python Puro\n",
    "print(\"\\nEstrat√©gia 1: Python Puro (loop + condi√ß√µes)\")\n",
    "start = time.perf_counter()\n",
    "sales_list = operation_data['sales'].to_pylist()\n",
    "total = 0\n",
    "count = 0\n",
    "for sale in sales_list:\n",
    "    if sale > 5000:\n",
    "        total += sale\n",
    "        count += 1\n",
    "avg_python = total / count if count > 0 else 0\n",
    "time_python = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_python:.4f}s\")\n",
    "print(f\"  Resultado: {avg_python:.2f}\")\n",
    "\n",
    "# Estrat√©gia 2: NumPy Vetorizado\n",
    "print(\"\\nEstrat√©gia 2: NumPy Vetorizado\")\n",
    "start = time.perf_counter()\n",
    "sales_np = operation_data['sales'].to_numpy()\n",
    "mask_np = sales_np > 5000\n",
    "avg_numpy = sales_np[mask_np].mean()\n",
    "time_numpy_calc = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_numpy_calc:.6f}s\")\n",
    "speedup_np_calc = time_python / max(time_numpy_calc, 1e-9)\n",
    "print(f\"  Speedup: {speedup_np_calc:.1f}x\")\n",
    "\n",
    "# Estrat√©gia 3: Arrow Compute\n",
    "print(\"\\nEstrat√©gia 3: Arrow Compute\")\n",
    "start = time.perf_counter()\n",
    "sales_col = operation_data['sales']\n",
    "mask_arrow = pc.greater(sales_col, 5000)\n",
    "filtered_sales = pc.filter(sales_col, mask_arrow)\n",
    "avg_arrow = pc.mean(filtered_sales).as_py()\n",
    "time_arrow_calc = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_arrow_calc:.6f}s\")\n",
    "speedup_arrow_python = time_python / max(time_arrow_calc, 1e-9)\n",
    "speedup_arrow_numpy_calc = time_numpy_calc / max(time_arrow_calc, 1e-9)\n",
    "print(f\"  Speedup vs Python: {speedup_arrow_python:.1f}x\")\n",
    "print(f\"  Speedup vs NumPy: {speedup_arrow_numpy_calc:.1f}x\")\n",
    "\n",
    "# Estrat√©gia 4: DuckDB SQL\n",
    "print(\"\\nEstrat√©gia 4: DuckDB SQL (vetorizado autom√°tico)\")\n",
    "start = time.perf_counter()\n",
    "result_sql = con.execute(\"\"\"\n",
    "    SELECT AVG(sales) as avg_sale\n",
    "    FROM operation_data\n",
    "    WHERE sales > 5000\n",
    "\"\"\").fetchone()\n",
    "time_sql = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_sql:.4f}s\")\n",
    "speedup_sql = time_python / max(time_sql, 1e-9)\n",
    "print(f\"  Speedup vs Python: {speedup_sql:.1f}x\")\n",
    "\n",
    "print(\"\\nüìä Resumo de Velocidades:\")\n",
    "print(\"-\" * 40)\n",
    "timings = [\n",
    "    (\"Python Puro\", time_python),\n",
    "    (\"NumPy\", time_numpy_calc),\n",
    "    (\"Arrow Compute\", time_arrow_calc),\n",
    "    (\"DuckDB SQL\", time_sql)\n",
    "]\n",
    "timings.sort(key=lambda x: x[1])\n",
    "\n",
    "for i, (strategy, elapsed) in enumerate(timings, 1):\n",
    "    speedup = time_python / max(elapsed, 1e-9)\n",
    "    print(f\"  {i}. {strategy:.<20} {elapsed:.6f}s ({speedup:.1f}x)\")\n",
    "\n",
    "print(\"\\n‚úÖ Processamento vetorizado √© fundamental para performance!\")\n",
    "print(\"   Loop scalares devem ser evitados a todo custo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a12148",
   "metadata": {},
   "source": [
    "## üíæ T√≥pico 3: Memory mapping\n",
    "\n",
    "T√©cnicas de mapeamento de mem√≥ria para efici√™ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Memory mapping'.upper()} ---\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "\n",
    "# 4.3.1 Conceitos de Memory Mapping\n",
    "print(\"\\n1. Memory Mapping: Conceito e Benef√≠cios:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar arquivo Parquet grande\n",
    "mmap_dir = 'mmap_data'\n",
    "if os.path.exists(mmap_dir):\n",
    "    shutil.rmtree(mmap_dir)\n",
    "os.makedirs(mmap_dir, exist_ok=True)\n",
    "\n",
    "# Criar dataset de 1 milh√£o de registros\n",
    "n_records = 1000000\n",
    "mmap_table = pa.table({\n",
    "    'id': list(range(1, n_records + 1)),\n",
    "    'value': np.random.randn(n_records),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_records),\n",
    "    'timestamp': pa.array([f'2024-{(i%12)+1:02d}-{(i%28)+1:02d}' for i in range(n_records)]),\n",
    "    'amount': np.random.uniform(10, 10000, n_records)\n",
    "})\n",
    "\n",
    "parquet_file = f'{mmap_dir}/large_dataset.parquet'\n",
    "pq.write_table(mmap_table, parquet_file)\n",
    "\n",
    "file_size = os.path.getsize(parquet_file)\n",
    "print(f\"Arquivo Parquet criado:\")\n",
    "print(f\"  Caminho: {parquet_file}\")\n",
    "print(f\"  Tamanho: {file_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Registros: {n_records:,}\")\n",
    "\n",
    "# 4.3.2 Leitura Tradicional vs Memory Mapping\n",
    "print(\"\\n2. Leitura Tradicional vs Memory Mapping:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Estrat√©gia 1: Leitura Tradicional (Carrega tudo em RAM)\n",
    "print(\"Estrat√©gia 1: Leitura Tradicional (load all into RAM)\")\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "start = time.time()\n",
    "full_table = pq.read_table(parquet_file)\n",
    "time_traditional = time.time() - start\n",
    "\n",
    "mem_after = process.memory_info().rss / 1024 / 1024\n",
    "mem_used = mem_after - mem_before\n",
    "\n",
    "print(f\"  Tempo de leitura: {time_traditional:.4f}s\")\n",
    "print(f\"  Mem√≥ria usada: {mem_used:.2f} MB\")\n",
    "print(f\"  Dados na mem√≥ria: {full_table.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Estrat√©gia 2: Leitura com Filter (Carrega apenas dados filtrados)\n",
    "print(\"\\nEstrat√©gia 2: Leitura com Filtro (load only filtered data)\")\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "start = time.time()\n",
    "filtered_table = pq.read_table(\n",
    "    parquet_file,\n",
    "    filters=[('amount', '>', 5000)]\n",
    ")\n",
    "time_filtered = time.time() - start\n",
    "\n",
    "mem_after = process.memory_info().rss / 1024 / 1024\n",
    "mem_used_filtered = mem_after - mem_before\n",
    "\n",
    "print(f\"  Tempo de leitura: {time_filtered:.4f}s\")\n",
    "print(f\"  Mem√≥ria usada: {mem_used_filtered:.2f} MB\")\n",
    "print(f\"  Dados na mem√≥ria: {filtered_table.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Registros recuperados: {filtered_table.num_rows:,}\")\n",
    "\n",
    "# 4.3.3 Processamento Streaming (Memory Mapping)\n",
    "print(\"\\n3. Leitura Streaming (Memory Mapping):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Usar Scanner para ler em batches (memory mapped)\n",
    "print(\"Usando PyArrow Scanner com batch_size:\")\n",
    "\n",
    "dataset = ds.dataset(parquet_file, format='parquet')\n",
    "scanner = dataset.scanner(batch_size=50000)\n",
    "\n",
    "total_rows = 0\n",
    "batch_count = 0\n",
    "start = time.time()\n",
    "\n",
    "for batch in scanner.to_batches():\n",
    "    batch_count += 1\n",
    "    total_rows += batch.num_rows\n",
    "    \n",
    "    # Processar batch (sem carregar tudo em mem√≥ria)\n",
    "    if batch_count == 1:\n",
    "        print(f\"  Batch 1: {batch.num_rows:,} registros\")\n",
    "\n",
    "time_streaming = time.time() - start\n",
    "\n",
    "print(f\"  Total de batches: {batch_count}\")\n",
    "print(f\"  Total de registros: {total_rows:,}\")\n",
    "print(f\"  Tempo: {time_streaming:.4f}s\")\n",
    "\n",
    "# 4.3.4 Mapeamento de Colunas (Column Mapping)\n",
    "print(\"\\n4. Column Mapping (Ler colunas seletivamente):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Op√ß√£o A: Ler todas as colunas\n",
    "print(\"Op√ß√£o A: Ler todas as 5 colunas\")\n",
    "start = time.time()\n",
    "all_cols = pq.read_table(parquet_file)\n",
    "time_all = time.time() - start\n",
    "mem_all = all_cols.nbytes / 1024 / 1024\n",
    "\n",
    "print(f\"  Tempo: {time_all:.4f}s\")\n",
    "print(f\"  Mem√≥ria: {mem_all:.2f} MB\")\n",
    "\n",
    "# Op√ß√£o B: Mapear apenas colunas necess√°rias\n",
    "print(\"\\nOp√ß√£o B: Mapear apenas 2 colunas (id, amount)\")\n",
    "start = time.time()\n",
    "mapped_cols = pq.read_table(parquet_file, columns=['id', 'amount'])\n",
    "time_mapped = time.time() - start\n",
    "mem_mapped = mapped_cols.nbytes / 1024 / 1024\n",
    "\n",
    "print(f\"  Tempo: {time_mapped:.4f}s\")\n",
    "print(f\"  Mem√≥ria: {mem_mapped:.2f} MB\")\n",
    "print(f\"  Economia: {(1 - mem_mapped / mem_all) * 100:.1f}%\")\n",
    "\n",
    "# 4.3.5 DuckDB com Memory Mapping Autom√°tico\n",
    "print(\"\\n5. DuckDB: Memory Mapping Autom√°tico:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# DuckDB otimiza automaticamente com memory mapping\n",
    "print(\"Query 1: Agrega√ß√£o com filtro\")\n",
    "start = time.time()\n",
    "\n",
    "result1 = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as count,\n",
    "        AVG(amount) as avg_amount,\n",
    "        MAX(amount) as max_amount\n",
    "    FROM read_parquet('{parquet_file}')\n",
    "    WHERE amount > 5000\n",
    "    GROUP BY category\n",
    "    ORDER BY avg_amount DESC\n",
    "\"\"\").df()\n",
    "\n",
    "time_query1 = time.time() - start\n",
    "\n",
    "print(f\"  Tempo: {time_query1:.4f}s\")\n",
    "print(f\"  Resultados: {len(result1)} categorias\")\n",
    "print(result1)\n",
    "\n",
    "# Demonstrar que dados n√£o est√£o todos carregados em RAM\n",
    "print(\"\\nQuery 2: Stat√≠sticas globais (sem carregar tudo)\")\n",
    "start = time.time()\n",
    "\n",
    "result2 = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        AVG(amount) as global_avg,\n",
    "        MIN(amount) as min_amount,\n",
    "        MAX(amount) as max_amount,\n",
    "        STDDEV(amount) as std_amount\n",
    "    FROM read_parquet('{parquet_file}')\n",
    "\"\"\").df()\n",
    "\n",
    "time_query2 = time.time() - start\n",
    "\n",
    "print(f\"  Tempo: {time_query2:.4f}s\")\n",
    "print(result2)\n",
    "\n",
    "# 4.3.6 Compara√ß√£o de Estrat√©gias\n",
    "print(\"\\n6. Compara√ß√£o: Strategies de Acesso a Dados\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "strategies = [\n",
    "    (\"Full Load + Process\", time_traditional + time_query1),\n",
    "    (\"Filtered Load\", time_filtered),\n",
    "    (\"Streaming (Memory Mapped)\", time_streaming),\n",
    "    (\"DuckDB Query\", time_query2)\n",
    "]\n",
    "\n",
    "strategies.sort(key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nOrdenado por velocidade:\")\n",
    "for i, (strategy, elapsed) in enumerate(strategies, 1):\n",
    "    speedup = time_traditional / elapsed\n",
    "    print(f\"  {i}. {strategy:.<35} {elapsed:.4f}s ({speedup:.1f}x)\")\n",
    "\n",
    "print(\"\\n‚úÖ Memory Mapping benef√≠cios:\")\n",
    "print(\"   - Carrega apenas dados necess√°rios\")\n",
    "print(\"   - Economiza RAM quando processando em streaming\")\n",
    "print(\"   - Deixa o SO gerenciar cache de p√°ginas\")\n",
    "print(\"   - Ideal para arquivos maiores que a RAM\")\n",
    "\n",
    "# Limpeza\n",
    "shutil.rmtree(mmap_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0e6dc",
   "metadata": {},
   "source": [
    "## üîÑ T√≥pico 4: Buffer management\n",
    "\n",
    "Gerenciamento eficiente de buffers de mem√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Buffer management'.upper()} ---\")\n",
    "\n",
    "# 4.4.1 Gerenciamento de Batches\n",
    "print(\"\\n1. Gerenciamento de Batch Size:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Criar dataset\n",
    "n = 5000000\n",
    "large_data = pa.table({\n",
    "    'id': list(range(1, n + 1)),\n",
    "    'value': np.random.randn(n),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n),\n",
    "    'amount': np.random.uniform(10, 10000, n)\n",
    "})\n",
    "\n",
    "# Salvar para teste\n",
    "buffer_dir = 'buffer_data'\n",
    "if os.path.exists(buffer_dir):\n",
    "    shutil.rmtree(buffer_dir)\n",
    "os.makedirs(buffer_dir, exist_ok=True)\n",
    "\n",
    "parquet_buffer = f'{buffer_dir}/buffer_test.parquet'\n",
    "pq.write_table(large_data, parquet_buffer)\n",
    "\n",
    "print(f\"Dataset: {n:,} registros, {large_data.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Teste 1: Batch pequeno\n",
    "print(\"\\nTeste 1: Batch Size = 10.000 registros\")\n",
    "dataset = ds.dataset(parquet_buffer, format='parquet')\n",
    "scanner_small = dataset.scanner(batch_size=10000)\n",
    "\n",
    "batch_count = 0\n",
    "peak_memory = 0\n",
    "start = time.perf_counter()\n",
    "\n",
    "for batch in scanner_small.to_batches():\n",
    "    batch_count += 1\n",
    "    peak_memory = max(peak_memory, batch.nbytes)\n",
    "\n",
    "time_small_batch = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Batches processados: {batch_count}\")\n",
    "print(f\"  Mem√≥ria de pico por batch: {peak_memory / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Tempo total: {time_small_batch:.4f}s\")\n",
    "\n",
    "# Teste 2: Batch m√©dio\n",
    "print(\"\\nTeste 2: Batch Size = 100.000 registros\")\n",
    "scanner_medium = dataset.scanner(batch_size=100000)\n",
    "\n",
    "batch_count = 0\n",
    "peak_memory = 0\n",
    "start = time.perf_counter()\n",
    "\n",
    "for batch in scanner_medium.to_batches():\n",
    "    batch_count += 1\n",
    "    peak_memory = max(peak_memory, batch.nbytes)\n",
    "\n",
    "time_medium_batch = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Batches processados: {batch_count}\")\n",
    "print(f\"  Mem√≥ria de pico por batch: {peak_memory / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Tempo total: {time_medium_batch:.4f}s\")\n",
    "\n",
    "# Teste 3: Batch grande\n",
    "print(\"\\nTeste 3: Batch Size = 500.000 registros\")\n",
    "scanner_large = dataset.scanner(batch_size=500000)\n",
    "\n",
    "batch_count = 0\n",
    "peak_memory = 0\n",
    "start = time.perf_counter()\n",
    "\n",
    "for batch in scanner_large.to_batches():\n",
    "    batch_count += 1\n",
    "    peak_memory = max(peak_memory, batch.nbytes)\n",
    "\n",
    "time_large_batch = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Batches processados: {batch_count}\")\n",
    "print(f\"  Mem√≥ria de pico por batch: {peak_memory / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Tempo total: {time_large_batch:.4f}s\")\n",
    "\n",
    "# 4.4.2 Buffer Pool Management\n",
    "print(\"\\n2. Buffer Pool: Reutilizar buffers:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Implementar pool de buffers manual\n",
    "class BufferPool:\n",
    "    def __init__(self, buffer_size=65536, pool_size=10):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffers = [bytearray(buffer_size) for _ in range(pool_size)]\n",
    "        self.available = list(range(pool_size))\n",
    "        \n",
    "    def acquire(self):\n",
    "        if self.available:\n",
    "            return self.buffers[self.available.pop()]\n",
    "        return bytearray(self.buffer_size)\n",
    "    \n",
    "    def release(self, buffer):\n",
    "        if len(self.available) < len(self.buffers):\n",
    "            self.available.append(self.buffers.index(buffer))\n",
    "\n",
    "# Pool pequeno (1 MB)\n",
    "print(\"Buffer Pool pequeno (10 buffers x 64KB):\")\n",
    "pool_small = BufferPool(buffer_size=65536, pool_size=10)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(1000):\n",
    "    buf = pool_small.acquire()\n",
    "    buf[0] = i % 256\n",
    "    pool_small.release(buf)\n",
    "time_pool_small = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo (1000 opera√ß√µes): {time_pool_small:.4f}s\")\n",
    "\n",
    "# Pool grande (10 MB)\n",
    "print(\"\\nBuffer Pool grande (100 buffers x 64KB):\")\n",
    "pool_large = BufferPool(buffer_size=65536, pool_size=100)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(1000):\n",
    "    buf = pool_large.acquire()\n",
    "    buf[0] = i % 256\n",
    "    pool_large.release(buf)\n",
    "time_pool_large = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo (1000 opera√ß√µes): {time_pool_large:.4f}s\")\n",
    "\n",
    "# 4.4.3 Aloca√ß√£o de Mem√≥ria Eficiente\n",
    "print(\"\\n3. Aloca√ß√£o de Mem√≥ria Eficiente:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Arrow alocador\n",
    "print(\"Arrow Memory Allocator:\")\n",
    "\n",
    "# Aloca√ß√£o manual usando MemoryPool\n",
    "allocator = pa.default_memory_pool()\n",
    "print(f\"  Default pool: {allocator.backend_name}\")\n",
    "\n",
    "# Alocar buffer (Usando allocate_buffer com o pool especificado)\n",
    "buffer1 = pa.allocate_buffer(1024 * 1024, memory_pool=allocator)  # 1 MB\n",
    "print(f\"  Alocado buffer 1: {buffer1.size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "buffer2 = pa.allocate_buffer(1024 * 1024, memory_pool=allocator)  # 1 MB\n",
    "print(f\"  Alocado buffer 2: {buffer2.size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Verificar estado (estat√≠sticas do pool)\n",
    "print(f\"  Mem√≥ria alocada no pool: {allocator.bytes_allocated() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Pico de mem√≥ria no pool: {allocator.max_memory() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 4.4.4 Processamento com controle de mem√≥ria\n",
    "print(\"\\n4. Processamento com Limite de Mem√≥ria:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Simular processamento com limite\n",
    "def process_data_with_limit(parquet_file, batch_size=50000, memory_limit_mb=100):\n",
    "    \"\"\"Processa dados com limite de mem√≥ria\"\"\"\n",
    "    \n",
    "    dataset = ds.dataset(parquet_file, format='parquet')\n",
    "    scanner = dataset.scanner(batch_size=batch_size)\n",
    "    \n",
    "    total_processed = 0\n",
    "    batches = 0\n",
    "    \n",
    "    for batch in scanner.to_batches():\n",
    "        batch_mem = batch.nbytes / 1024 / 1024\n",
    "        \n",
    "        if batch_mem > memory_limit_mb:\n",
    "            print(f\"  ‚ö†Ô∏è Batch de {batch_mem:.2f} MB excede limite de {memory_limit_mb} MB\")\n",
    "            break\n",
    "        \n",
    "        # Processar batch\n",
    "        total_processed += batch.num_rows\n",
    "        batches += 1\n",
    "    \n",
    "    return total_processed, batches\n",
    "\n",
    "print(f\"Processando com limite de 50 MB por batch:\")\n",
    "total, batch_count = process_data_with_limit(parquet_buffer, batch_size=50000, memory_limit_mb=50)\n",
    "print(f\"  Registros processados: {total:,}\")\n",
    "print(f\"  Batches: {batch_count}\")\n",
    "\n",
    "# 4.4.5 Configura√ß√£o Otimizada\n",
    "print(\"\\n5. Configura√ß√£o Otimizada de Buffer:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Diferentes configura√ß√µes\n",
    "configs = [\n",
    "    (\"Conservador\", 10000, 100),      # batch_size, max_io_concurrency\n",
    "    (\"Balanceado\", 50000, 50),\n",
    "    (\"Agressivo\", 200000, 10)\n",
    "]\n",
    "\n",
    "print(\"Benchmark de configura√ß√µes:\")\n",
    "print(f\"{'Modo':<15} {'Batch Size':<12} {'IO Conc':<10} {'Tempo (s)':<10} {'Batches':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, batch_size, io_conc in configs:\n",
    "    dataset = ds.dataset(parquet_buffer, format='parquet')\n",
    "    scanner = dataset.scanner(batch_size=batch_size)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in scanner.to_batches():\n",
    "        batch_count += 1\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"{name:<15} {batch_size:<12,} {io_conc:<10} {elapsed:<10.4f} {batch_count:<10}\")\n",
    "\n",
    "# 4.4.6 DuckDB Buffer Management\n",
    "print(\"\\n6. DuckDB Buffer Management Autom√°tico:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# DuckDB gerencia buffers automaticamente\n",
    "print(\"DuckDB otimiza buffer management automaticamente\")\n",
    "\n",
    "# Query com processamento em streaming\n",
    "start = time.perf_counter()\n",
    "\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as count,\n",
    "        AVG(amount) as avg_amount,\n",
    "        SUM(amount) as total_amount\n",
    "    FROM (\n",
    "        SELECT * FROM read_parquet('{parquet_buffer}')\n",
    "        WHERE amount > 1000\n",
    "    )\n",
    "    GROUP BY category\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\").df()\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nQuery com streaming:\")\n",
    "print(f\"  Tempo: {elapsed:.4f}s\")\n",
    "print(f\"  Resultados:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n‚úÖ Buffer Management otimizado:\")\n",
    "print(\"   - Batch size apropriado = melhor performance\")\n",
    "print(\"   - Reutilizar buffers reduz aloca√ß√µes\")\n",
    "print(\"   - DuckDB gerencia automaticamente\")\n",
    "print(\"   - Limite de mem√≥ria = processamento previs√≠vel\")\n",
    "\n",
    "# Limpeza\n",
    "shutil.rmtree(buffer_dir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528873b",
   "metadata": {},
   "source": [
    "## üìä T√≥pico 5: Benchmarks\n",
    "\n",
    "Medindo e comparando performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19926e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- {'Benchmarks'.upper()} ---\")\n",
    "\n",
    "# 4.5.1 Benchmark: Zero-Copy vs Copy\n",
    "print(\"\\n1. Benchmark: Zero-Copy vs Copy:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Preparar dados grandes\n",
    "n = 10000000\n",
    "bench_data = pa.table({\n",
    "    'id': list(range(1, n + 1)),\n",
    "    'value1': np.random.randn(n),\n",
    "    'value2': np.random.randn(n),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {n:,} registros, {bench_data.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Teste 1: C√≥pia de dados (COM c√≥pia)\n",
    "print(\"\\nTeste 1: COM C√≥pia (create new objects)\")\n",
    "start = time.perf_counter()\n",
    "copy1 = bench_data.to_pandas()\n",
    "copy2 = pa.Table.from_pandas(copy1)\n",
    "copy3 = copy2.to_pandas()\n",
    "time_with_copy = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_with_copy:.4f}s\")\n",
    "print(f\"  Opera√ß√µes: to_pandas() -> from_pandas() -> to_pandas()\")\n",
    "\n",
    "# Teste 2: Zero-Copy (SEM c√≥pia desnecess√°ria)\n",
    "print(\"\\nTeste 2: ZERO-Copy (reference same buffers)\")\n",
    "start = time.perf_counter()\n",
    "# Proje√ß√µes e concatena√ß√µes horizontais em Arrow s√£o zero-copy\n",
    "view1 = bench_data.select(['id', 'value1'])\n",
    "view2 = bench_data.select(['value2', 'category'])\n",
    "\n",
    "# Combinar colunas (zero-copy)\n",
    "# Criamos uma nova tabela referenciando as colunas existentes\n",
    "view3 = pa.table({\n",
    "    'id': view1['id'],\n",
    "    'value1': view1['value1'],\n",
    "    'value2': view2['value2'],\n",
    "    'category': view2['category']\n",
    "})\n",
    "time_zero_copy = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_zero_copy:.6f}s\")\n",
    "speedup_zero = time_with_copy / max(time_zero_copy, 1e-9)\n",
    "print(f\"  Speedup: {speedup_zero:.1f}x\")\n",
    "\n",
    "# 4.5.2 Benchmark: Processamento Vetorizado\n",
    "print(\"\\n2. Benchmark: Vetorizado vs Escalar:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Dataset para computa√ß√£o\n",
    "n_comp = 5000000\n",
    "comp_data = pa.table({\n",
    "    'a': np.random.randint(1, 1000, n_comp),\n",
    "    'b': np.random.randint(1, 1000, n_comp),\n",
    "    'c': np.random.uniform(10, 1000, n_comp)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {n_comp:,} registros\")\n",
    "\n",
    "# Opera√ß√£o: (a * 2 + b) / c\n",
    "\n",
    "# Escalar\n",
    "print(\"\\nOp√ß√£o 1: Loop Escalar (Python loop)\")\n",
    "a_list = comp_data['a'].to_pylist()\n",
    "b_list = comp_data['b'].to_pylist()\n",
    "c_list = comp_data['c'].to_pylist()\n",
    "\n",
    "start = time.perf_counter()\n",
    "result_scalar = []\n",
    "for i in range(min(10000, len(a_list))):  # Apenas 10k para n√£o demorar\n",
    "    result_scalar.append((a_list[i] * 2 + b_list[i]) / c_list[i])\n",
    "# Extrapolamos para o tamanho real para compara√ß√£o justa\n",
    "time_scalar = (time.perf_counter() - start) * (n_comp / 10000)\n",
    "\n",
    "print(f\"  Tempo (extrapolado para {n_comp:,} linhas): {time_scalar:.4f}s\")\n",
    "\n",
    "# Vetorizado com Arrow Compute\n",
    "print(\"\\nOp√ß√£o 2: Arrow Compute (Vetorizado)\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "result_arrow = pc.divide(\n",
    "    pc.add(pc.multiply(comp_data['a'], 2), comp_data['b']),\n",
    "    comp_data['c']\n",
    ")\n",
    "\n",
    "time_vector = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_vector:.6f}s\")\n",
    "speedup_vec = time_scalar / max(time_vector, 1e-9)\n",
    "print(f\"  Speedup: {speedup_vec:.1f}x\")\n",
    "\n",
    "# 4.5.3 Benchmark: Diferentes Estrat√©gias de I/O\n",
    "print(\"\\n3. Benchmark: Estrat√©gias de I/O:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Preparar arquivo Parquet\n",
    "bench_dir = 'bench_data'\n",
    "if os.path.exists(bench_dir):\n",
    "    shutil.rmtree(bench_dir)\n",
    "os.makedirs(bench_dir, exist_ok=True)\n",
    "\n",
    "io_table = pa.table({\n",
    "    'id': list(range(100000)),\n",
    "    'value': np.random.randn(100000),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100000),\n",
    "    'amount': np.random.uniform(10, 1000, 100000)\n",
    "})\n",
    "\n",
    "parquet_bench = f'{bench_dir}/benchmark.parquet'\n",
    "pq.write_table(io_table, parquet_bench)\n",
    "\n",
    "print(f\"Arquivo: {os.path.getsize(parquet_bench) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Estrat√©gia 1: Full Load\n",
    "print(\"\\nEstrat√©gia 1: Full Load (ler arquivo completo)\")\n",
    "start = time.perf_counter()\n",
    "full_tbl = pq.read_table(parquet_bench)\n",
    "result_full = con.execute(\"SELECT COUNT(*) FROM full_tbl\").fetchone()\n",
    "time_full = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_full:.4f}s\")\n",
    "\n",
    "# Estrat√©gia 2: With Filter (push-down)\n",
    "print(\"\\nEstrat√©gia 2: With Filter (push-down no Parquet)\")\n",
    "start = time.perf_counter()\n",
    "filtered_tbl = pq.read_table(parquet_bench, filters=[('amount', '>', 500)])\n",
    "result_filtered = con.execute(\"SELECT COUNT(*) FROM filtered_tbl\").fetchone()\n",
    "time_filtered = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_filtered:.4f}s\")\n",
    "speedup_filt = time_full / max(time_filtered, 1e-9)\n",
    "print(f\"  Speedup: {speedup_filt:.1f}x\")\n",
    "\n",
    "# Estrat√©gia 3: With Projection\n",
    "print(\"\\nEstrat√©gia 3: With Projection (selecionar colunas)\")\n",
    "start = time.perf_counter()\n",
    "projected_tbl = pq.read_table(parquet_bench, columns=['id', 'amount'])\n",
    "result_proj = con.execute(\"SELECT COUNT(*) FROM projected_tbl\").fetchone()\n",
    "time_proj = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_proj:.4f}s\")\n",
    "speedup_proj = time_full / max(time_proj, 1e-9)\n",
    "print(f\"  Speedup: {speedup_proj:.1f}x\")\n",
    "\n",
    "# Estrat√©gia 4: With Filter + Projection\n",
    "print(\"\\nEstrat√©gia 4: With Filter + Projection (otimizado)\")\n",
    "start = time.perf_counter()\n",
    "optimized_tbl = pq.read_table(\n",
    "    parquet_bench,\n",
    "    columns=['id', 'amount'],\n",
    "    filters=[('amount', '>', 500)]\n",
    ")\n",
    "result_opt = con.execute(\"SELECT COUNT(*) FROM optimized_tbl\").fetchone()\n",
    "time_opt = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_opt:.4f}s\")\n",
    "speedup_opt = time_full / max(time_opt, 1e-9)\n",
    "print(f\"  Speedup: {speedup_opt:.1f}x\")\n",
    "\n",
    "# 4.5.4 Benchmark: Memory Usage\n",
    "print(\"\\n4. Benchmark: Uso de Mem√≥ria:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Compara√ß√£o de t√©cnicas:\")\n",
    "print(f\"{'T√©cnica':<30} {'Mem√≥ria (MB)':<15} {'Tempo (s)':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# T√©cnica 1: Full pandas\n",
    "mem_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "start = time.perf_counter()\n",
    "df_pd = io_table.to_pandas()\n",
    "mem_pandas = (psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024) - mem_before\n",
    "time_pandas = time.perf_counter() - start\n",
    "\n",
    "print(f\"{'Pandas Full':<30} {mem_pandas:<15.2f} {time_pandas:<10.4f}\")\n",
    "\n",
    "# T√©cnica 2: Arrow (zero-copy)\n",
    "mem_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "start = time.perf_counter()\n",
    "arrow_view = io_table.select(['id', 'amount'])\n",
    "mem_arrow = (psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024) - mem_before\n",
    "time_arrow = time.perf_counter() - start\n",
    "\n",
    "print(f\"{'Arrow Zero-Copy':<30} {mem_arrow:<15.2f} {time_arrow:<10.4f}\")\n",
    "\n",
    "# T√©cnica 3: Streaming\n",
    "mem_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "start = time.perf_counter()\n",
    "dataset_bench = ds.dataset(parquet_bench, format='parquet')\n",
    "scanner_bench = dataset_bench.scanner(batch_size=10000)\n",
    "batch_count_idx = 0\n",
    "for _ in scanner_bench.to_batches():\n",
    "    batch_count_idx += 1\n",
    "mem_stream = (psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024) - mem_before\n",
    "time_stream = time.perf_counter() - start\n",
    "\n",
    "print(f\"{'Streaming (10K batch)':<30} {mem_stream:<15.2f} {time_stream:<10.4f}\")\n",
    "\n",
    "# 4.5.5 Benchmark Final: End-to-End\n",
    "print(\"\\n5. Benchmark End-to-End: Diferentes Fluxos:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Carga de trabalho real√≠stica\n",
    "print(\"Carga: Ler, filtrar, agregar, salvar resultado\")\n",
    "\n",
    "# Fluxo 1: Pandas tradicional\n",
    "print(\"\\nFluxo 1: Pandas (copy-based)\")\n",
    "start = time.perf_counter()\n",
    "pdf = pq.read_table(parquet_bench).to_pandas()\n",
    "filtered_pdf = pdf[pdf['amount'] > 500]\n",
    "result_pandas = filtered_pdf.groupby('category')['amount'].agg(['count', 'mean', 'sum'])\n",
    "result_pandas.to_csv(f'{bench_dir}/result_pandas.csv')\n",
    "time_workflow_pandas = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_workflow_pandas:.4f}s\")\n",
    "\n",
    "# Fluxo 2: Arrow + DuckDB\n",
    "print(\"\\nFluxo 2: Arrow + DuckDB (zero-copy)\")\n",
    "start = time.perf_counter()\n",
    "tbl_bench = pq.read_table(parquet_bench)\n",
    "result_duckdb = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as count,\n",
    "        AVG(amount) as mean,\n",
    "        SUM(amount) as sum\n",
    "    FROM tbl_bench\n",
    "    WHERE amount > 500\n",
    "    GROUP BY category\n",
    "\"\"\").df()\n",
    "result_duckdb.to_csv(f'{bench_dir}/result_duckdb.csv', index=False)\n",
    "time_workflow_arrow = time.perf_counter() - start\n",
    "\n",
    "print(f\"  Tempo: {time_workflow_arrow:.4f}s\")\n",
    "speedup_workflow = time_workflow_pandas / max(time_workflow_arrow, 1e-9)\n",
    "print(f\"  Speedup: {speedup_workflow:.1f}x\")\n",
    "\n",
    "# 4.5.6 Resumo de Performance\n",
    "print(\"\\n6. Resumo de Performance:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "benchmarks = [\n",
    "    (\"Zero-Copy vs Copy\", time_zero_copy, time_with_copy),\n",
    "    (\"Vetorizado vs Escalar\", time_vector, time_scalar),\n",
    "    (\"Full vs Optimized\", time_full, time_opt),\n",
    "    (\"Pandas vs Arrow\", time_workflow_pandas, time_workflow_arrow)\n",
    "]\n",
    "\n",
    "print(f\"{'Benchmark':<30} {'Tempo':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, time_new, time_old in benchmarks:\n",
    "    speedup = time_old / max(time_new, 1e-9)\n",
    "    print(f\"{name:<30} {time_new:<12.6f}s {speedup:<10.1f}x\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance Insights:\")\n",
    "print(\"   ‚Ä¢ Zero-Copy: evite c√≥pias desnecess√°rias\")\n",
    "print(\"   ‚Ä¢ Vetoriza√ß√£o: 10-100x mais r√°pido que loops\")\n",
    "print(\"   ‚Ä¢ Push-down Filters: reduz I/O\")\n",
    "print(\"   ‚Ä¢ Projections: economiza mem√≥ria\")\n",
    "print(\"   ‚Ä¢ Arrow + DuckDB: otimiza√ß√£o autom√°tica\")\n",
    "\n",
    "# Limpeza\n",
    "shutil.rmtree(bench_dir, ignore_errors=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
