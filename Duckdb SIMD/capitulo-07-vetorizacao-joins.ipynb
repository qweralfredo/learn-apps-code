{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 7: Vetorização de Joins\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Hash Joins são uma das operações mais importantes em SQL. O DuckDB otimiza joins usando **prefetching** para esconder latência de memória e **processamento vetorizado** em todas as fases.\n",
    "\n",
    "### Objetivos:\n",
    "1. Entender as fases Build e Probe de Hash Join\n",
    "2. Implementar prefetching para esconder latência\n",
    "3. Comparar Nested Loop vs Hash Join\n",
    "4. Otimizar para diferentes tamanhos de tabela\n",
    "5. Entender Bloom Filters para joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb pandas numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Hash Join: Build vs Probe\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                     HASH JOIN                                │\n",
    "├──────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  FASE BUILD (tabela menor):                                  │\n",
    "│    1. Para cada linha da tabela build                        │\n",
    "│    2. Calcular hash da chave de join                         │\n",
    "│    3. Inserir (chave, valor) na hash table                   │\n",
    "│                                                              │\n",
    "│  ┌─────────────┐                                             │\n",
    "│  │ Build Table │  →  ┌─────────────────────┐                 │\n",
    "│  │   (menor)   │     │    Hash Table       │                 │\n",
    "│  └─────────────┘     │  bucket[0]: [...]   │                 │\n",
    "│                      │  bucket[1]: [...]   │                 │\n",
    "│                      │  ...                │                 │\n",
    "│                      └─────────────────────┘                 │\n",
    "│                                                              │\n",
    "│  FASE PROBE (tabela maior):                                  │\n",
    "│    1. Para cada linha da tabela probe                        │\n",
    "│    2. Calcular hash da chave de join                         │\n",
    "│    3. Buscar matches na hash table                           │\n",
    "│    4. Emitir resultado para cada match                       │\n",
    "│                                                              │\n",
    "│  ┌─────────────┐     ┌─────────────────────┐                 │\n",
    "│  │ Probe Table │  →  │    Hash Table       │  →  Resultado   │\n",
    "│  │   (maior)   │     │  (lookup)           │                 │\n",
    "│  └─────────────┘     └─────────────────────┘                 │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Complexidade: O(n + m) vs O(n × m) do Nested Loop!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação básica de Hash Join (linha por linha)\n",
    "def hash_join_basic(build_keys, build_values, probe_keys, probe_values):\n",
    "    \"\"\"Hash Join básico - processa uma linha por vez\"\"\"\n",
    "    \n",
    "    # FASE BUILD: construir hash table\n",
    "    hash_table = defaultdict(list)\n",
    "    for i in range(len(build_keys)):\n",
    "        key = build_keys[i]\n",
    "        hash_table[key].append(build_values[i])\n",
    "    \n",
    "    # FASE PROBE: buscar matches\n",
    "    result_probe = []\n",
    "    result_build = []\n",
    "    \n",
    "    for i in range(len(probe_keys)):\n",
    "        key = probe_keys[i]\n",
    "        if key in hash_table:\n",
    "            for build_val in hash_table[key]:\n",
    "                result_probe.append(probe_values[i])\n",
    "                result_build.append(build_val)\n",
    "    \n",
    "    return result_probe, result_build\n",
    "\n",
    "# Teste\n",
    "np.random.seed(42)\n",
    "build_keys = np.random.randint(0, 1000, 10000)   # 10K linhas\n",
    "build_values = np.arange(10000)\n",
    "probe_keys = np.random.randint(0, 1000, 100000)  # 100K linhas\n",
    "probe_values = np.arange(100000)\n",
    "\n",
    "start = time.perf_counter()\n",
    "r1, r2 = hash_join_basic(build_keys, build_values, probe_keys, probe_values)\n",
    "basic_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Hash Join básico: {basic_time*1000:.2f} ms\")\n",
    "print(f\"Matches encontrados: {len(r1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested Loop Join para comparação\n",
    "def nested_loop_join(build_keys, build_values, probe_keys, probe_values, limit=10000):\n",
    "    \"\"\"Nested Loop Join - O(n × m), muito lento!\"\"\"\n",
    "    result_probe = []\n",
    "    result_build = []\n",
    "    \n",
    "    # Limitar para não demorar demais\n",
    "    for i in range(min(len(probe_keys), limit)):\n",
    "        for j in range(len(build_keys)):\n",
    "            if probe_keys[i] == build_keys[j]:\n",
    "                result_probe.append(probe_values[i])\n",
    "                result_build.append(build_values[j])\n",
    "    \n",
    "    return result_probe, result_build\n",
    "\n",
    "# Benchmark com subset menor\n",
    "limit = 1000\n",
    "start = time.perf_counter()\n",
    "r1_nl, r2_nl = nested_loop_join(build_keys, build_values, probe_keys, probe_values, limit)\n",
    "nl_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Nested Loop ({limit} probe rows): {nl_time*1000:.2f} ms\")\n",
    "print(f\"Estimativa para 100K rows: {nl_time * 100:.0f} ms\")\n",
    "print(f\"Hash Join é ~{(nl_time * 100) / basic_time:.0f}x mais rápido!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Prefetching: Escondendo Latência\n",
    "\n",
    "O problema com hash joins é que os acessos à hash table são **aleatórios**:\n",
    "\n",
    "```\n",
    "SEM PREFETCH:\n",
    "  Probe key 0 → hash → lookup → WAIT (cache miss) → resultado\n",
    "  Probe key 1 → hash → lookup → WAIT (cache miss) → resultado\n",
    "  ...\n",
    "  CPU fica parada esperando memória!\n",
    "\n",
    "COM PREFETCH:\n",
    "  1. Calcular hashes para keys 0-7\n",
    "  2. Emitir prefetch para posições 0-7 da hash table\n",
    "  3. Enquanto RAM busca dados:\n",
    "     - Processar resultados do batch anterior!\n",
    "  4. Repetir\n",
    "  \n",
    "  CPU nunca fica ociosa!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulação de Hash Join com prefetching (conceitual)\n",
    "def hash_join_with_prefetch_simulation(build_keys, build_values, probe_keys, probe_values, batch_size=64):\n",
    "    \"\"\"\n",
    "    Hash Join simulando prefetching.\n",
    "    Em C++, usaríamos _mm_prefetch para instruções reais.\n",
    "    \"\"\"\n",
    "    \n",
    "    # FASE BUILD\n",
    "    hash_table = defaultdict(list)\n",
    "    for i in range(len(build_keys)):\n",
    "        hash_table[build_keys[i]].append(build_values[i])\n",
    "    \n",
    "    # FASE PROBE com batching (simula prefetch)\n",
    "    result_probe = []\n",
    "    result_build = []\n",
    "    \n",
    "    n_probe = len(probe_keys)\n",
    "    \n",
    "    for batch_start in range(0, n_probe, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, n_probe)\n",
    "        \n",
    "        # 1. Calcular hashes do batch (vetorizado)\n",
    "        batch_keys = probe_keys[batch_start:batch_end]\n",
    "        batch_values = probe_values[batch_start:batch_end]\n",
    "        \n",
    "        # 2. \"Prefetch\" - em C++ seria _mm_prefetch\n",
    "        # Aqui apenas organizamos o trabalho em batches\n",
    "        \n",
    "        # 3. Processar batch\n",
    "        for i, key in enumerate(batch_keys):\n",
    "            if key in hash_table:\n",
    "                for build_val in hash_table[key]:\n",
    "                    result_probe.append(batch_values[i])\n",
    "                    result_build.append(build_val)\n",
    "    \n",
    "    return result_probe, result_build\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "r1_pf, r2_pf = hash_join_with_prefetch_simulation(\n",
    "    build_keys, build_values, probe_keys, probe_values, batch_size=64\n",
    ")\n",
    "prefetch_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Hash Join com prefetch simulado: {prefetch_time*1000:.2f} ms\")\n",
    "print(f\"Matches: {len(r1_pf):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Hash Join Vetorizado\n",
    "\n",
    "Processa múltiplas chaves de uma vez usando NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_join_vectorized(build_keys, build_values, probe_keys, probe_values):\n",
    "    \"\"\"\n",
    "    Hash Join usando operações vetorizadas NumPy.\n",
    "    Mais próximo de como o DuckDB implementa.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build: criar índice de posições para cada chave\n",
    "    # Usando sorting para agrupar chaves iguais\n",
    "    build_sort_idx = np.argsort(build_keys)\n",
    "    build_keys_sorted = build_keys[build_sort_idx]\n",
    "    build_values_sorted = build_values[build_sort_idx]\n",
    "    \n",
    "    # Encontrar onde cada chave única começa\n",
    "    unique_build, build_starts = np.unique(build_keys_sorted, return_index=True)\n",
    "    build_ends = np.append(build_starts[1:], len(build_keys_sorted))\n",
    "    \n",
    "    # Criar dicionário de ranges\n",
    "    key_to_range = {k: (build_starts[i], build_ends[i]) for i, k in enumerate(unique_build)}\n",
    "    \n",
    "    # Probe: encontrar quais probe keys existem no build\n",
    "    probe_in_build = np.isin(probe_keys, unique_build)\n",
    "    matching_probe_idx = np.where(probe_in_build)[0]\n",
    "    \n",
    "    # Coletar resultados\n",
    "    result_probe = []\n",
    "    result_build = []\n",
    "    \n",
    "    for idx in matching_probe_idx:\n",
    "        key = probe_keys[idx]\n",
    "        start, end = key_to_range[key]\n",
    "        \n",
    "        for build_idx in range(start, end):\n",
    "            result_probe.append(probe_values[idx])\n",
    "            result_build.append(build_values_sorted[build_idx])\n",
    "    \n",
    "    return np.array(result_probe), np.array(result_build)\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "r1_vec, r2_vec = hash_join_vectorized(build_keys, build_values, probe_keys, probe_values)\n",
    "vec_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Hash Join vetorizado: {vec_time*1000:.2f} ms\")\n",
    "print(f\"Matches: {len(r1_vec):,}\")\n",
    "print(f\"Speedup vs básico: {basic_time/vec_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Bloom Filters\n",
    "\n",
    "Bloom Filters permitem descartar rapidamente chaves que **certamente não têm match**:\n",
    "\n",
    "```\n",
    "BLOOM FILTER:\n",
    "  - Estrutura probabilística\n",
    "  - Responde: \"Definitivamente não está\" ou \"Talvez esteja\"\n",
    "  - Muito compacto (bits em vez de valores)\n",
    "\n",
    "USO EM JOINS:\n",
    "  1. Durante BUILD: inserir todas as chaves no Bloom Filter\n",
    "  2. Durante PROBE: primeiro checar Bloom Filter\n",
    "     - Se \"não está\": pular (evita lookup na hash table)\n",
    "     - Se \"talvez\": fazer lookup normal\n",
    "\n",
    "Benefício: Evita lookups desnecessários na hash table!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBloomFilter:\n",
    "    \"\"\"Implementação simples de Bloom Filter\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 10000, num_hashes: int = 3):\n",
    "        self.size = size\n",
    "        self.num_hashes = num_hashes\n",
    "        self.bits = np.zeros(size, dtype=bool)\n",
    "    \n",
    "    def _hashes(self, key):\n",
    "        \"\"\"Gera múltiplos hashes para uma chave\"\"\"\n",
    "        hashes = []\n",
    "        for i in range(self.num_hashes):\n",
    "            h = hash((key, i)) % self.size\n",
    "            hashes.append(h)\n",
    "        return hashes\n",
    "    \n",
    "    def add(self, key):\n",
    "        \"\"\"Adiciona chave ao filtro\"\"\"\n",
    "        for h in self._hashes(key):\n",
    "            self.bits[h] = True\n",
    "    \n",
    "    def might_contain(self, key) -> bool:\n",
    "        \"\"\"Retorna True se a chave PODE estar presente\"\"\"\n",
    "        return all(self.bits[h] for h in self._hashes(key))\n",
    "    \n",
    "    def add_batch(self, keys):\n",
    "        \"\"\"Adiciona múltiplas chaves (vetorizado quando possível)\"\"\"\n",
    "        for key in keys:\n",
    "            self.add(key)\n",
    "\n",
    "# Demonstração\n",
    "bloom = SimpleBloomFilter(size=50000, num_hashes=3)\n",
    "\n",
    "# Adicionar chaves do build\n",
    "unique_build_keys = np.unique(build_keys)\n",
    "bloom.add_batch(unique_build_keys)\n",
    "\n",
    "# Testar algumas chaves\n",
    "test_keys = [0, 1, 999, 1500, 2000]  # 0-999 devem existir, 1500+ não\n",
    "for k in test_keys:\n",
    "    result = bloom.might_contain(k)\n",
    "    actually_exists = k in unique_build_keys\n",
    "    print(f\"Key {k}: Bloom diz '{result}', realmente existe: {actually_exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_join_with_bloom(build_keys, build_values, probe_keys, probe_values):\n",
    "    \"\"\"\n",
    "    Hash Join usando Bloom Filter para filtrar probes.\n",
    "    \"\"\"\n",
    "    # Criar Bloom Filter durante BUILD\n",
    "    bloom = SimpleBloomFilter(size=len(build_keys) * 10, num_hashes=3)\n",
    "    \n",
    "    hash_table = defaultdict(list)\n",
    "    for i in range(len(build_keys)):\n",
    "        key = build_keys[i]\n",
    "        hash_table[key].append(build_values[i])\n",
    "        bloom.add(key)\n",
    "    \n",
    "    # PROBE com Bloom Filter\n",
    "    result_probe = []\n",
    "    result_build = []\n",
    "    bloom_filtered = 0\n",
    "    \n",
    "    for i in range(len(probe_keys)):\n",
    "        key = probe_keys[i]\n",
    "        \n",
    "        # Primeiro: checar Bloom Filter\n",
    "        if not bloom.might_contain(key):\n",
    "            bloom_filtered += 1\n",
    "            continue  # Definitivamente não tem match!\n",
    "        \n",
    "        # Talvez tenha match: fazer lookup normal\n",
    "        if key in hash_table:\n",
    "            for build_val in hash_table[key]:\n",
    "                result_probe.append(probe_values[i])\n",
    "                result_build.append(build_val)\n",
    "    \n",
    "    return result_probe, result_build, bloom_filtered\n",
    "\n",
    "# Teste com probe keys que têm muitos non-matches\n",
    "probe_keys_sparse = np.random.randint(0, 5000, 100000)  # Muitos não existirão\n",
    "probe_values_sparse = np.arange(100000)\n",
    "\n",
    "start = time.perf_counter()\n",
    "r1_bloom, r2_bloom, filtered = hash_join_with_bloom(\n",
    "    build_keys, build_values, probe_keys_sparse, probe_values_sparse\n",
    ")\n",
    "bloom_time = time.perf_counter() - start\n",
    "\n",
    "# Sem Bloom\n",
    "start = time.perf_counter()\n",
    "r1_no_bloom, r2_no_bloom = hash_join_basic(\n",
    "    build_keys, build_values, probe_keys_sparse, probe_values_sparse\n",
    ")\n",
    "no_bloom_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Sem Bloom Filter: {no_bloom_time*1000:.2f} ms\")\n",
    "print(f\"Com Bloom Filter: {bloom_time*1000:.2f} ms\")\n",
    "print(f\"Probes filtrados pelo Bloom: {filtered:,} ({100*filtered/len(probe_keys_sparse):.1f}%)\")\n",
    "print(f\"Speedup: {no_bloom_time/bloom_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Benchmark: Impacto do Tamanho das Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variar tamanho da tabela build\n",
    "probe_size = 100000\n",
    "probe_keys_fixed = np.random.randint(0, 10000, probe_size)\n",
    "probe_values_fixed = np.arange(probe_size)\n",
    "\n",
    "build_sizes = [100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "results = []\n",
    "\n",
    "for bs in build_sizes:\n",
    "    build_keys_test = np.random.randint(0, 10000, bs)\n",
    "    build_values_test = np.arange(bs)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(3):\n",
    "        start = time.perf_counter()\n",
    "        _ = hash_join_basic(build_keys_test, build_values_test, \n",
    "                           probe_keys_fixed, probe_values_fixed)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000\n",
    "    results.append({'build_size': bs, 'time_ms': avg_time})\n",
    "    print(f\"Build size {bs:>6}: {avg_time:6.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_results['build_size'], df_results['time_ms'], marker='o', linewidth=2)\n",
    "plt.xlabel('Tamanho da Tabela Build')\n",
    "plt.ylabel('Tempo (ms)')\n",
    "plt.title('Tempo de Hash Join vs Tamanho da Tabela Build\\n(Probe fixo em 100K linhas)')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 DuckDB: Joins em Ação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "# Criar tabelas\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE produtos AS\n",
    "    SELECT \n",
    "        i AS produto_id,\n",
    "        'Produto_' || LPAD(i::VARCHAR, 5, '0') AS nome,\n",
    "        random() * 100 AS preco,\n",
    "        (random() * 10)::INTEGER AS categoria_id\n",
    "    FROM generate_series(1, 10000) AS t(i)\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE vendas AS\n",
    "    SELECT \n",
    "        i AS venda_id,\n",
    "        (random() * 10000)::INTEGER + 1 AS produto_id,\n",
    "        random() * 100 AS quantidade,\n",
    "        DATE '2024-01-01' + (random() * 365)::INTEGER AS data_venda\n",
    "    FROM generate_series(1, 10000000) AS t(i)\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE categorias AS\n",
    "    SELECT \n",
    "        i AS categoria_id,\n",
    "        'Categoria_' || i AS nome_categoria\n",
    "    FROM generate_series(0, 10) AS t(i)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabelas criadas:\")\n",
    "print(\"  - produtos: 10K linhas\")\n",
    "print(\"  - vendas: 10M linhas\")\n",
    "print(\"  - categorias: 11 linhas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark de diferentes tipos de joins\n",
    "queries = {\n",
    "    'INNER JOIN simples': '''\n",
    "        SELECT COUNT(*) \n",
    "        FROM vendas v \n",
    "        INNER JOIN produtos p ON v.produto_id = p.produto_id\n",
    "    ''',\n",
    "    'INNER JOIN com projeção': '''\n",
    "        SELECT v.venda_id, p.nome, v.quantidade\n",
    "        FROM vendas v \n",
    "        INNER JOIN produtos p ON v.produto_id = p.produto_id\n",
    "        LIMIT 1000000\n",
    "    ''',\n",
    "    'JOIN + Agregação': '''\n",
    "        SELECT p.nome, SUM(v.quantidade * p.preco) as total\n",
    "        FROM vendas v\n",
    "        INNER JOIN produtos p ON v.produto_id = p.produto_id\n",
    "        GROUP BY p.nome\n",
    "    ''',\n",
    "    'LEFT JOIN': '''\n",
    "        SELECT COUNT(*)\n",
    "        FROM produtos p\n",
    "        LEFT JOIN vendas v ON p.produto_id = v.produto_id\n",
    "    ''',\n",
    "    'Multi-table JOIN': '''\n",
    "        SELECT c.nome_categoria, COUNT(*) as total_vendas\n",
    "        FROM vendas v\n",
    "        JOIN produtos p ON v.produto_id = p.produto_id\n",
    "        JOIN categorias c ON p.categoria_id = c.categoria_id\n",
    "        GROUP BY c.nome_categoria\n",
    "    ''',\n",
    "    'JOIN com filtro': '''\n",
    "        SELECT COUNT(*)\n",
    "        FROM vendas v\n",
    "        JOIN produtos p ON v.produto_id = p.produto_id\n",
    "        WHERE p.preco > 50 AND v.quantidade > 50\n",
    "    ''',\n",
    "}\n",
    "\n",
    "print(\"Performance de Joins DuckDB:\\n\")\n",
    "join_results = []\n",
    "for name, query in queries.items():\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.perf_counter()\n",
    "        con.execute(query).fetchall()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000\n",
    "    join_results.append({'query': name, 'time_ms': avg_time})\n",
    "    print(f\"{name:30} {avg_time:8.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "df_joins = pd.DataFrame(join_results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(df_joins['query'], df_joins['time_ms'], color='steelblue')\n",
    "plt.xlabel('Tempo (ms)')\n",
    "plt.title('Performance de Diferentes Joins no DuckDB')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, df_joins['time_ms']):\n",
    "    plt.text(val + 5, bar.get_y() + bar.get_height()/2, f'{val:.1f}ms', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver plano de execução\n",
    "print(\"=== EXPLAIN ===\")\n",
    "plan = con.execute(\"\"\"\n",
    "    EXPLAIN\n",
    "    SELECT p.nome, SUM(v.quantidade * p.preco) as total\n",
    "    FROM vendas v\n",
    "    JOIN produtos p ON v.produto_id = p.produto_id\n",
    "    GROUP BY p.nome\n",
    "\"\"\").df()\n",
    "print(plan.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain analyze para ver estatísticas\n",
    "print(\"\\n=== EXPLAIN ANALYZE ===\")\n",
    "plan = con.execute(\"\"\"\n",
    "    EXPLAIN ANALYZE\n",
    "    SELECT COUNT(*) \n",
    "    FROM vendas v \n",
    "    INNER JOIN produtos p ON v.produto_id = p.produto_id\n",
    "\"\"\").df()\n",
    "print(plan.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Comparação: DuckDB vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados para Pandas (subset para não demorar muito)\n",
    "df_vendas = con.execute(\"SELECT produto_id, quantidade FROM vendas\").df()\n",
    "df_produtos = con.execute(\"SELECT produto_id, nome, preco FROM produtos\").df()\n",
    "\n",
    "print(f\"Vendas: {len(df_vendas):,} linhas\")\n",
    "print(f\"Produtos: {len(df_produtos):,} linhas\")\n",
    "\n",
    "# Benchmark: JOIN simples\n",
    "# DuckDB\n",
    "times_duck = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    con.execute(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM vendas v \n",
    "        JOIN produtos p ON v.produto_id = p.produto_id\n",
    "    \"\"\").fetchall()\n",
    "    times_duck.append(time.perf_counter() - start)\n",
    "\n",
    "# Pandas\n",
    "times_pandas = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    merged = df_vendas.merge(df_produtos, on='produto_id')\n",
    "    count = len(merged)\n",
    "    times_pandas.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"\\nINNER JOIN (10M × 10K):\")\n",
    "print(f\"DuckDB: {np.mean(times_duck)*1000:.2f} ms\")\n",
    "print(f\"Pandas: {np.mean(times_pandas)*1000:.2f} ms\")\n",
    "print(f\"Speedup: {np.mean(times_pandas)/np.mean(times_duck):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Resumo\n",
    "\n",
    "| Otimização | Descrição | Benefício |\n",
    "|------------|-----------|----------|\n",
    "| **Build menor** | Tabela menor vira hash table | Cabe no cache |\n",
    "| **Prefetching** | Emite loads antes de precisar | Esconde latência RAM |\n",
    "| **Probe vetorizado** | Processa múltiplas chaves por vez | SIMD + cache |\n",
    "| **Bloom filters** | Filtra probes que não têm match | Evita lookups |\n",
    "| **Particionamento** | Divide hash table por threads | Paralelismo |\n",
    "\n",
    "### Pontos Chave:\n",
    "\n",
    "1. **Hash Join O(n+m)**: Muito melhor que Nested Loop O(n×m)\n",
    "2. **Build a tabela menor**: Hash table menor = mais cache hits\n",
    "3. **Prefetching**: CPU nunca fica ociosa esperando memória\n",
    "4. **Bloom Filter**: Descarta non-matches rapidamente\n",
    "5. **Vetorização**: Processa batches de chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}