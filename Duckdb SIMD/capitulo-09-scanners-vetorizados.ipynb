{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 9: Scanners Vetorizados\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Scanners são responsáveis por ler dados de fontes externas (arquivos Parquet, CSV, etc.) e convertê-los para o formato interno do DuckDB. Scanners vetorizados utilizam **SIMD para descompressão** e técnicas como **bit-unpacking** para maximizar throughput.\n",
    "\n",
    "### Objetivos:\n",
    "1. Entender como Parquet armazena dados compactados\n",
    "2. Implementar bit-packing e unpacking vetorizado\n",
    "3. Usar SIMD shuffle para descompressão\n",
    "4. Benchmark de leitura vetorizada vs linha-a-linha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb pandas numpy matplotlib pyarrow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import struct\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Bit-Packing: Compressão de Inteiros\n",
    "\n",
    "Parquet usa **bit-packing** para compactar inteiros pequenos:\n",
    "\n",
    "```\n",
    "Valores: [0, 1, 2, 3, 4, 5, 6, 7]  (precisam de apenas 3 bits cada)\n",
    "\n",
    "Sem compressão (32 bits cada): 8 × 4 bytes = 32 bytes\n",
    "Com bit-packing (3 bits cada): 8 × 3 bits = 24 bits = 3 bytes\n",
    "\n",
    "Layout compactado:\n",
    "┌─────────────────────────────────┐\n",
    "│ 000 001 010 011 │ 100 101 110 111 │\n",
    "│   Byte 0-1      │    Byte 2       │\n",
    "└─────────────────────────────────┘\n",
    "\n",
    "Compressão: ~10x menor!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de Bit-Packing\n",
    "\n",
    "def bit_pack(values: np.ndarray, bit_width: int) -> bytes:\n",
    "    \"\"\"Compacta valores inteiros usando bit_width bits por valor\"\"\"\n",
    "    result = []\n",
    "    buffer = 0\n",
    "    bits_in_buffer = 0\n",
    "    \n",
    "    for val in values:\n",
    "        # Adiciona valor ao buffer\n",
    "        buffer |= (val & ((1 << bit_width) - 1)) << bits_in_buffer\n",
    "        bits_in_buffer += bit_width\n",
    "        \n",
    "        # Emite bytes completos\n",
    "        while bits_in_buffer >= 8:\n",
    "            result.append(buffer & 0xFF)\n",
    "            buffer >>= 8\n",
    "            bits_in_buffer -= 8\n",
    "    \n",
    "    # Emite bits restantes\n",
    "    if bits_in_buffer > 0:\n",
    "        result.append(buffer & 0xFF)\n",
    "    \n",
    "    return bytes(result)\n",
    "\n",
    "def bit_unpack_scalar(packed: bytes, bit_width: int, count: int) -> np.ndarray:\n",
    "    \"\"\"Descompacta valores um por vez (lento)\"\"\"\n",
    "    result = np.zeros(count, dtype=np.int32)\n",
    "    mask = (1 << bit_width) - 1\n",
    "    \n",
    "    buffer = 0\n",
    "    bits_in_buffer = 0\n",
    "    byte_idx = 0\n",
    "    \n",
    "    for i in range(count):\n",
    "        # Carregar mais bytes se necessário\n",
    "        while bits_in_buffer < bit_width:\n",
    "            buffer |= packed[byte_idx] << bits_in_buffer\n",
    "            bits_in_buffer += 8\n",
    "            byte_idx += 1\n",
    "        \n",
    "        # Extrair valor\n",
    "        result[i] = buffer & mask\n",
    "        buffer >>= bit_width\n",
    "        bits_in_buffer -= bit_width\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Teste\n",
    "original = np.array([0, 1, 2, 3, 4, 5, 6, 7], dtype=np.int32)\n",
    "packed = bit_pack(original, 3)\n",
    "unpacked = bit_unpack_scalar(packed, 3, len(original))\n",
    "\n",
    "print(f\"Original:   {original}\")\n",
    "print(f\"Packed:     {list(packed)} ({len(packed)} bytes)\")\n",
    "print(f\"Unpacked:   {unpacked}\")\n",
    "print(f\"Compressão: {len(original) * 4} bytes -> {len(packed)} bytes ({len(original) * 4 / len(packed):.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Unpacking Vetorizado\n",
    "\n",
    "Em vez de extrair bit por bit, processamos múltiplos valores de uma vez:\n",
    "\n",
    "```\n",
    "Estratégia SIMD (simplificada):\n",
    "1. Carregar 128 bits de dados compactados\n",
    "2. Usar lookup tables para mapear grupos de bits\n",
    "3. Shuffle para reorganizar bytes\n",
    "4. Máscara AND para extrair valores\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_unpack_vectorized(packed: bytes, bit_width: int, count: int) -> np.ndarray:\n",
    "    \"\"\"Descompacta valores de forma vetorizada usando NumPy\"\"\"\n",
    "    # Converter bytes para array de inteiros grandes\n",
    "    # Processa 8 valores por vez (64 bits)\n",
    "    \n",
    "    result = np.zeros(count, dtype=np.int32)\n",
    "    mask = (1 << bit_width) - 1\n",
    "    \n",
    "    # Processar em chunks de 8 valores\n",
    "    bits_per_chunk = 8 * bit_width\n",
    "    bytes_per_chunk = (bits_per_chunk + 7) // 8\n",
    "    \n",
    "    for chunk_start in range(0, count, 8):\n",
    "        chunk_end = min(chunk_start + 8, count)\n",
    "        values_in_chunk = chunk_end - chunk_start\n",
    "        \n",
    "        # Calcular posição no buffer\n",
    "        bit_offset = chunk_start * bit_width\n",
    "        byte_offset = bit_offset // 8\n",
    "        bit_shift = bit_offset % 8\n",
    "        \n",
    "        # Carregar bytes necessários como inteiro grande\n",
    "        bytes_needed = (values_in_chunk * bit_width + bit_shift + 7) // 8\n",
    "        chunk_bytes = packed[byte_offset:byte_offset + bytes_needed + 1]\n",
    "        \n",
    "        # Converter para inteiro\n",
    "        value = int.from_bytes(chunk_bytes, 'little')\n",
    "        value >>= bit_shift\n",
    "        \n",
    "        # Extrair valores (pode ser vetorizado com shifts)\n",
    "        for i in range(values_in_chunk):\n",
    "            result[chunk_start + i] = value & mask\n",
    "            value >>= bit_width\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usando NumPy puro para simulação mais realista\n",
    "def bit_unpack_numpy(packed_array: np.ndarray, bit_width: int, count: int) -> np.ndarray:\n",
    "    \"\"\"Simulação de unpacking vetorizado com NumPy\"\"\"\n",
    "    # Converte bytes para bits\n",
    "    bits = np.unpackbits(packed_array)\n",
    "    \n",
    "    # Reorganiza em grupos de bit_width\n",
    "    total_bits = count * bit_width\n",
    "    bits = bits[:total_bits].reshape(-1, bit_width)\n",
    "    \n",
    "    # Converte cada grupo para inteiro (vetorizado)\n",
    "    powers = 2 ** np.arange(bit_width)\n",
    "    result = np.sum(bits * powers, axis=1)\n",
    "    \n",
    "    return result.astype(np.int32)\n",
    "\n",
    "# Teste\n",
    "packed_array = np.frombuffer(packed, dtype=np.uint8)\n",
    "result_vec = bit_unpack_vectorized(packed, 3, 8)\n",
    "print(f\"Vetorizado: {result_vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Benchmark: Scalar vs Vetorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark com dados maiores\n",
    "n = 100_000\n",
    "bit_widths = [3, 5, 8, 12, 16]\n",
    "\n",
    "results = []\n",
    "\n",
    "for bw in bit_widths:\n",
    "    # Gerar dados que cabem em bit_width bits\n",
    "    max_val = (1 << bw) - 1\n",
    "    original = np.random.randint(0, max_val + 1, n, dtype=np.int32)\n",
    "    packed = bit_pack(original, bw)\n",
    "    \n",
    "    # Benchmark scalar\n",
    "    times_scalar = []\n",
    "    for _ in range(3):\n",
    "        start = time.perf_counter()\n",
    "        _ = bit_unpack_scalar(packed, bw, n)\n",
    "        times_scalar.append(time.perf_counter() - start)\n",
    "    \n",
    "    # Benchmark vetorizado\n",
    "    times_vec = []\n",
    "    for _ in range(3):\n",
    "        start = time.perf_counter()\n",
    "        _ = bit_unpack_vectorized(packed, bw, n)\n",
    "        times_vec.append(time.perf_counter() - start)\n",
    "    \n",
    "    results.append({\n",
    "        'bit_width': bw,\n",
    "        'scalar_ms': np.mean(times_scalar) * 1000,\n",
    "        'vectorized_ms': np.mean(times_vec) * 1000,\n",
    "        'compression': (n * 4) / len(packed)\n",
    "    })\n",
    "    \n",
    "    print(f\"Bit width {bw:2}: Scalar {results[-1]['scalar_ms']:6.1f}ms, \"\n",
    "          f\"Vec {results[-1]['vectorized_ms']:6.1f}ms, \"\n",
    "          f\"Compressão {results[-1]['compression']:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Tempo de unpacking\n",
    "x = np.arange(len(bit_widths))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, df['scalar_ms'], width, label='Scalar', color='#e74c3c')\n",
    "ax1.bar(x + width/2, df['vectorized_ms'], width, label='Vetorizado', color='#27ae60')\n",
    "ax1.set_xlabel('Bit Width')\n",
    "ax1.set_ylabel('Tempo (ms)')\n",
    "ax1.set_title('Tempo de Unpacking por Bit Width')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(bit_widths)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Taxa de compressão\n",
    "ax2.bar(bit_widths, df['compression'], color='#3498db')\n",
    "ax2.set_xlabel('Bit Width')\n",
    "ax2.set_ylabel('Taxa de Compressão')\n",
    "ax2.set_title('Compressão vs Bit Width')\n",
    "ax2.axhline(y=1, color='red', linestyle='--', label='Sem compressão')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 DuckDB: Leitura de Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "# Criar dados de teste\n",
    "n = 5_000_000\n",
    "df = pd.DataFrame({\n",
    "    'id': np.arange(n),\n",
    "    'small_int': np.random.randint(0, 100, n),      # Valores pequenos - alta compressão\n",
    "    'medium_int': np.random.randint(0, 10000, n),   # Valores médios\n",
    "    'large_int': np.random.randint(0, 1000000, n),  # Valores grandes\n",
    "    'float_col': np.random.randn(n)\n",
    "})\n",
    "\n",
    "# Salvar como Parquet\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "parquet_path = os.path.join(temp_dir, 'test_data.parquet')\n",
    "df.to_parquet(parquet_path, compression='snappy')\n",
    "\n",
    "file_size = os.path.getsize(parquet_path)\n",
    "print(f\"Arquivo Parquet: {file_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Dados originais: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Compressão total: {df.memory_usage(deep=True).sum() / file_size:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark de leitura\n",
    "queries = {\n",
    "    'Full Scan': f\"SELECT COUNT(*) FROM '{parquet_path}'\",\n",
    "    'Single Column': f\"SELECT SUM(small_int) FROM '{parquet_path}'\",\n",
    "    'Multiple Columns': f\"SELECT SUM(small_int), AVG(medium_int), MAX(large_int) FROM '{parquet_path}'\",\n",
    "    'Filtered Scan': f\"SELECT SUM(small_int) FROM '{parquet_path}' WHERE medium_int > 5000\",\n",
    "    'Projection': f\"SELECT id, small_int FROM '{parquet_path}' WHERE id < 1000000\",\n",
    "}\n",
    "\n",
    "print(\"Benchmark de Leitura Parquet:\\n\")\n",
    "for name, query in queries.items():\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.perf_counter()\n",
    "        con.execute(query).fetchall()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    throughput = file_size / np.mean(times) / 1024 / 1024  # MB/s\n",
    "    print(f\"{name:20} {np.mean(times)*1000:8.2f} ms ({throughput:.0f} MB/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar com Pandas\n",
    "print(\"\\nComparação DuckDB vs Pandas:\\n\")\n",
    "\n",
    "# DuckDB\n",
    "times_duck = []\n",
    "for _ in range(3):\n",
    "    start = time.perf_counter()\n",
    "    con.execute(f\"SELECT SUM(small_int), AVG(float_col) FROM '{parquet_path}'\").fetchall()\n",
    "    times_duck.append(time.perf_counter() - start)\n",
    "\n",
    "# Pandas\n",
    "times_pandas = []\n",
    "for _ in range(3):\n",
    "    start = time.perf_counter()\n",
    "    df_read = pd.read_parquet(parquet_path)\n",
    "    result = df_read['small_int'].sum(), df_read['float_col'].mean()\n",
    "    times_pandas.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"DuckDB: {np.mean(times_duck)*1000:.1f} ms\")\n",
    "print(f\"Pandas: {np.mean(times_pandas)*1000:.1f} ms\")\n",
    "print(f\"Speedup: {np.mean(times_pandas)/np.mean(times_duck):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Dictionary Encoding\n",
    "\n",
    "Parquet também usa **dictionary encoding** para strings repetidas:\n",
    "\n",
    "```\n",
    "Dados originais: [\"São Paulo\", \"Rio\", \"São Paulo\", \"Curitiba\", \"Rio\", ...]\n",
    "\n",
    "Dictionary: {0: \"São Paulo\", 1: \"Rio\", 2: \"Curitiba\"}\n",
    "Encoded:    [0, 1, 0, 2, 1, ...]  <- Apenas índices!\n",
    "\n",
    "Se houver 3 valores únicos, cada índice usa apenas 2 bits!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração de Dictionary Encoding\n",
    "class DictionaryEncoder:\n",
    "    \"\"\"Simula dictionary encoding do Parquet\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dictionary = []\n",
    "        self.value_to_idx = {}\n",
    "    \n",
    "    def encode(self, values: list) -> tuple:\n",
    "        \"\"\"Retorna (dictionary, indices)\"\"\"\n",
    "        indices = []\n",
    "        \n",
    "        for val in values:\n",
    "            if val not in self.value_to_idx:\n",
    "                self.value_to_idx[val] = len(self.dictionary)\n",
    "                self.dictionary.append(val)\n",
    "            indices.append(self.value_to_idx[val])\n",
    "        \n",
    "        return self.dictionary, np.array(indices, dtype=np.int32)\n",
    "    \n",
    "    def decode_vectorized(self, indices: np.ndarray) -> list:\n",
    "        \"\"\"Decodifica usando lookup vetorizado\"\"\"\n",
    "        dict_array = np.array(self.dictionary)\n",
    "        return dict_array[indices]  # Gather operation!\n",
    "\n",
    "# Teste com dados de exemplo\n",
    "cities = ['São Paulo', 'Rio de Janeiro', 'Curitiba', 'Belo Horizonte', 'Salvador']\n",
    "data = np.random.choice(cities, 100000)\n",
    "\n",
    "encoder = DictionaryEncoder()\n",
    "dictionary, indices = encoder.encode(data)\n",
    "\n",
    "print(f\"Dicionário: {dictionary}\")\n",
    "print(f\"Primeiros índices: {indices[:20]}\")\n",
    "print(f\"\\nCompressão:\")\n",
    "print(f\"  Original: {sum(len(s) for s in data)} bytes (strings)\")\n",
    "print(f\"  Dicionário: {sum(len(s) for s in dictionary)} + {len(indices) * 4} bytes\")\n",
    "\n",
    "# Bits necessários para índices\n",
    "bits_needed = int(np.ceil(np.log2(len(dictionary))))\n",
    "print(f\"  Com bit-packing ({bits_needed} bits): {len(indices) * bits_needed // 8} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Resumo\n",
    "\n",
    "| Técnica | Descrição | Benefício |\n",
    "|---------|-----------|----------|\n",
    "| **Bit-Packing** | Compacta inteiros usando bits mínimos | Redução de I/O |\n",
    "| **SIMD Unpack** | Descompacta múltiplos valores por instrução | CPU eficiente |\n",
    "| **Dictionary Encoding** | Substitui valores por índices | Strings compactas |\n",
    "| **Column Pruning** | Lê apenas colunas necessárias | Menos I/O |\n",
    "| **Predicate Pushdown** | Filtra durante leitura | Menos dados |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "con.close()\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}