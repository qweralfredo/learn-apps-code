{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 1: O Fim do Modelo \"Volcano\"\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Durante décadas, o modelo **Volcano** (também conhecido como Iterator Model) foi o padrão de facto para execução de consultas em bancos de dados relacionais. Sistemas como PostgreSQL, MySQL e Oracle utilizam variações deste modelo.\n",
    "\n",
    "No entanto, o DuckDB representa uma mudança de paradigma: ele utiliza o **Modelo Vetorizado**, que processa dados em blocos (vetores) ao invés de linha por linha.\n",
    "\n",
    "### Objetivos deste capítulo:\n",
    "1. Entender o modelo Volcano e suas limitações\n",
    "2. Compreender o modelo vetorizado do DuckDB\n",
    "3. Comparar performance entre os dois modelos\n",
    "4. Visualizar o impacto na CPU e cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 O Modelo Volcano Tradicional\n",
    "\n",
    "No modelo Volcano, cada operador implementa três métodos:\n",
    "- `open()`: Inicializa o operador\n",
    "- `next()`: Retorna a **próxima tupla** (uma linha por vez)\n",
    "- `close()`: Libera recursos\n",
    "\n",
    "### Problemas do Modelo Volcano:\n",
    "1. **Overhead de chamadas de função**: Cada linha requer uma chamada virtual\n",
    "2. **Cache misses**: Dados não ficam no cache L1/L2\n",
    "3. **Branch misprediction**: CPU não consegue prever o fluxo\n",
    "4. **Sem SIMD**: Não aproveita instruções vetoriais modernas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das dependências\n",
    "!pip install duckdb pandas numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuração visual\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Simulando o Modelo Volcano em Python\n",
    "\n",
    "Vamos criar uma simulação didática do modelo Volcano para entender o overhead de processar linha por linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolcanoIterator:\n",
    "    \"\"\"\n",
    "    Simulação do modelo Volcano - processa uma linha por vez\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "    \n",
    "    def open(self):\n",
    "        self.index = 0\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Retorna próxima tupla - chamada de função por linha!\"\"\"\n",
    "        if self.index >= len(self.data):\n",
    "            return None\n",
    "        row = self.data[self.index]\n",
    "        self.index += 1\n",
    "        return row\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VolcanoSum(VolcanoIterator):\n",
    "    \"\"\"Operador de soma no modelo Volcano\"\"\"\n",
    "    def __init__(self, child, col_a, col_b):\n",
    "        self.child = child\n",
    "        self.col_a = col_a\n",
    "        self.col_b = col_b\n",
    "    \n",
    "    def open(self):\n",
    "        self.child.open()\n",
    "    \n",
    "    def next(self):\n",
    "        row = self.child.next()  # Chamada virtual!\n",
    "        if row is None:\n",
    "            return None\n",
    "        # Processa UMA linha\n",
    "        return row[self.col_a] + row[self.col_b]\n",
    "    \n",
    "    def close(self):\n",
    "        self.child.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo Volcano\n",
    "n_rows = 100_000\n",
    "data = [(i, i*2) for i in range(n_rows)]  # Lista de tuplas (a, b)\n",
    "\n",
    "def volcano_execution(data):\n",
    "    \"\"\"Executa consulta SELECT a + b usando modelo Volcano\"\"\"\n",
    "    scan = VolcanoIterator(data)\n",
    "    sum_op = VolcanoSum(scan, 0, 1)\n",
    "    \n",
    "    sum_op.open()\n",
    "    results = []\n",
    "    while True:\n",
    "        result = sum_op.next()  # Uma chamada por linha!\n",
    "        if result is None:\n",
    "            break\n",
    "        results.append(result)\n",
    "    sum_op.close()\n",
    "    return results\n",
    "\n",
    "# Medindo tempo\n",
    "start = time.perf_counter()\n",
    "results_volcano = volcano_execution(data)\n",
    "volcano_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Modelo Volcano: {volcano_time*1000:.2f} ms para {n_rows:,} linhas\")\n",
    "print(f\"Chamadas de função next(): {n_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 O Modelo Vetorizado\n",
    "\n",
    "No modelo vetorizado, os dados são processados em **blocos** (vetores) de tamanho fixo (tipicamente 1024-2048 linhas).\n",
    "\n",
    "### Vantagens:\n",
    "1. **Menos chamadas de função**: Uma chamada processa milhares de linhas\n",
    "2. **Cache friendly**: Dados sequenciais ficam no cache L1\n",
    "3. **SIMD ready**: Operações podem usar instruções vetoriais\n",
    "4. **Melhor branch prediction**: Loops internos são previsíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedOperator:\n",
    "    \"\"\"\n",
    "    Simulação do modelo vetorizado - processa blocos de dados\n",
    "    \"\"\"\n",
    "    VECTOR_SIZE = 1024  # Tamanho típico do DuckDB\n",
    "    \n",
    "    def __init__(self, data_a, data_b):\n",
    "        self.data_a = np.array(data_a)\n",
    "        self.data_b = np.array(data_b)\n",
    "        self.offset = 0\n",
    "    \n",
    "    def next_chunk(self):\n",
    "        \"\"\"\n",
    "        Retorna próximo chunk de dados (vetor de até 1024 elementos)\n",
    "        Uma única chamada processa MUITAS linhas!\n",
    "        \"\"\"\n",
    "        if self.offset >= len(self.data_a):\n",
    "            return None\n",
    "        \n",
    "        end = min(self.offset + self.VECTOR_SIZE, len(self.data_a))\n",
    "        chunk_a = self.data_a[self.offset:end]\n",
    "        chunk_b = self.data_b[self.offset:end]\n",
    "        self.offset = end\n",
    "        \n",
    "        # Operação vetorizada - SIMD friendly!\n",
    "        # O compilador/NumPy traduz isso para instruções SIMD\n",
    "        return chunk_a + chunk_b\n",
    "\n",
    "\n",
    "def vectorized_execution(data_a, data_b):\n",
    "    \"\"\"Executa consulta SELECT a + b usando modelo vetorizado\"\"\"\n",
    "    op = VectorizedOperator(data_a, data_b)\n",
    "    \n",
    "    results = []\n",
    "    while True:\n",
    "        chunk = op.next_chunk()  # Uma chamada por bloco de 1024!\n",
    "        if chunk is None:\n",
    "            break\n",
    "        results.append(chunk)\n",
    "    \n",
    "    return np.concatenate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando dados para modelo vetorizado\n",
    "data_a = [row[0] for row in data]\n",
    "data_b = [row[1] for row in data]\n",
    "\n",
    "# Medindo tempo do modelo vetorizado\n",
    "start = time.perf_counter()\n",
    "results_vectorized = vectorized_execution(data_a, data_b)\n",
    "vectorized_time = time.perf_counter() - start\n",
    "\n",
    "n_chunks = (n_rows + 1023) // 1024\n",
    "print(f\"Modelo Vetorizado: {vectorized_time*1000:.2f} ms para {n_rows:,} linhas\")\n",
    "print(f\"Chamadas de função next_chunk(): {n_chunks}\")\n",
    "print(f\"\\nSpeedup: {volcano_time/vectorized_time:.1f}x mais rápido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Comparação Visual: Volcano vs Vetorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark com diferentes tamanhos de dados\n",
    "sizes = [1_000, 10_000, 50_000, 100_000, 500_000, 1_000_000]\n",
    "volcano_times = []\n",
    "vectorized_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    # Gerar dados\n",
    "    test_data = [(i, i*2) for i in range(size)]\n",
    "    test_a = [row[0] for row in test_data]\n",
    "    test_b = [row[1] for row in test_data]\n",
    "    \n",
    "    # Volcano\n",
    "    start = time.perf_counter()\n",
    "    _ = volcano_execution(test_data)\n",
    "    volcano_times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    # Vetorizado\n",
    "    start = time.perf_counter()\n",
    "    _ = vectorized_execution(test_a, test_b)\n",
    "    vectorized_times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    print(f\"Size: {size:>10,} | Volcano: {volcano_times[-1]:>8.2f}ms | Vectorized: {vectorized_times[-1]:>8.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico de tempo absoluto\n",
    "ax1 = axes[0]\n",
    "x = range(len(sizes))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar([i - width/2 for i in x], volcano_times, width, label='Volcano', color='#e74c3c')\n",
    "bars2 = ax1.bar([i + width/2 for i in x], vectorized_times, width, label='Vetorizado', color='#27ae60')\n",
    "\n",
    "ax1.set_xlabel('Número de Linhas')\n",
    "ax1.set_ylabel('Tempo (ms)')\n",
    "ax1.set_title('Tempo de Execução: Volcano vs Vetorizado')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'{s:,}' for s in sizes], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Gráfico de speedup\n",
    "ax2 = axes[1]\n",
    "speedups = [v/vec for v, vec in zip(volcano_times, vectorized_times)]\n",
    "ax2.bar(x, speedups, color='#3498db')\n",
    "ax2.set_xlabel('Número de Linhas')\n",
    "ax2.set_ylabel('Speedup (x vezes mais rápido)')\n",
    "ax2.set_title('Speedup do Modelo Vetorizado')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f'{s:,}' for s in sizes], rotation=45)\n",
    "ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 DuckDB em Ação: Execução Vetorizada Real\n",
    "\n",
    "Agora vamos ver o DuckDB executando consultas reais e comparar com processamento linha a linha em Python/Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma tabela grande no DuckDB\n",
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "# Gerar dados de teste\n",
    "n = 10_000_000  # 10 milhões de linhas\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE test AS\n",
    "    SELECT \n",
    "        i AS id,\n",
    "        random() * 1000 AS value_a,\n",
    "        random() * 1000 AS value_b,\n",
    "        CASE WHEN random() > 0.5 THEN 'A' ELSE 'B' END AS category\n",
    "    FROM generate_series(1, {n}) AS t(i)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Tabela criada com {n:,} linhas\")\n",
    "con.execute(\"SELECT * FROM test LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta: SELECT value_a + value_b FROM test\n",
    "# Comparando DuckDB vs Pandas\n",
    "\n",
    "# DuckDB (vetorizado)\n",
    "start = time.perf_counter()\n",
    "result_duck = con.execute(\"SELECT value_a + value_b AS soma FROM test\").df()\n",
    "duckdb_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"DuckDB (Vetorizado): {duckdb_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas (também vetorizado via NumPy, mas com overhead de DataFrame)\n",
    "df = con.execute(\"SELECT * FROM test\").df()\n",
    "\n",
    "start = time.perf_counter()\n",
    "result_pandas = df['value_a'] + df['value_b']\n",
    "pandas_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Pandas: {pandas_time*1000:.2f} ms\")\n",
    "print(f\"\\nDuckDB é {pandas_time/duckdb_time:.1f}x mais rápido que Pandas para esta operação\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulação de processamento linha a linha (estilo Volcano em Python)\n",
    "def row_by_row_sum(df):\n",
    "    \"\"\"Simula processamento linha por linha - MUITO lento!\"\"\"\n",
    "    results = []\n",
    "    for idx in range(len(df)):\n",
    "        results.append(df.iloc[idx]['value_a'] + df.iloc[idx]['value_b'])\n",
    "    return results\n",
    "\n",
    "# Testar apenas com subset (seria muito lento com 10M linhas)\n",
    "df_small = df.head(100_000)\n",
    "\n",
    "start = time.perf_counter()\n",
    "_ = row_by_row_sum(df_small)\n",
    "row_time = time.perf_counter() - start\n",
    "\n",
    "# Tempo vetorizado para mesma quantidade\n",
    "start = time.perf_counter()\n",
    "_ = df_small['value_a'] + df_small['value_b']\n",
    "vec_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Linha a linha (100k rows): {row_time*1000:.2f} ms\")\n",
    "print(f\"Vetorizado (100k rows): {vec_time*1000:.4f} ms\")\n",
    "print(f\"\\nVetorização é {row_time/vec_time:.0f}x mais rápida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Visualizando o Plano de Execução do DuckDB\n",
    "\n",
    "O DuckDB permite visualizar como ele processa consultas internamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver o plano de execução\n",
    "print(\"=== EXPLAIN ===\\n\")\n",
    "plan = con.execute(\"EXPLAIN SELECT value_a + value_b FROM test WHERE category = 'A'\").df()\n",
    "print(plan.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise detalhada com EXPLAIN ANALYZE\n",
    "print(\"=== EXPLAIN ANALYZE ===\\n\")\n",
    "analyze = con.execute(\"EXPLAIN ANALYZE SELECT SUM(value_a + value_b) FROM test WHERE category = 'A'\").df()\n",
    "print(analyze.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Por Que Vetorização Funciona Tão Bem?\n",
    "\n",
    "### Diagrama conceitual do cache da CPU:\n",
    "\n",
    "```\n",
    "+-------------------------------------------------------------+\n",
    "|                         CPU                                  |\n",
    "|  +-----------------------------------------------------+   |\n",
    "|  |  Registradores (< 1ns)                               |   |\n",
    "|  |  +-----------------------------------------------------+ |\n",
    "|  |  |  Cache L1 (32KB, ~1ns) <-- Dados do Chunk       |   |\n",
    "|  |  +-----------------------------------------------------+ |\n",
    "|  |  +-----------------------------------------------------+ |\n",
    "|  |  |  Cache L2 (256KB, ~3ns)                         |   |\n",
    "|  |  +-----------------------------------------------------+ |\n",
    "|  +-----------------------------------------------------+   |\n",
    "|  +-----------------------------------------------------+   |\n",
    "|  |  Cache L3 (8MB, ~10ns)                               |   |\n",
    "|  +-----------------------------------------------------+   |\n",
    "+-------------------------------------------------------------+\n",
    "                          |\n",
    "                          v (~100ns latência!)\n",
    "+-------------------------------------------------------------+\n",
    "|                     RAM Principal                            |\n",
    "+-------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "**O segredo**: Um chunk de 1024-2048 linhas com poucos campos cabe inteiramente no Cache L1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração: tamanho de um chunk típico\n",
    "chunk_size = 2048\n",
    "bytes_per_double = 8  # float64\n",
    "num_columns = 3\n",
    "\n",
    "chunk_memory = chunk_size * bytes_per_double * num_columns\n",
    "l1_cache_size = 32 * 1024  # 32 KB típico\n",
    "\n",
    "print(f\"Tamanho de um chunk (2048 linhas x 3 colunas float64):\")\n",
    "print(f\"  {chunk_memory:,} bytes ({chunk_memory/1024:.1f} KB)\")\n",
    "print(f\"\\nCache L1 típico: {l1_cache_size:,} bytes ({l1_cache_size/1024:.0f} KB)\")\n",
    "print(f\"\\n O chunk cabe no L1? {'Sim!' if chunk_memory < l1_cache_size else 'Não'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Exercícios Práticos\n",
    "\n",
    "### Exercício 1: Implemente um operador de filtro no modelo Volcano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implemente a classe VolcanoFilter que filtra linhas onde coluna > threshold\n",
    "class VolcanoFilter(VolcanoIterator):\n",
    "    def __init__(self, child, column_idx, threshold):\n",
    "        self.child = child\n",
    "        self.column_idx = column_idx\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def open(self):\n",
    "        self.child.open()\n",
    "    \n",
    "    def next(self):\n",
    "        # Sua implementação aqui\n",
    "        while True:\n",
    "            row = self.child.next()\n",
    "            if row is None:\n",
    "                return None\n",
    "            if row[self.column_idx] > self.threshold:\n",
    "                return row\n",
    "    \n",
    "    def close(self):\n",
    "        self.child.close()\n",
    "\n",
    "# Teste\n",
    "test_data = [(i, i*2) for i in range(10)]\n",
    "scan = VolcanoIterator(test_data)\n",
    "filter_op = VolcanoFilter(scan, 0, 5)  # Filtrar onde col 0 > 5\n",
    "\n",
    "filter_op.open()\n",
    "results = []\n",
    "while (row := filter_op.next()) is not None:\n",
    "    results.append(row)\n",
    "filter_op.close()\n",
    "\n",
    "print(\"Resultado do filtro (coluna 0 > 5):\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: Compare o custo de chamadas de função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o overhead puro de chamadas de função\n",
    "def empty_function():\n",
    "    pass\n",
    "\n",
    "n_calls = 1_000_000\n",
    "\n",
    "# Com chamadas de função\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_calls):\n",
    "    empty_function()\n",
    "with_calls = time.perf_counter() - start\n",
    "\n",
    "# Sem chamadas (loop vazio)\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_calls):\n",
    "    pass\n",
    "without_calls = time.perf_counter() - start\n",
    "\n",
    "overhead = (with_calls - without_calls) * 1e9 / n_calls  # nanosegundos por chamada\n",
    "\n",
    "print(f\"Overhead por chamada de função: ~{overhead:.1f} ns\")\n",
    "print(f\"Em 1 milhão de linhas: ~{overhead * n_calls / 1e6:.1f} ms de overhead puro!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Resumo do Capítulo\n",
    "\n",
    "| Aspecto | Modelo Volcano | Modelo Vetorizado |\n",
    "|---------|---------------|-------------------|\n",
    "| Unidade de processamento | 1 linha | 1024-2048 linhas |\n",
    "| Chamadas de função | N (uma por linha) | N/1024 |\n",
    "| Cache efficiency | Baixa | Alta |\n",
    "| SIMD | Não utiliza | Utiliza |\n",
    "| Branch prediction | Ruim | Excelente |\n",
    "| Complexidade | Simples | Moderada |\n",
    "\n",
    "### Principais takeaways:\n",
    "1. O modelo Volcano é elegante mas ineficiente para CPUs modernas\n",
    "2. Vetorização reduz drasticamente o overhead de chamadas de função\n",
    "3. Processar dados em chunks permite que eles fiquem no cache L1\n",
    "4. O DuckDB escolhe tamanhos de chunk otimizados para o cache da CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar recursos\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
