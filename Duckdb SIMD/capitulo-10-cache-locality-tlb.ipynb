{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 10: Cache Locality e TLB\n",
    "\n",
    "## Introdução\n",
    "\n",
    "O desempenho de sistemas de banco de dados modernos é frequentemente limitado pela **hierarquia de memória**, não pela CPU. Entender como caches L1/L2/L3 e TLB (Translation Lookaside Buffer) funcionam é essencial para otimizar consultas.\n",
    "\n",
    "### Objetivos:\n",
    "1. Entender a hierarquia de cache (L1, L2, L3)\n",
    "2. Por que o DuckDB usa chunks de ~2048 linhas\n",
    "3. Medir impacto de cache misses\n",
    "4. Otimizar acesso à memória para localidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb pandas numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Hierarquia de Memória\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     HIERARQUIA DE CACHE                     │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│   Registradores  │  ~1 ciclo   │  ~1 KB    │  Mais rápido  │\n",
    "│        ↓         │             │           │               │\n",
    "│   Cache L1       │  ~4 ciclos  │  32-48 KB │               │\n",
    "│        ↓         │             │           │               │\n",
    "│   Cache L2       │  ~12 ciclos │  256 KB   │               │\n",
    "│        ↓         │             │           │               │\n",
    "│   Cache L3       │  ~40 ciclos │  8-32 MB  │               │\n",
    "│        ↓         │             │           │               │\n",
    "│   RAM (DRAM)     │  ~200 ciclos│  GB-TB    │  Mais lento   │\n",
    "│        ↓         │             │           │               │\n",
    "│   Disco/SSD      │  ~100K ciclos│  TB      │               │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Latência L1 vs RAM: ~50x diferença!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulação de latências de cache\n",
    "cache_levels = {\n",
    "    'L1': {'size_kb': 32, 'latency_cycles': 4, 'latency_ns': 1},\n",
    "    'L2': {'size_kb': 256, 'latency_cycles': 12, 'latency_ns': 3},\n",
    "    'L3': {'size_kb': 8192, 'latency_cycles': 40, 'latency_ns': 10},\n",
    "    'RAM': {'size_kb': 16*1024*1024, 'latency_cycles': 200, 'latency_ns': 50},\n",
    "}\n",
    "\n",
    "print(\"Hierarquia de Cache (valores típicos):\")\n",
    "print(f\"{'Nível':<6} {'Tamanho':<12} {'Latência (ciclos)':<20} {'Latência (ns)'}\")\n",
    "print(\"-\" * 55)\n",
    "for level, info in cache_levels.items():\n",
    "    size_str = f\"{info['size_kb']} KB\" if info['size_kb'] < 1024 else f\"{info['size_kb']//1024} MB\"\n",
    "    if info['size_kb'] >= 1024*1024:\n",
    "        size_str = f\"{info['size_kb']//1024//1024} GB\"\n",
    "    print(f\"{level:<6} {size_str:<12} {info['latency_cycles']:<20} {info['latency_ns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 O \"Número Mágico\" 2048\n",
    "\n",
    "Por que o DuckDB processa blocos de ~2048 linhas?\n",
    "\n",
    "```\n",
    "Cálculo:\n",
    "  - 3 colunas de int32 (4 bytes) × 2048 linhas = 24 KB\n",
    "  - Cache L1 típico = 32-48 KB\n",
    "  - Sobra espaço para vetores auxiliares!\n",
    "\n",
    "Se usarmos 100.000 linhas:\n",
    "  - 3 colunas × 100.000 × 4 bytes = 1.2 MB\n",
    "  - Não cabe nem no L2!\n",
    "  - Resultado: Cache thrashing, CPU esperando dados\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração: Por que 2048?\n",
    "\n",
    "def calculate_chunk_size(num_columns: int, bytes_per_element: int, target_cache_kb: int = 32):\n",
    "    \"\"\"Calcula tamanho ideal do chunk para caber no cache L1\"\"\"\n",
    "    target_bytes = target_cache_kb * 1024\n",
    "    # Deixar 25% de folga para vetores auxiliares\n",
    "    usable_bytes = target_bytes * 0.75\n",
    "    bytes_per_row = num_columns * bytes_per_element\n",
    "    return int(usable_bytes / bytes_per_row)\n",
    "\n",
    "# Cenários comuns\n",
    "scenarios = [\n",
    "    (1, 4, \"1 coluna int32\"),\n",
    "    (3, 4, \"3 colunas int32\"),\n",
    "    (5, 4, \"5 colunas int32\"),\n",
    "    (3, 8, \"3 colunas int64/double\"),\n",
    "    (10, 4, \"10 colunas int32\"),\n",
    "]\n",
    "\n",
    "print(\"Tamanho ideal de chunk para L1 (32 KB):\")\n",
    "print(f\"{'Cenário':<25} {'Chunk Size':<12} {'Memória (KB)'}\")\n",
    "print(\"-\" * 50)\n",
    "for cols, bytes_per, desc in scenarios:\n",
    "    chunk = calculate_chunk_size(cols, bytes_per)\n",
    "    mem = chunk * cols * bytes_per / 1024\n",
    "    print(f\"{desc:<25} {chunk:<12} {mem:.1f}\")\n",
    "\n",
    "print(f\"\\n→ DuckDB usa VECTOR_SIZE = 2048 como padrão\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Benchmark: Impacto do Tamanho do Bloco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_chunks(data: np.ndarray, chunk_size: int, operation):\n",
    "    \"\"\"Processa dados em chunks do tamanho especificado\"\"\"\n",
    "    n = len(data)\n",
    "    result = 0\n",
    "    \n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = data[start:end]\n",
    "        result += operation(chunk)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Operação de teste: soma com transformação\n",
    "def complex_operation(arr):\n",
    "    return np.sum(arr * arr + arr * 2 + 1)\n",
    "\n",
    "# Benchmark com diferentes tamanhos de chunk\n",
    "n = 10_000_000\n",
    "data = np.random.randn(n).astype(np.float64)\n",
    "\n",
    "chunk_sizes = [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
    "results = []\n",
    "\n",
    "for cs in chunk_sizes:\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.perf_counter()\n",
    "        _ = process_in_chunks(data, cs, complex_operation)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    results.append({\n",
    "        'chunk_size': cs,\n",
    "        'time_ms': np.mean(times) * 1000,\n",
    "        'memory_kb': cs * 8 / 1024  # float64 = 8 bytes\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Tempo vs Chunk Size\n",
    "ax1.plot(df['chunk_size'], df['time_ms'], marker='o', linewidth=2)\n",
    "ax1.axvline(x=2048, color='red', linestyle='--', label='DuckDB default (2048)')\n",
    "ax1.axvspan(1024, 4096, alpha=0.2, color='green', label='Zona ótima')\n",
    "ax1.set_xlabel('Tamanho do Chunk')\n",
    "ax1.set_ylabel('Tempo (ms)')\n",
    "ax1.set_title('Tempo de Processamento vs Tamanho do Chunk')\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Memória do chunk vs níveis de cache\n",
    "ax2.bar(range(len(chunk_sizes)), df['memory_kb'], color='steelblue')\n",
    "ax2.axhline(y=32, color='red', linestyle='--', linewidth=2, label='L1 Cache (32 KB)')\n",
    "ax2.axhline(y=256, color='orange', linestyle='--', linewidth=2, label='L2 Cache (256 KB)')\n",
    "ax2.set_xticks(range(len(chunk_sizes)))\n",
    "ax2.set_xticklabels([str(cs) for cs in chunk_sizes], rotation=45)\n",
    "ax2.set_xlabel('Tamanho do Chunk')\n",
    "ax2.set_ylabel('Memória do Chunk (KB)')\n",
    "ax2.set_title('Memória por Chunk vs Níveis de Cache')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Localidade Espacial vs Temporal\n",
    "\n",
    "```\n",
    "LOCALIDADE ESPACIAL:\n",
    "  Acessar dados próximos na memória.\n",
    "  Arrays são ótimos! A[i] e A[i+1] estão lado a lado.\n",
    "  \n",
    "  BOM:  for i in range(n): sum += arr[i]     ← Acesso sequencial\n",
    "  RUIM: for i in range(n): sum += arr[random]  ← Acesso aleatório\n",
    "\n",
    "LOCALIDADE TEMPORAL:\n",
    "  Reusar dados recentemente acessados.\n",
    "  Chunks pequenos permitem reusar dados no cache!\n",
    "  \n",
    "  DuckDB: Processa TODO o pipeline em um chunk antes de ir para o próximo.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração: Acesso sequencial vs aleatório\n",
    "\n",
    "n = 10_000_000\n",
    "data = np.random.randn(n)\n",
    "\n",
    "# Gerar índices aleatórios\n",
    "random_indices = np.random.permutation(n)\n",
    "\n",
    "# Acesso sequencial\n",
    "times_seq = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += data[i]\n",
    "    times_seq.append(time.perf_counter() - start)\n",
    "\n",
    "# Acesso aleatório\n",
    "times_random = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += data[random_indices[i]]\n",
    "    times_random.append(time.perf_counter() - start)\n",
    "\n",
    "# Vetorizado (NumPy) - para comparação\n",
    "times_vec = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    total = np.sum(data)\n",
    "    times_vec.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"Soma de {n:,} elementos:\")\n",
    "print(f\"  Sequencial:     {np.mean(times_seq)*1000:.1f} ms\")\n",
    "print(f\"  Aleatório:      {np.mean(times_random)*1000:.1f} ms\")\n",
    "print(f\"  Vetorizado:     {np.mean(times_vec)*1000:.2f} ms\")\n",
    "print(f\"\\n  Penalidade aleatório: {np.mean(times_random)/np.mean(times_seq):.1f}x mais lento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Cache Lines e Prefetching\n",
    "\n",
    "```\n",
    "CACHE LINE:\n",
    "  A CPU não carrega 1 byte por vez, carrega uma \"linha\" de 64 bytes.\n",
    "  \n",
    "  Exemplo com int32 (4 bytes):\n",
    "  ┌────────────────────────────────────────────┐\n",
    "  │ int[0] │ int[1] │ ... │ int[15] │  = 64 bytes = 1 cache line\n",
    "  └────────────────────────────────────────────┘\n",
    "  \n",
    "  Se você acessa int[0], automaticamente tem int[1] até int[15] no cache!\n",
    "\n",
    "PREFETCHING:\n",
    "  A CPU tenta prever quais dados você vai precisar.\n",
    "  Acesso sequencial = fácil de prever = prefetch eficiente\n",
    "  Acesso aleatório = impossível prever = muitos stalls\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração: Stride de acesso e cache lines\n",
    "\n",
    "def sum_with_stride(arr, stride):\n",
    "    \"\"\"Soma elementos pulando 'stride' posições\"\"\"\n",
    "    total = 0\n",
    "    for i in range(0, len(arr), stride):\n",
    "        total += arr[i]\n",
    "    return total\n",
    "\n",
    "n = 1_000_000\n",
    "data = np.random.randn(n)\n",
    "\n",
    "strides = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "results = []\n",
    "\n",
    "for stride in strides:\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.perf_counter()\n",
    "        _ = sum_with_stride(data, stride)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    # Normalizar pelo número de elementos acessados\n",
    "    elements = n // stride\n",
    "    time_per_element = np.mean(times) / elements * 1e9  # ns\n",
    "    \n",
    "    results.append({\n",
    "        'stride': stride,\n",
    "        'elements': elements,\n",
    "        'time_ms': np.mean(times) * 1000,\n",
    "        'ns_per_element': time_per_element\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"Impacto do Stride no acesso:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['stride'], df['ns_per_element'], marker='o', linewidth=2)\n",
    "plt.axvline(x=8, color='red', linestyle='--', label='8 doubles = 64 bytes (cache line)')\n",
    "plt.xlabel('Stride (elementos)')\n",
    "plt.ylabel('Tempo por Elemento (ns)')\n",
    "plt.title('Impacto do Stride no Desempenho\\n(Stride > 8 desperdiça cache lines)')\n",
    "plt.xscale('log', base=2)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 TLB (Translation Lookaside Buffer)\n",
    "\n",
    "```\n",
    "O QUE É TLB:\n",
    "  Cache de traduções de endereços virtuais → físicos.\n",
    "  Sem TLB, cada acesso à memória precisaria de 4+ acessos extras!\n",
    "\n",
    "TLB MISS:\n",
    "  Quando acessamos muitas páginas diferentes (tipicamente 4KB cada),\n",
    "  o TLB não consegue manter todas as traduções.\n",
    "  \n",
    "  Solução: Manter dados em poucas páginas = chunks pequenos!\n",
    "\n",
    "HUGE PAGES:\n",
    "  Páginas de 2MB em vez de 4KB = menos entradas no TLB necessárias.\n",
    "  DuckDB pode usar huge pages para estruturas grandes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração conceitual de TLB\n",
    "\n",
    "PAGE_SIZE = 4096  # 4 KB típico\n",
    "\n",
    "def calculate_pages_touched(array_size_bytes, access_pattern='sequential'):\n",
    "    \"\"\"Calcula quantas páginas de memória são tocadas\"\"\"\n",
    "    if access_pattern == 'sequential':\n",
    "        return (array_size_bytes + PAGE_SIZE - 1) // PAGE_SIZE\n",
    "    else:  # random\n",
    "        # No pior caso, cada acesso pode tocar uma página diferente\n",
    "        return array_size_bytes // 8  # Assumindo float64\n",
    "\n",
    "# Comparar chunks pequenos vs grandes\n",
    "scenarios = [\n",
    "    (2048, 3, 8, \"DuckDB: 2048 rows × 3 cols × 8 bytes\"),\n",
    "    (10000, 3, 8, \"Médio: 10K rows × 3 cols × 8 bytes\"),\n",
    "    (100000, 3, 8, \"Grande: 100K rows × 3 cols × 8 bytes\"),\n",
    "    (1000000, 3, 8, \"Enorme: 1M rows × 3 cols × 8 bytes\"),\n",
    "]\n",
    "\n",
    "print(\"Páginas de memória tocadas por cenário:\")\n",
    "print(f\"{'Cenário':<40} {'Tamanho (KB)':<15} {'Páginas (4KB)'}\")\n",
    "print(\"-\" * 70)\n",
    "for rows, cols, bytes_per, desc in scenarios:\n",
    "    size_bytes = rows * cols * bytes_per\n",
    "    pages = calculate_pages_touched(size_bytes)\n",
    "    print(f\"{desc:<40} {size_bytes/1024:<15.1f} {pages}\")\n",
    "\n",
    "print(f\"\\nTLB típico: ~64-1024 entradas\")\n",
    "print(f\"→ Chunks de 2048 linhas: poucas páginas, TLB hits!\")\n",
    "print(f\"→ Chunks de 1M linhas: muitas páginas, TLB thrashing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 DuckDB: Configurações de Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "# Ver configurações de memória\n",
    "print(\"Configurações de memória do DuckDB:\")\n",
    "settings = con.execute(\"\"\"\n",
    "    SELECT name, value, description \n",
    "    FROM duckdb_settings() \n",
    "    WHERE name LIKE '%memory%' OR name LIKE '%thread%' OR name LIKE '%buffer%'\n",
    "\"\"\").df()\n",
    "print(settings.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Impacto do paralelismo e memória\n",
    "\n",
    "# Criar tabela grande\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE benchmark AS\n",
    "    SELECT \n",
    "        i AS id,\n",
    "        random() * 100 AS val1,\n",
    "        random() * 100 AS val2,\n",
    "        random() * 100 AS val3\n",
    "    FROM generate_series(1, 50000000) AS t(i)\n",
    "\"\"\")\n",
    "print(\"Tabela criada com 50M linhas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark de operações\n",
    "queries = {\n",
    "    'Full Scan': 'SELECT COUNT(*) FROM benchmark',\n",
    "    'Agregação Simples': 'SELECT SUM(val1), AVG(val2) FROM benchmark',\n",
    "    'Agregação Complexa': 'SELECT SUM(val1 * val2 + val3) FROM benchmark',\n",
    "    'Com Filtro': 'SELECT SUM(val1) FROM benchmark WHERE val2 > 50',\n",
    "    'Group By': 'SELECT id % 1000 AS grp, SUM(val1) FROM benchmark GROUP BY grp',\n",
    "}\n",
    "\n",
    "print(\"Performance com 50M linhas:\\n\")\n",
    "for name, query in queries.items():\n",
    "    times = []\n",
    "    for _ in range(3):\n",
    "        start = time.perf_counter()\n",
    "        con.execute(query).fetchall()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    rows_per_sec = 50_000_000 / np.mean(times) / 1_000_000\n",
    "    print(f\"{name:25} {np.mean(times)*1000:8.1f} ms ({rows_per_sec:.1f}M rows/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver plano de execução com estatísticas\n",
    "print(\"\\n=== EXPLAIN ANALYZE ===\")\n",
    "plan = con.execute(\"\"\"\n",
    "    EXPLAIN ANALYZE\n",
    "    SELECT SUM(val1 * val2) \n",
    "    FROM benchmark \n",
    "    WHERE val3 > 50\n",
    "\"\"\").df()\n",
    "print(plan.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Resumo do Curso\n",
    "\n",
    "### Técnicas de Otimização do DuckDB:\n",
    "\n",
    "| Capítulo | Técnica | Benefício |\n",
    "|----------|---------|----------|\n",
    "| 1 | Execução Vetorizada | Elimina overhead de chamadas virtuais |\n",
    "| 2 | DataChunk | Estrutura cache-friendly de 2048 linhas |\n",
    "| 3 | Selection Vectors | Zero-copy filtering |\n",
    "| 4 | SIMD/Branchless | Processamento paralelo em registradores |\n",
    "| 5 | Avaliação de Expressões | Resultados intermediários no cache |\n",
    "| 6 | Hash Aggregation | Scatter/Gather vetorizado |\n",
    "| 7 | Joins Vetorizados | Prefetching esconde latência |\n",
    "| 8 | German Strings | Comparação rápida via prefixo inline |\n",
    "| 9 | Scanners Vetorizados | SIMD unpacking de dados compactados |\n",
    "| 10 | Cache Locality | Chunks dimensionados para L1 |\n",
    "\n",
    "### Princípios Gerais:\n",
    "\n",
    "1. **Processar em batches**: 2048 linhas por vez\n",
    "2. **Evitar branches**: Usar máscaras e operações SIMD\n",
    "3. **Manter dados no cache**: Chunks pequenos = L1 hits\n",
    "4. **Acesso sequencial**: Prefetching e cache lines\n",
    "5. **Evitar alocações**: Reusar buffers, zero-copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização final: Por que vetorização funciona\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Hierarquia de latência\n",
    "ax = axes[0, 0]\n",
    "levels = ['Registrador', 'L1 Cache', 'L2 Cache', 'L3 Cache', 'RAM']\n",
    "latencies = [1, 4, 12, 40, 200]\n",
    "colors = ['#27ae60', '#2ecc71', '#f1c40f', '#e67e22', '#e74c3c']\n",
    "ax.barh(levels, latencies, color=colors)\n",
    "ax.set_xlabel('Latência (ciclos)')\n",
    "ax.set_title('Latência por Nível de Memória')\n",
    "ax.set_xscale('log')\n",
    "for i, v in enumerate(latencies):\n",
    "    ax.text(v + 5, i, str(v), va='center')\n",
    "\n",
    "# 2. Throughput por técnica\n",
    "ax = axes[0, 1]\n",
    "techniques = ['Volcano\\n(linha a linha)', 'Vetorizado\\n(sem SIMD)', 'Vetorizado\\n(com SIMD)']\n",
    "throughput = [1, 10, 40]  # Valores relativos\n",
    "ax.bar(techniques, throughput, color=['#e74c3c', '#f39c12', '#27ae60'])\n",
    "ax.set_ylabel('Throughput Relativo')\n",
    "ax.set_title('Ganho de Performance por Técnica')\n",
    "ax.set_ylim(0, 50)\n",
    "\n",
    "# 3. Chunk size vs Cache\n",
    "ax = axes[1, 0]\n",
    "chunk_sizes = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "mem_usage = [cs * 3 * 8 / 1024 for cs in chunk_sizes]  # 3 cols × 8 bytes\n",
    "ax.bar(range(len(chunk_sizes)), mem_usage, color='steelblue')\n",
    "ax.axhline(y=32, color='red', linestyle='--', linewidth=2, label='L1 (32KB)')\n",
    "ax.axhline(y=256, color='orange', linestyle='--', linewidth=2, label='L2 (256KB)')\n",
    "ax.set_xticks(range(len(chunk_sizes)))\n",
    "ax.set_xticklabels(chunk_sizes)\n",
    "ax.set_xlabel('Chunk Size')\n",
    "ax.set_ylabel('Memória (KB)')\n",
    "ax.set_title('Memória por Chunk (3 colunas float64)')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Resumo de benefícios\n",
    "ax = axes[1, 1]\n",
    "benefits = ['Cache Hits', 'SIMD', 'Prefetch', 'Zero-Copy', 'Branchless']\n",
    "impact = [35, 30, 15, 12, 8]  # Contribuição relativa %\n",
    "ax.pie(impact, labels=benefits, autopct='%1.0f%%', startangle=90,\n",
    "       colors=['#3498db', '#2ecc71', '#f1c40f', '#9b59b6', '#e74c3c'])\n",
    "ax.set_title('Contribuição para Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('duckdb_optimization_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Curso completo! Agora você entende como o DuckDB alcança alta performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}