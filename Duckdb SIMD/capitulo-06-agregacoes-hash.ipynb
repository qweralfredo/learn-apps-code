{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 6: Agregações Hash em Paralelo\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Agregações como `GROUP BY` são operações fundamentais em SQL. O DuckDB usa **hash tables vetorizadas** e técnicas de **scatter/gather** para maximizar o throughput, processando múltiplas chaves simultaneamente.\n",
    "\n",
    "### Objetivos:\n",
    "1. Entender hash aggregation tradicional vs vetorizada\n",
    "2. Implementar técnicas de scatter/gather\n",
    "3. Otimizar para diferentes cardinalidades\n",
    "4. Entender particionamento para paralelismo\n",
    "5. Comparar performance com Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb pandas numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Hash Aggregation: Tradicional vs Vetorizada\n",
    "\n",
    "### Abordagem Tradicional (Volcano)\n",
    "```\n",
    "Para cada linha:\n",
    "  1. Calcular hash da chave        ← 1 hash por vez\n",
    "  2. Lookup na hash table          ← Cache miss provável\n",
    "  3. Atualizar agregado            ← Acesso aleatório\n",
    "\n",
    "Problemas: Muitos acessos aleatórios à memória!\n",
    "```\n",
    "\n",
    "### Abordagem Vetorizada (DuckDB)\n",
    "```\n",
    "Para cada chunk de 2048 linhas:\n",
    "  1. Calcular hashes de TODAS as chaves (SIMD)\n",
    "  2. Agrupar por bucket (scatter)\n",
    "  3. Processar cada bucket em sequência\n",
    "  4. Agregar resultados (gather)\n",
    "\n",
    "Vantagens:\n",
    "  - Hash de múltiplas chaves em paralelo\n",
    "  - Melhor uso de cache (dados agrupados)\n",
    "  - Prefetching possível\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação tradicional (linha por linha)\n",
    "def hash_aggregate_traditional(keys: np.ndarray, values: np.ndarray):\n",
    "    \"\"\"GROUP BY tradicional - uma linha por vez\"\"\"\n",
    "    aggregates = defaultdict(lambda: {'sum': 0.0, 'count': 0})\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        val = values[i]\n",
    "        aggregates[key]['sum'] += val\n",
    "        aggregates[key]['count'] += 1\n",
    "    \n",
    "    return dict(aggregates)\n",
    "\n",
    "# Teste\n",
    "n = 100_000\n",
    "num_groups = 100\n",
    "keys = np.random.randint(0, num_groups, n)\n",
    "values = np.random.randn(n)\n",
    "\n",
    "start = time.perf_counter()\n",
    "result = hash_aggregate_traditional(keys, values)\n",
    "trad_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Agregação tradicional: {trad_time*1000:.2f} ms\")\n",
    "print(f\"Grupos: {len(result)}\")\n",
    "print(f\"Exemplo grupo 0: sum={result[0]['sum']:.2f}, count={result[0]['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação vetorizada usando NumPy\n",
    "def hash_aggregate_vectorized(keys: np.ndarray, values: np.ndarray):\n",
    "    \"\"\"GROUP BY vetorizado usando operações NumPy\"\"\"\n",
    "    # Encontrar chaves únicas e mapear para índices\n",
    "    unique_keys, inverse_indices = np.unique(keys, return_inverse=True)\n",
    "    \n",
    "    # Agregar usando bincount (altamente otimizado, usa SIMD internamente)\n",
    "    sums = np.bincount(inverse_indices, weights=values)\n",
    "    counts = np.bincount(inverse_indices)\n",
    "    \n",
    "    return {\n",
    "        'keys': unique_keys,\n",
    "        'sums': sums,\n",
    "        'counts': counts,\n",
    "        'means': sums / counts\n",
    "    }\n",
    "\n",
    "start = time.perf_counter()\n",
    "result_vec = hash_aggregate_vectorized(keys, values)\n",
    "vec_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Agregação vetorizada: {vec_time*1000:.4f} ms\")\n",
    "print(f\"Speedup: {trad_time/vec_time:.0f}x\")\n",
    "print(f\"\\nExemplo grupo 0: sum={result_vec['sums'][0]:.2f}, count={result_vec['counts'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Scatter/Gather: Como Funciona\n",
    "\n",
    "```\n",
    "SCATTER (Espalhar):\n",
    "  Dados:     [A, B, C, A, B, A]    (valores)\n",
    "  Hashes:    [0, 1, 2, 0, 1, 0]    (buckets)\n",
    "  \n",
    "  Bucket 0:  [A, A, A]  ← Todos os 'A' juntos\n",
    "  Bucket 1:  [B, B]\n",
    "  Bucket 2:  [C]\n",
    "\n",
    "GATHER (Coletar):\n",
    "  Agrega cada bucket separadamente\n",
    "  Bucket 0: SUM = 3*A\n",
    "  Bucket 1: SUM = 2*B\n",
    "  Bucket 2: SUM = 1*C\n",
    "\n",
    "Benefício: Dados no mesmo bucket ficam próximos → Cache hits!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração visual de Scatter/Gather\n",
    "def scatter_gather_demo(keys, values):\n",
    "    \"\"\"Demonstra o processo de scatter/gather\"\"\"\n",
    "    print(\"=== SCATTER ===\")\n",
    "    print(f\"Chaves:  {keys}\")\n",
    "    print(f\"Valores: {values}\")\n",
    "    \n",
    "    # Scatter: agrupar por chave\n",
    "    buckets = defaultdict(list)\n",
    "    for k, v in zip(keys, values):\n",
    "        buckets[k].append(v)\n",
    "    \n",
    "    print(\"\\nBuckets após scatter:\")\n",
    "    for k in sorted(buckets.keys()):\n",
    "        print(f\"  Bucket {k}: {buckets[k]}\")\n",
    "    \n",
    "    # Gather: agregar cada bucket\n",
    "    print(\"\\n=== GATHER ===\")\n",
    "    results = {}\n",
    "    for k, vals in buckets.items():\n",
    "        results[k] = {'sum': sum(vals), 'count': len(vals)}\n",
    "        print(f\"  Grupo {k}: SUM={sum(vals):.1f}, COUNT={len(vals)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Exemplo pequeno\n",
    "demo_keys = np.array([0, 1, 2, 0, 1, 0, 2, 1])\n",
    "demo_values = np.array([10, 20, 30, 15, 25, 12, 35, 22])\n",
    "_ = scatter_gather_demo(demo_keys, demo_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de agregação com scatter/gather explícito\n",
    "def hash_aggregate_scatter_gather(keys: np.ndarray, values: np.ndarray, num_partitions: int = 16):\n",
    "    \"\"\"\n",
    "    Agregação usando scatter/gather com partições.\n",
    "    Simula como DuckDB particiona para paralelismo.\n",
    "    \"\"\"\n",
    "    n = len(keys)\n",
    "    \n",
    "    # Fase 1: Calcular hashes para particionamento\n",
    "    partition_ids = keys % num_partitions\n",
    "    \n",
    "    # Fase 2: Scatter - separar por partição\n",
    "    partitions = []\n",
    "    for p in range(num_partitions):\n",
    "        mask = partition_ids == p\n",
    "        partitions.append({\n",
    "            'keys': keys[mask],\n",
    "            'values': values[mask]\n",
    "        })\n",
    "    \n",
    "    # Fase 3: Agregar cada partição (pode ser paralelo!)\n",
    "    partial_results = []\n",
    "    for p in partitions:\n",
    "        if len(p['keys']) > 0:\n",
    "            unique, inverse = np.unique(p['keys'], return_inverse=True)\n",
    "            sums = np.bincount(inverse, weights=p['values'])\n",
    "            counts = np.bincount(inverse)\n",
    "            partial_results.append({\n",
    "                'keys': unique,\n",
    "                'sums': sums,\n",
    "                'counts': counts\n",
    "            })\n",
    "    \n",
    "    # Fase 4: Gather - combinar resultados\n",
    "    final_sums = defaultdict(float)\n",
    "    final_counts = defaultdict(int)\n",
    "    \n",
    "    for pr in partial_results:\n",
    "        for i, key in enumerate(pr['keys']):\n",
    "            final_sums[key] += pr['sums'][i]\n",
    "            final_counts[key] += pr['counts'][i]\n",
    "    \n",
    "    return dict(final_sums), dict(final_counts)\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "sums, counts = hash_aggregate_scatter_gather(keys, values)\n",
    "sg_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Scatter/Gather: {sg_time*1000:.2f} ms\")\n",
    "print(f\"Grupos: {len(sums)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Impacto da Cardinalidade\n",
    "\n",
    "A **cardinalidade** (número de grupos distintos) afeta drasticamente a performance:\n",
    "\n",
    "```\n",
    "Baixa cardinalidade (poucos grupos):\n",
    "  - Hash table pequena → cabe no cache L1\n",
    "  - Muitas colisões → mais processamento por bucket\n",
    "  - Muito rápido!\n",
    "\n",
    "Alta cardinalidade (muitos grupos):\n",
    "  - Hash table grande → cache misses\n",
    "  - Poucas colisões → menos trabalho por bucket\n",
    "  - Mais lento devido a memória\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Impacto da cardinalidade\n",
    "n = 1_000_000\n",
    "cardinalities = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "results_card = []\n",
    "for card in cardinalities:\n",
    "    keys = np.random.randint(0, card, n)\n",
    "    values = np.random.randn(n)\n",
    "    \n",
    "    # Vetorizado\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.perf_counter()\n",
    "        _ = hash_aggregate_vectorized(keys, values)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000\n",
    "    rows_per_ms = n / avg_time / 1000\n",
    "    \n",
    "    results_card.append({\n",
    "        'cardinality': card,\n",
    "        'time_ms': avg_time,\n",
    "        'rows_per_ms': rows_per_ms\n",
    "    })\n",
    "    print(f\"Cardinalidade {card:>6}: {avg_time:6.2f} ms ({rows_per_ms:.0f}K rows/ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "df_card = pd.DataFrame(results_card)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Tempo vs Cardinalidade\n",
    "ax1.plot(df_card['cardinality'], df_card['time_ms'], marker='o', linewidth=2)\n",
    "ax1.set_xlabel('Número de Grupos (Cardinalidade)')\n",
    "ax1.set_ylabel('Tempo (ms)')\n",
    "ax1.set_title('Tempo de Agregação vs Cardinalidade')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput vs Cardinalidade\n",
    "ax2.bar(range(len(cardinalities)), df_card['rows_per_ms'], color='steelblue')\n",
    "ax2.set_xticks(range(len(cardinalities)))\n",
    "ax2.set_xticklabels([str(c) for c in cardinalities], rotation=45)\n",
    "ax2.set_xlabel('Cardinalidade')\n",
    "ax2.set_ylabel('Throughput (K rows/ms)')\n",
    "ax2.set_title('Throughput vs Cardinalidade')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Agregação por Chunks\n",
    "\n",
    "Para datasets grandes, processar em chunks mantém dados no cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_aggregate_chunked(keys: np.ndarray, values: np.ndarray, chunk_size: int = 2048):\n",
    "    \"\"\"\n",
    "    Agregação processando em chunks de tamanho fixo.\n",
    "    Simula como DuckDB processa DataChunks.\n",
    "    \"\"\"\n",
    "    n = len(keys)\n",
    "    partial_sums = defaultdict(float)\n",
    "    partial_counts = defaultdict(int)\n",
    "    \n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk_keys = keys[start:end]\n",
    "        chunk_values = values[start:end]\n",
    "        \n",
    "        # Agregar chunk (cabe no cache L1!)\n",
    "        unique, inverse = np.unique(chunk_keys, return_inverse=True)\n",
    "        sums = np.bincount(inverse, weights=chunk_values)\n",
    "        counts = np.bincount(inverse)\n",
    "        \n",
    "        # Merge com resultados parciais\n",
    "        for i, key in enumerate(unique):\n",
    "            partial_sums[key] += sums[i]\n",
    "            partial_counts[key] += counts[i]\n",
    "    \n",
    "    return dict(partial_sums), dict(partial_counts)\n",
    "\n",
    "# Benchmark com diferentes chunk sizes\n",
    "n = 5_000_000\n",
    "keys = np.random.randint(0, 1000, n)\n",
    "values = np.random.randn(n)\n",
    "\n",
    "chunk_sizes = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "chunk_results = []\n",
    "\n",
    "for cs in chunk_sizes:\n",
    "    times = []\n",
    "    for _ in range(3):\n",
    "        start = time.perf_counter()\n",
    "        _ = hash_aggregate_chunked(keys, values, cs)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000\n",
    "    chunk_results.append({'chunk_size': cs, 'time_ms': avg_time})\n",
    "    print(f\"Chunk size {cs:>6}: {avg_time:6.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 DuckDB: Agregações em Ação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "# Criar tabela de vendas simulada\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE vendas AS\n",
    "    SELECT \n",
    "        (random() * 1000)::INTEGER AS produto_id,\n",
    "        (random() * 100)::INTEGER AS loja_id,\n",
    "        (random() * 10)::INTEGER AS regiao_id,\n",
    "        random() * 1000 AS valor,\n",
    "        (random() * 10)::INTEGER AS quantidade,\n",
    "        DATE '2024-01-01' + (random() * 365)::INTEGER AS data_venda\n",
    "    FROM generate_series(1, 10000000)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabela criada com 10M linhas\")\n",
    "con.execute(\"SELECT * FROM vendas LIMIT 3\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark de diferentes tipos de agregação\n",
    "queries = {\n",
    "    'COUNT simples': 'SELECT COUNT(*) FROM vendas',\n",
    "    'SUM simples': 'SELECT SUM(valor) FROM vendas',\n",
    "    'Múltiplas agregações': 'SELECT SUM(valor), AVG(valor), MIN(valor), MAX(valor), COUNT(*) FROM vendas',\n",
    "    'GROUP BY baixa card (10)': 'SELECT regiao_id, SUM(valor) FROM vendas GROUP BY regiao_id',\n",
    "    'GROUP BY média card (100)': 'SELECT loja_id, SUM(valor) FROM vendas GROUP BY loja_id',\n",
    "    'GROUP BY alta card (1000)': 'SELECT produto_id, SUM(valor) FROM vendas GROUP BY produto_id',\n",
    "    'GROUP BY composto': 'SELECT produto_id, loja_id, SUM(valor) FROM vendas GROUP BY produto_id, loja_id',\n",
    "    'GROUP BY com filtro': 'SELECT loja_id, SUM(valor) FROM vendas WHERE valor > 500 GROUP BY loja_id',\n",
    "    'GROUP BY com HAVING': 'SELECT loja_id, SUM(valor) as total FROM vendas GROUP BY loja_id HAVING SUM(valor) > 50000000',\n",
    "}\n",
    "\n",
    "print(\"Performance de Agregações DuckDB:\\n\")\n",
    "agg_results = []\n",
    "for name, query in queries.items():\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.perf_counter()\n",
    "        result = con.execute(query).fetchall()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000\n",
    "    agg_results.append({'query': name, 'time_ms': avg_time})\n",
    "    print(f\"{name:35} {avg_time:8.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "df_agg = pd.DataFrame(agg_results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(df_agg['query'], df_agg['time_ms'], color='steelblue')\n",
    "plt.xlabel('Tempo (ms)')\n",
    "plt.title('Performance de Diferentes Agregações no DuckDB (10M linhas)')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, df_agg['time_ms']):\n",
    "    plt.text(val + 1, bar.get_y() + bar.get_height()/2, f'{val:.1f}ms', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver plano de execução\n",
    "print(\"=== EXPLAIN ANALYZE ===\")\n",
    "plan = con.execute(\"\"\"\n",
    "    EXPLAIN ANALYZE\n",
    "    SELECT produto_id, loja_id, SUM(valor), COUNT(*)\n",
    "    FROM vendas\n",
    "    GROUP BY produto_id, loja_id\n",
    "\"\"\").df()\n",
    "print(plan.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Comparação: DuckDB vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame Pandas equivalente\n",
    "df_pandas = con.execute(\"SELECT produto_id, loja_id, valor FROM vendas\").df()\n",
    "print(f\"DataFrame: {len(df_pandas):,} linhas\")\n",
    "\n",
    "# Benchmark: GROUP BY com múltiplas agregações\n",
    "# DuckDB\n",
    "times_duck = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    con.execute(\"\"\"\n",
    "        SELECT loja_id, SUM(valor), AVG(valor), COUNT(*)\n",
    "        FROM vendas GROUP BY loja_id\n",
    "    \"\"\").fetchall()\n",
    "    times_duck.append(time.perf_counter() - start)\n",
    "\n",
    "# Pandas\n",
    "times_pandas = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    df_pandas.groupby('loja_id')['valor'].agg(['sum', 'mean', 'count'])\n",
    "    times_pandas.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"\\nGROUP BY loja_id com SUM, AVG, COUNT:\")\n",
    "print(f\"DuckDB: {np.mean(times_duck)*1000:.2f} ms\")\n",
    "print(f\"Pandas: {np.mean(times_pandas)*1000:.2f} ms\")\n",
    "print(f\"Speedup: {np.mean(times_pandas)/np.mean(times_duck):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: GROUP BY composto\n",
    "# DuckDB\n",
    "times_duck2 = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    con.execute(\"\"\"\n",
    "        SELECT produto_id, loja_id, SUM(valor)\n",
    "        FROM vendas GROUP BY produto_id, loja_id\n",
    "    \"\"\").fetchall()\n",
    "    times_duck2.append(time.perf_counter() - start)\n",
    "\n",
    "# Pandas\n",
    "times_pandas2 = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    df_pandas.groupby(['produto_id', 'loja_id'])['valor'].sum()\n",
    "    times_pandas2.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"\\nGROUP BY produto_id, loja_id com SUM:\")\n",
    "print(f\"DuckDB: {np.mean(times_duck2)*1000:.2f} ms\")\n",
    "print(f\"Pandas: {np.mean(times_pandas2)*1000:.2f} ms\")\n",
    "print(f\"Speedup: {np.mean(times_pandas2)/np.mean(times_duck2):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Técnicas Avançadas\n",
    "\n",
    "### Pre-Aggregation\n",
    "```\n",
    "Antes de fazer hash join/merge, agregar localmente:\n",
    "\n",
    "Thread 1: chunk1 → partial_agg1 (100K → 1K grupos)\n",
    "Thread 2: chunk2 → partial_agg2 (100K → 1K grupos)\n",
    "Thread 3: chunk3 → partial_agg3 (100K → 1K grupos)\n",
    "\n",
    "Merge final: 3K entradas em vez de 300K!\n",
    "```\n",
    "\n",
    "### Partitioned Aggregation\n",
    "```\n",
    "Particionar por hash da chave:\n",
    "\n",
    "Partition 0: keys onde hash % 4 == 0\n",
    "Partition 1: keys onde hash % 4 == 1\n",
    "...\n",
    "\n",
    "Cada partição pode ser processada em paralelo sem locks!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração de pre-aggregation\n",
    "def pre_aggregate_chunks(keys, values, chunk_size=100000):\n",
    "    \"\"\"Simula pre-aggregation antes do merge final\"\"\"\n",
    "    n = len(keys)\n",
    "    partial_results = []\n",
    "    \n",
    "    # Fase 1: Agregar cada chunk localmente\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk_keys = keys[start:end]\n",
    "        chunk_values = values[start:end]\n",
    "        \n",
    "        unique, inverse = np.unique(chunk_keys, return_inverse=True)\n",
    "        sums = np.bincount(inverse, weights=chunk_values)\n",
    "        counts = np.bincount(inverse)\n",
    "        \n",
    "        partial_results.append({\n",
    "            'keys': unique,\n",
    "            'sums': sums,\n",
    "            'counts': counts\n",
    "        })\n",
    "    \n",
    "    # Estatísticas de redução\n",
    "    total_partial_rows = sum(len(pr['keys']) for pr in partial_results)\n",
    "    print(f\"Linhas originais: {n:,}\")\n",
    "    print(f\"Linhas após pre-aggregation: {total_partial_rows:,}\")\n",
    "    print(f\"Redução: {n/total_partial_rows:.1f}x\")\n",
    "    \n",
    "    return partial_results\n",
    "\n",
    "# Teste\n",
    "n = 1_000_000\n",
    "keys = np.random.randint(0, 1000, n)\n",
    "values = np.random.randn(n)\n",
    "\n",
    "_ = pre_aggregate_chunks(keys, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Resumo\n",
    "\n",
    "| Técnica | Descrição | Benefício |\n",
    "|---------|-----------|----------|\n",
    "| **Hash vetorizado** | Calcula hashes de múltiplas chaves via SIMD | CPU eficiente |\n",
    "| **Scatter/Gather** | Agrupa dados por bucket antes de agregar | Cache friendly |\n",
    "| **Pre-aggregation** | Agrega localmente antes do merge | Reduz dados |\n",
    "| **Particionamento** | Divide por hash para paralelismo sem locks | Escalabilidade |\n",
    "| **Chunked processing** | Processa 2048 linhas por vez | Cabe no L1 |\n",
    "\n",
    "### Pontos Chave:\n",
    "\n",
    "1. **Cardinalidade importa**: Poucos grupos = mais rápido\n",
    "2. **Chunks de 2048**: Hash table parcial cabe no cache\n",
    "3. **Particionamento**: Permite paralelismo sem contenção\n",
    "4. **Pre-aggregation**: Reduz volume antes do merge\n",
    "5. **SIMD para hash**: Múltiplas chaves em paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}