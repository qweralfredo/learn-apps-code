{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033a0847",
   "metadata": {},
   "source": [
    "# Cap√≠tulo 04 - Trabalhando com Parquet\n",
    "\n",
    "Este notebook explora o formato Parquet, um formato colunar otimizado para grandes volumes de dados, amplamente usado em an√°lise de dados e data lakes.\n",
    "\n",
    "## üìö T√≥picos Abordados:\n",
    "1. Introdu√ß√£o ao Formato Parquet\n",
    "2. Instala√ß√£o e Prepara√ß√£o\n",
    "3. Leitura de Arquivos Parquet\n",
    "4. Exporta√ß√£o para Parquet\n",
    "5. Compress√£o em Parquet\n",
    "6. Particionamento de Dados\n",
    "7. Schema e Metadados\n",
    "8. Performance: Parquet vs CSV\n",
    "9. Parquet com M√∫ltiplos Arquivos\n",
    "10. Convers√£o CSV ‚Üí Parquet\n",
    "11. Boas Pr√°ticas e Otimiza√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655f251",
   "metadata": {},
   "source": [
    "## 1. Introdu√ß√£o ao Formato Parquet\n",
    "\n",
    "### O que √© Parquet?\n",
    "- **Formato colunar**: Armazena dados por coluna, n√£o por linha\n",
    "- **Compress√£o eficiente**: Economiza espa√ßo e aumenta velocidade\n",
    "- **Otimizado para leitura**: Perfeito para an√°lise de dados\n",
    "- **Compat√≠vel**: Usado por Spark, Pandas, DuckDB, etc.\n",
    "\n",
    "### Vantagens sobre CSV:\n",
    "- ‚úÖ **Menor tamanho** (compress√£o autom√°tica)\n",
    "- ‚úÖ **Leitura mais r√°pida** (formato colunar)\n",
    "- ‚úÖ **Preserva tipos de dados** (sem convers√µes)\n",
    "- ‚úÖ **Metadados inclu√≠dos** (schema embutido)\n",
    "- ‚úÖ **Suporta dados complexos** (nested structures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce669b",
   "metadata": {},
   "source": [
    "## 2. Instala√ß√£o e Prepara√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar depend√™ncias\n",
    "%pip install duckdb pandas pyarrow -q\n",
    "print(\"‚úì Pacotes instalados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(f\"DuckDB vers√£o: {duckdb.__version__}\")\n",
    "print(f\"Pandas vers√£o: {pd.__version__}\")\n",
    "print(\"\\n‚úì Imports realizados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cac32",
   "metadata": {},
   "source": [
    "## 3. Leitura de Arquivos Parquet\n",
    "\n",
    "### 3.1 Criar Dados de Teste e Exportar como Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9277b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar conex√£o\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Criar dados de exemplo e salvar como Parquet\n",
    "con.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            i AS id,\n",
    "            'Usuario_' || i AS nome,\n",
    "            20 + (i % 50) AS idade,\n",
    "            CASE (i % 5)\n",
    "                WHEN 0 THEN 'S√£o Paulo'\n",
    "                WHEN 1 THEN 'Rio de Janeiro'\n",
    "                WHEN 2 THEN 'Belo Horizonte'\n",
    "                WHEN 3 THEN 'Bras√≠lia'\n",
    "                ELSE 'Curitiba'\n",
    "            END AS cidade,\n",
    "            (i % 10 + 1) * 1000.0 AS salario,\n",
    "            (i % 2 = 0) AS ativo\n",
    "        FROM range(1, 1001) t(i)\n",
    "    ) TO 'example.parquet'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Arquivo 'example.parquet' criado!\")\n",
    "print(f\"Tamanho: {os.path.getsize('example.parquet'):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7dddd",
   "metadata": {},
   "source": [
    "### 3.2 Ler com `read_parquet()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a62c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler Parquet como Relation\n",
    "rel = duckdb.read_parquet(\"example.parquet\")\n",
    "\n",
    "print(\"Tipo do objeto:\", type(rel))\n",
    "print(\"\\nPrimeiras 5 linhas:\")\n",
    "rel.limit(5).show()\n",
    "\n",
    "print(\"\\n‚úì Parquet lido com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790134a4",
   "metadata": {},
   "source": [
    "### 3.3 Query SQL Direta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query direta no arquivo Parquet\n",
    "print(\"Consulta SQL direta:\")\n",
    "result = duckdb.sql(\"SELECT * FROM 'example.parquet' LIMIT 10\")\n",
    "result.show()\n",
    "\n",
    "print(\"\\n‚úì Query executada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd0df5",
   "metadata": {},
   "source": [
    "### 3.4 Filtros e Agrega√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf145965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtros\n",
    "print(\"Pessoas com sal√°rio > 7000:\")\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT nome, idade, cidade, salario\n",
    "    FROM 'example.parquet'\n",
    "    WHERE salario > 7000\n",
    "    ORDER BY salario DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "# Agrega√ß√µes por cidade\n",
    "print(\"\\nEstat√≠sticas por cidade:\")\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        cidade,\n",
    "        COUNT(*) AS total,\n",
    "        AVG(idade) AS idade_media,\n",
    "        AVG(salario) AS salario_medio,\n",
    "        SUM(CASE WHEN ativo THEN 1 ELSE 0 END) AS ativos\n",
    "    FROM 'example.parquet'\n",
    "    GROUP BY cidade\n",
    "    ORDER BY total DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf09b00",
   "metadata": {},
   "source": [
    "### 3.5 Converter para DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para Pandas\n",
    "df = duckdb.read_parquet(\"example.parquet\").df()\n",
    "\n",
    "print(\"DataFrame criado:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Colunas: {df.columns.tolist()}\")\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa329d1",
   "metadata": {},
   "source": [
    "## 4. Exporta√ß√£o para Parquet\n",
    "\n",
    "### 4.1 Exporta√ß√£o B√°sica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela tempor√°ria\n",
    "con.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE usuarios AS\n",
    "    SELECT * FROM 'example.parquet' WHERE idade >= 30\n",
    "\"\"\")\n",
    "\n",
    "# Exportar usando write_parquet()\n",
    "result = con.sql(\"SELECT * FROM usuarios\")\n",
    "result.write_parquet(\"usuarios_30plus.parquet\")\n",
    "\n",
    "print(\"‚úì Arquivo 'usuarios_30plus.parquet' criado!\")\n",
    "print(f\"Tamanho: {os.path.getsize('usuarios_30plus.parquet'):,} bytes\")\n",
    "\n",
    "# Verificar conte√∫do\n",
    "print(\"\\nPrimeiras linhas do arquivo exportado:\")\n",
    "duckdb.sql(\"SELECT * FROM 'usuarios_30plus.parquet' LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17594b",
   "metadata": {},
   "source": [
    "### 4.2 Exporta√ß√£o com COPY TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432bc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar COPY TO (mais controle)\n",
    "con.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT cidade, COUNT(*) AS total, AVG(salario) AS salario_medio\n",
    "        FROM usuarios\n",
    "        GROUP BY cidade\n",
    "    ) TO 'resumo_cidades.parquet' (FORMAT parquet)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Arquivo 'resumo_cidades.parquet' criado!\")\n",
    "\n",
    "# Verificar\n",
    "print(\"\\nConte√∫do:\")\n",
    "duckdb.sql(\"SELECT * FROM 'resumo_cidades.parquet'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85637c",
   "metadata": {},
   "source": [
    "## 5. Compress√£o em Parquet\n",
    "\n",
    "Parquet suporta diferentes algoritmos de compress√£o.\n",
    "\n",
    "### 5.1 Algoritmos Dispon√≠veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar diferentes compress√µes\n",
    "compressoes = ['uncompressed', 'snappy', 'gzip', 'zstd']\n",
    "\n",
    "print(\"Compara√ß√£o de compress√µes:\\n\")\n",
    "print(f\"{'Compress√£o':<15} {'Tamanho (bytes)':<20} {'Economia'}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for comp in compressoes:\n",
    "    arquivo = f'test_{comp}.parquet'\n",
    "    \n",
    "    con.sql(f\"\"\"\n",
    "        COPY (\n",
    "            SELECT * FROM 'example.parquet'\n",
    "        ) TO '{arquivo}' (FORMAT parquet, COMPRESSION {comp})\n",
    "    \"\"\")\n",
    "    \n",
    "    tamanho = os.path.getsize(arquivo)\n",
    "    \n",
    "    if comp == 'uncompressed':\n",
    "        tamanho_original = tamanho\n",
    "        economia = \"-\"\n",
    "    else:\n",
    "        economia = f\"{(1 - tamanho/tamanho_original)*100:.1f}%\"\n",
    "    \n",
    "    print(f\"{comp:<15} {tamanho:>15,}     {economia}\")\n",
    "\n",
    "print(\"\\n‚úì Compara√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19b4d5",
   "metadata": {},
   "source": [
    "### 5.2 Recomenda√ß√µes de Compress√£o\n",
    "\n",
    "- **SNAPPY**: Mais r√°pido, compress√£o moderada (padr√£o)\n",
    "- **GZIP**: Compress√£o melhor, mais lento\n",
    "- **ZSTD**: Melhor equil√≠brio (recomendado)\n",
    "- **Uncompressed**: Apenas para testes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46abf6",
   "metadata": {},
   "source": [
    "## 6. Particionamento de Dados\n",
    "\n",
    "Particionar dados melhora performance em grandes volumes.\n",
    "\n",
    "### 6.1 Criar Dados Particionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ced092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diret√≥rio\n",
    "os.makedirs('dados_particionados', exist_ok=True)\n",
    "\n",
    "# Particionar por cidade\n",
    "con.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT * FROM 'example.parquet'\n",
    "    ) TO 'dados_particionados' \n",
    "    (FORMAT parquet, PARTITION_BY (cidade))\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Dados particionados criados!\\n\")\n",
    "\n",
    "# Listar parti√ß√µes\n",
    "print(\"Parti√ß√µes criadas:\")\n",
    "for root, dirs, files in os.walk('dados_particionados'):\n",
    "    for file in files:\n",
    "        caminho = os.path.join(root, file)\n",
    "        tamanho = os.path.getsize(caminho)\n",
    "        print(f\"  {caminho}: {tamanho:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e572ad",
   "metadata": {},
   "source": [
    "### 6.2 Ler Dados Particionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler todos os arquivos particionados\n",
    "print(\"Lendo dados particionados:\")\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT cidade, COUNT(*) AS total\n",
    "    FROM 'dados_particionados/**/*.parquet'\n",
    "    GROUP BY cidade\n",
    "    ORDER BY cidade\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n‚úì DuckDB l√™ automaticamente parti√ß√µes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ef9d9",
   "metadata": {},
   "source": [
    "### 6.3 Filtro de Parti√ß√£o (Partition Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta que usa apenas uma parti√ß√£o\n",
    "print(\"Consulta com filtro de parti√ß√£o:\")\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT nome, idade, salario\n",
    "    FROM 'dados_particionados/**/*.parquet'\n",
    "    WHERE cidade = 'S√£o Paulo'\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n‚úì DuckDB l√™ apenas a parti√ß√£o necess√°ria!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fcc67",
   "metadata": {},
   "source": [
    "## 7. Schema e Metadados\n",
    "\n",
    "### 7.1 Inspecionar Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver schema do arquivo Parquet\n",
    "print(\"Schema do arquivo:\")\n",
    "duckdb.sql(\"DESCRIBE SELECT * FROM 'example.parquet'\").show()\n",
    "\n",
    "# Estat√≠sticas das colunas\n",
    "print(\"\\nEstat√≠sticas:\")\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS total_linhas,\n",
    "        COUNT(DISTINCT cidade) AS cidades_distintas,\n",
    "        MIN(idade) AS idade_min,\n",
    "        MAX(idade) AS idade_max,\n",
    "        MIN(salario) AS salario_min,\n",
    "        MAX(salario) AS salario_max\n",
    "    FROM 'example.parquet'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ed0c2",
   "metadata": {},
   "source": [
    "### 7.2 Metadados do Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f521e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes sobre o arquivo Parquet\n",
    "print(\"Metadados do arquivo Parquet:\")\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        file_name,\n",
    "        row_group_id,\n",
    "        row_group_num_rows,\n",
    "        total_compressed_size\n",
    "    FROM parquet_metadata('example.parquet')\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n‚úì Metadados lidos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede67ec",
   "metadata": {},
   "source": [
    "## 8. Performance: Parquet vs CSV\n",
    "\n",
    "### 8.1 Criar Arquivos de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3badff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset grande\n",
    "con.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE dataset_grande AS\n",
    "    SELECT \n",
    "        i AS id,\n",
    "        'Nome_' || i AS nome,\n",
    "        20 + (i % 60) AS idade,\n",
    "        'Cidade_' || (i % 100) AS cidade,\n",
    "        (i % 20 + 1) * 500.0 AS valor,\n",
    "        DATE '2024-01-01' + INTERVAL (i % 365) DAY AS data,\n",
    "        (i % 2 = 0) AS flag\n",
    "    FROM range(1, 100001) t(i)\n",
    "\"\"\")\n",
    "\n",
    "# Exportar como CSV e Parquet\n",
    "con.sql(\"COPY dataset_grande TO 'benchmark.csv' (HEADER)\")\n",
    "con.sql(\"COPY dataset_grande TO 'benchmark.parquet'\")\n",
    "\n",
    "# Comparar tamanhos\n",
    "tamanho_csv = os.path.getsize('benchmark.csv')\n",
    "tamanho_parquet = os.path.getsize('benchmark.parquet')\n",
    "\n",
    "print(\"Compara√ß√£o de tamanho (100k linhas):\")\n",
    "print(f\"CSV:     {tamanho_csv:>10,} bytes ({tamanho_csv/1024/1024:.2f} MB)\")\n",
    "print(f\"Parquet: {tamanho_parquet:>10,} bytes ({tamanho_parquet/1024/1024:.2f} MB)\")\n",
    "print(f\"\\nEconomia: {(1 - tamanho_parquet/tamanho_csv)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0f18f",
   "metadata": {},
   "source": [
    "### 8.2 Benchmark de Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Teste CSV\n",
    "start = time.time()\n",
    "result_csv = duckdb.sql(\"\"\"\n",
    "    SELECT cidade, COUNT(*), AVG(valor)\n",
    "    FROM 'benchmark.csv'\n",
    "    WHERE idade > 40\n",
    "    GROUP BY cidade\n",
    "\"\"\").fetchall()\n",
    "time_csv = time.time() - start\n",
    "\n",
    "# Teste Parquet\n",
    "start = time.time()\n",
    "result_parquet = duckdb.sql(\"\"\"\n",
    "    SELECT cidade, COUNT(*), AVG(valor)\n",
    "    FROM 'benchmark.parquet'\n",
    "    WHERE idade > 40\n",
    "    GROUP BY cidade\n",
    "\"\"\").fetchall()\n",
    "time_parquet = time.time() - start\n",
    "\n",
    "print(\"Benchmark: Agrega√ß√£o em 100k linhas\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"CSV:     {time_csv*1000:>8.2f}ms\")\n",
    "print(f\"Parquet: {time_parquet*1000:>8.2f}ms\")\n",
    "print(f\"\\nParquet √© {time_csv/time_parquet:.1f}x mais r√°pido!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb9fd4",
   "metadata": {},
   "source": [
    "### 8.3 Benchmark de Leitura Seletiva (Proje√ß√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler apenas 2 colunas (formato colunar brilha aqui!)\n",
    "print(\"Leitura seletiva (apenas 2 de 7 colunas):\\n\")\n",
    "\n",
    "# CSV - precisa ler tudo\n",
    "start = time.time()\n",
    "result_csv = duckdb.sql(\"SELECT idade, valor FROM 'benchmark.csv'\").fetchall()\n",
    "time_csv = time.time() - start\n",
    "\n",
    "# Parquet - l√™ apenas as colunas necess√°rias\n",
    "start = time.time()\n",
    "result_parquet = duckdb.sql(\"SELECT idade, valor FROM 'benchmark.parquet'\").fetchall()\n",
    "time_parquet = time.time() - start\n",
    "\n",
    "print(f\"CSV:     {time_csv*1000:>8.2f}ms\")\n",
    "print(f\"Parquet: {time_parquet*1000:>8.2f}ms\")\n",
    "print(f\"\\nParquet √© {time_csv/time_parquet:.1f}x mais r√°pido!\")\n",
    "print(\"\\nüí° Formato colunar s√≥ l√™ as colunas necess√°rias!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46125715",
   "metadata": {},
   "source": [
    "## 9. Parquet com M√∫ltiplos Arquivos\n",
    "\n",
    "### 9.1 Criar M√∫ltiplos Arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4176f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diret√≥rio\n",
    "os.makedirs('vendas_parquet', exist_ok=True)\n",
    "\n",
    "# Criar arquivos para diferentes meses\n",
    "meses = ['jan', 'fev', 'mar', 'abr']\n",
    "for i, mes in enumerate(meses, 1):\n",
    "    con.sql(f\"\"\"\n",
    "        COPY (\n",
    "            SELECT \n",
    "                j AS id,\n",
    "                '{mes}' AS mes,\n",
    "                'Produto_' || (j % 20) AS produto,\n",
    "                (j % 100 + 10) * 5.0 AS valor\n",
    "            FROM range({(i-1)*50 + 1}, {i*50 + 1}) t(j)\n",
    "        ) TO 'vendas_parquet/vendas_{mes}.parquet'\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úì Arquivos Parquet criados:\")\n",
    "for arquivo in os.listdir('vendas_parquet'):\n",
    "    tamanho = os.path.getsize(f'vendas_parquet/{arquivo}')\n",
    "    print(f\"  - {arquivo}: {tamanho:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c71ee3",
   "metadata": {},
   "source": [
    "### 9.2 Ler Todos os Arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glob pattern para ler m√∫ltiplos Parquet\n",
    "print(\"Agrega√ß√£o em m√∫ltiplos arquivos:\")\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        mes,\n",
    "        COUNT(*) AS total_vendas,\n",
    "        SUM(valor) AS valor_total,\n",
    "        AVG(valor) AS valor_medio,\n",
    "        MIN(valor) AS valor_min,\n",
    "        MAX(valor) AS valor_max\n",
    "    FROM 'vendas_parquet/*.parquet'\n",
    "    GROUP BY mes\n",
    "    ORDER BY mes\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n‚úì Todos os arquivos processados em uma query!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec0142",
   "metadata": {},
   "source": [
    "## 10. Convers√£o CSV ‚Üí Parquet\n",
    "\n",
    "### 10.1 Convers√£o Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar CSV de exemplo\n",
    "csv_data = \"\"\"id,produto,preco,quantidade\n",
    "1,Notebook,3500.00,5\n",
    "2,Mouse,50.00,100\n",
    "3,Teclado,150.00,50\n",
    "4,Monitor,800.00,20\n",
    "5,Webcam,200.00,30\n",
    "\"\"\"\n",
    "\n",
    "with open('produtos.csv', 'w') as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Converter para Parquet\n",
    "duckdb.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT * FROM 'produtos.csv'\n",
    "    ) TO 'produtos.parquet'\n",
    "\"\"\")\n",
    "\n",
    "# Comparar tamanhos\n",
    "csv_size = os.path.getsize('produtos.csv')\n",
    "parquet_size = os.path.getsize('produtos.parquet')\n",
    "\n",
    "print(\"Convers√£o CSV ‚Üí Parquet:\")\n",
    "print(f\"CSV:     {csv_size:,} bytes\")\n",
    "print(f\"Parquet: {parquet_size:,} bytes\")\n",
    "print(f\"Economia: {(1 - parquet_size/csv_size)*100:.1f}%\")\n",
    "\n",
    "# Verificar dados\n",
    "print(\"\\nDados convertidos:\")\n",
    "duckdb.sql(\"SELECT * FROM 'produtos.parquet'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc14427",
   "metadata": {},
   "source": [
    "### 10.2 Convers√£o em Lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar m√∫ltiplos CSVs\n",
    "os.makedirs('csv_origem', exist_ok=True)\n",
    "os.makedirs('parquet_destino', exist_ok=True)\n",
    "\n",
    "for i in range(1, 4):\n",
    "    con.sql(f\"\"\"\n",
    "        COPY (\n",
    "            SELECT \n",
    "                j AS id,\n",
    "                'Item_' || j AS nome,\n",
    "                (j * 10.5) AS valor\n",
    "            FROM range({(i-1)*20 + 1}, {i*20 + 1}) t(j)\n",
    "        ) TO 'csv_origem/dados_{i}.csv' (HEADER)\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úì CSVs criados:\")\n",
    "for arquivo in os.listdir('csv_origem'):\n",
    "    print(f\"  - {arquivo}\")\n",
    "\n",
    "# Converter todos para Parquet (sem particionamento complexo)\n",
    "print(\"\\nConvertendo...\")\n",
    "duckdb.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT * FROM 'csv_origem/*.csv'\n",
    "    ) TO 'parquet_destino/dados_convertidos.parquet' (FORMAT parquet)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úì Convers√£o conclu√≠da!\")\n",
    "print(\"\\nArquivos Parquet criados:\")\n",
    "for root, dirs, files in os.walk('parquet_destino'):\n",
    "    for file in files:\n",
    "        caminho = os.path.join(root, file)\n",
    "        tamanho = os.path.getsize(caminho)\n",
    "        print(f\"  - {caminho}: {tamanho:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b084273",
   "metadata": {},
   "source": [
    "## 11. Boas Pr√°ticas e Otimiza√ß√µes\n",
    "\n",
    "### üìä Dicas de Uso:\n",
    "\n",
    "1. **Use Parquet para dados anal√≠ticos**: Ideal para data lakes e an√°lise\n",
    "2. **Particione dados grandes**: Melhora performance em queries filtradas\n",
    "3. **Escolha compress√£o ZSTD**: Melhor equil√≠brio velocidade/tamanho\n",
    "4. **Aproveite formato colunar**: SELECT apenas colunas necess√°rias\n",
    "5. **Evite muitos arquivos pequenos**: Combine em arquivos maiores\n",
    "6. **Use para dados imut√°veis**: Parquet n√£o √© para dados que mudam frequentemente\n",
    "\n",
    "### 11.1 Exemplo Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eceb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå N√£o Otimizado\n",
    "# df = pd.read_csv('grande.csv')\n",
    "# df_filtrado = df[df['valor'] > 1000][['id', 'nome', 'valor']]\n",
    "\n",
    "# ‚úÖ Otimizado com Parquet\n",
    "df_otimizado = duckdb.sql(\"\"\"\n",
    "    SELECT id, nome, valor\n",
    "    FROM 'benchmark.parquet'\n",
    "    WHERE valor > 5000\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Consulta otimizada:\")\n",
    "print(df_otimizado)\n",
    "\n",
    "print(\"\\n‚úì Parquet + proje√ß√£o de colunas + filtro = m√°xima efici√™ncia!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc082ccd",
   "metadata": {},
   "source": [
    "### 11.2 Quando Usar Parquet vs CSV\n",
    "\n",
    "| Caracter√≠stica | CSV | Parquet |\n",
    "|---------------|-----|--------|\n",
    "| **Tamanho** | Grande | Pequeno (comprimido) |\n",
    "| **Velocidade leitura** | Lento | Muito r√°pido |\n",
    "| **Leitura seletiva** | L√™ tudo | L√™ apenas colunas necess√°rias |\n",
    "| **Tipos de dados** | Texto (precisa converter) | Preserva tipos |\n",
    "| **Compatibilidade** | Universal | Requer bibliotecas |\n",
    "| **Edi√ß√£o manual** | F√°cil (texto) | Imposs√≠vel |\n",
    "| **Uso recomendado** | Dados pequenos, interoperabilidade | Dados grandes, an√°lise |\n",
    "\n",
    "### Recomenda√ß√£o:\n",
    "- üìÑ **Use CSV** para: Dados pequenos, troca de dados, edi√ß√£o manual\n",
    "- üìä **Use Parquet** para: Data lakes, an√°lise de grandes volumes, armazenamento eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e01db1",
   "metadata": {},
   "source": [
    "## üéØ Resumo do Cap√≠tulo\n",
    "\n",
    "Neste cap√≠tulo, exploramos:\n",
    "\n",
    "1. ‚úÖ **Introdu√ß√£o ao Parquet** - formato colunar otimizado\n",
    "2. ‚úÖ **Leitura** com `read_parquet()` e SQL direto\n",
    "3. ‚úÖ **Exporta√ß√£o** com m√∫ltiplas op√ß√µes\n",
    "4. ‚úÖ **Compress√£o** (SNAPPY, GZIP, ZSTD)\n",
    "5. ‚úÖ **Particionamento** para melhor performance\n",
    "6. ‚úÖ **Schema e metadados** embutidos\n",
    "7. ‚úÖ **Performance** superior ao CSV\n",
    "8. ‚úÖ **M√∫ltiplos arquivos** com glob patterns\n",
    "9. ‚úÖ **Convers√£o** CSV ‚Üí Parquet\n",
    "10. ‚úÖ **Boas pr√°ticas** e otimiza√ß√µes\n",
    "\n",
    "### üîë Pontos-Chave:\n",
    "- Parquet √© **muito mais r√°pido e compacto** que CSV\n",
    "- Formato **colunar** permite leitura seletiva\n",
    "- **Preserva tipos** de dados e metadados\n",
    "- Ideal para **data lakes** e an√°lise\n",
    "- **Particionamento** melhora performance\n",
    "\n",
    "### üìö Pr√≥ximo Cap√≠tulo:\n",
    "Importa√ß√£o e Exporta√ß√£o de JSON!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58654c8",
   "metadata": {},
   "source": [
    "## üßπ Limpeza (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Arquivos\n",
    "arquivos = [\n",
    "    'example.parquet', 'usuarios_30plus.parquet', 'resumo_cidades.parquet',\n",
    "    'test_uncompressed.parquet', 'test_snappy.parquet', 'test_gzip.parquet', 'test_zstd.parquet',\n",
    "    'benchmark.csv', 'benchmark.parquet', 'produtos.csv', 'produtos.parquet'\n",
    "]\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    if os.path.exists(arquivo):\n",
    "        os.remove(arquivo)\n",
    "        print(f\"‚úì Removido: {arquivo}\")\n",
    "\n",
    "# Diret√≥rios\n",
    "diretorios = ['dados_particionados', 'vendas_parquet', 'csv_origem', 'parquet_destino']\n",
    "for dir in diretorios:\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "        print(f\"‚úì Diret√≥rio '{dir}' removido\")\n",
    "\n",
    "print(\"\\n‚úì Limpeza conclu√≠da!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
