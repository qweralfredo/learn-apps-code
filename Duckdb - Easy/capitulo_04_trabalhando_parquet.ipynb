{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Capítulo 4: Trabalhando com Arquivos Parquet\n",
        "DuckDB - Easy Course\n",
        "\n",
        "Este script demonstra todas as funcionalidades para trabalhar com arquivos Parquet.\n",
        "\"\"\"\n",
        "\n",
        "import duckdb\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "# Configurar encoding UTF-8 para Windows\n",
        "if sys.platform == 'win32':\n",
        "    import io\n",
        "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
        "    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')\n",
        "\n",
        "\n",
        "def criar_dados_exemplo():\n",
        "    \"\"\"Cria dados de exemplo e salva em diferentes formatos.\"\"\"\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Criar tabela de exemplo\n",
        "    con.sql(\"\"\"\n",
        "        CREATE OR REPLACE TABLE vendas_completas AS\n",
        "        SELECT\n",
        "            range as id,\n",
        "            'Produto_' || (range % 10) as produto,\n",
        "            'Categoria_' || (range % 3) as categoria,\n",
        "            (range % 100) + 1 as quantidade,\n",
        "            (range % 1000) + 10.50 as preco,\n",
        "            DATE '2024-01-01' + INTERVAL (range % 365) DAY as data\n",
        "        FROM range(1000)\n",
        "    \"\"\")\n",
        "\n",
        "    # Salvar como CSV para comparação\n",
        "    csv_file = os.path.join(temp_dir, 'vendas.csv')\n",
        "    con.sql(f\"COPY vendas_completas TO '{csv_file}'\")\n",
        "\n",
        "    # Salvar como Parquet\n",
        "    parquet_file = os.path.join(temp_dir, 'vendas.parquet')\n",
        "    con.sql(f\"COPY vendas_completas TO '{parquet_file}' (FORMAT parquet)\")\n",
        "\n",
        "    con.close()\n",
        "    return temp_dir\n",
        "\n",
        "\n",
        "def exemplo_01_leitura_basica():\n",
        "    \"\"\"Exemplo 1: Leitura básica de Parquet.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"EXEMPLO 1: Leitura Básica de Parquet\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = criar_dados_exemplo()\n",
        "    parquet_file = os.path.join(temp_dir, 'vendas.parquet')\n",
        "\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Leitura direta\n",
        "    print(\"\\n1.1 - Leitura direta:\")\n",
        "    result = con.sql(f\"SELECT * FROM '{parquet_file}' LIMIT 5\")\n",
        "    result.show()\n",
        "\n",
        "    # Usando read_parquet\n",
        "    print(\"\\n1.2 - Usando read_parquet:\")\n",
        "    result = con.sql(f\"SELECT * FROM read_parquet('{parquet_file}') LIMIT 5\")\n",
        "    result.show()\n",
        "\n",
        "    # DESCRIBE para ver schema\n",
        "    print(\"\\n1.3 - DESCRIBE para ver schema:\")\n",
        "    con.sql(f\"DESCRIBE SELECT * FROM '{parquet_file}'\").show()\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exemplo_02_comparacao_csv_parquet():\n",
        "    \"\"\"Exemplo 2: Comparação de performance CSV vs Parquet.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 2: Performance CSV vs Parquet\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = criar_dados_exemplo()\n",
        "    csv_file = os.path.join(temp_dir, 'vendas.csv')\n",
        "    parquet_file = os.path.join(temp_dir, 'vendas.parquet')\n",
        "\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Tamanho dos arquivos\n",
        "    csv_size = os.path.getsize(csv_file)\n",
        "    parquet_size = os.path.getsize(parquet_file)\n",
        "\n",
        "    print(f\"\\n2.1 - Tamanho dos arquivos:\")\n",
        "    print(f\"CSV:     {csv_size:,} bytes ({csv_size / 1024:.2f} KB)\")\n",
        "    print(f\"Parquet: {parquet_size:,} bytes ({parquet_size / 1024:.2f} KB)\")\n",
        "    print(f\"Economia: {100 * (1 - parquet_size / csv_size):.1f}%\")\n",
        "\n",
        "    # Performance de leitura - CSV\n",
        "    print(f\"\\n2.2 - Performance de leitura:\")\n",
        "    start = time.time()\n",
        "    con.sql(f\"SELECT count(*) FROM '{csv_file}'\").fetchone()\n",
        "    csv_time = time.time() - start\n",
        "    print(f\"CSV:     {csv_time:.4f}s\")\n",
        "\n",
        "    # Performance de leitura - Parquet\n",
        "    start = time.time()\n",
        "    con.sql(f\"SELECT count(*) FROM '{parquet_file}'\").fetchone()\n",
        "    parquet_time = time.time() - start\n",
        "    print(f\"Parquet: {parquet_time:.4f}s\")\n",
        "    print(f\"Speedup: {csv_time / parquet_time:.1f}x mais rápido\")\n",
        "\n",
        "    # Projection pushdown (ler apenas 2 colunas)\n",
        "    print(f\"\\n2.3 - Projection Pushdown (2 colunas de 6):\")\n",
        "    start = time.time()\n",
        "    con.sql(f\"SELECT produto, preco FROM '{csv_file}'\").fetchall()\n",
        "    csv_proj_time = time.time() - start\n",
        "    print(f\"CSV:     {csv_proj_time:.4f}s\")\n",
        "\n",
        "    start = time.time()\n",
        "    con.sql(f\"SELECT produto, preco FROM '{parquet_file}'\").fetchall()\n",
        "    parquet_proj_time = time.time() - start\n",
        "    print(f\"Parquet: {parquet_proj_time:.4f}s\")\n",
        "    print(f\"Speedup: {csv_proj_time / parquet_proj_time:.1f}x mais rápido\")\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exemplo_03_multiplos_arquivos():\n",
        "    \"\"\"Exemplo 3: Leitura de múltiplos arquivos Parquet.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 3: Múltiplos Arquivos Parquet\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Criar múltiplos arquivos Parquet\n",
        "    for mes in range(1, 4):\n",
        "        con.sql(f\"\"\"\n",
        "            COPY (\n",
        "                SELECT\n",
        "                    'Produto_' || range as produto,\n",
        "                    range as quantidade,\n",
        "                    range * 10.5 as preco,\n",
        "                    {mes} as mes\n",
        "                FROM range(10)\n",
        "            ) TO '{temp_dir}/vendas_2024_{mes:02d}.parquet'\n",
        "            (FORMAT parquet)\n",
        "        \"\"\")\n",
        "\n",
        "    # Ler com lista explícita\n",
        "    print(\"\\n3.1 - Lista explícita de arquivos:\")\n",
        "    result = con.sql(f\"\"\"\n",
        "        SELECT * FROM read_parquet([\n",
        "            '{temp_dir}/vendas_2024_01.parquet',\n",
        "            '{temp_dir}/vendas_2024_02.parquet',\n",
        "            '{temp_dir}/vendas_2024_03.parquet'\n",
        "        ])\n",
        "    \"\"\")\n",
        "    print(f\"Total de linhas: {result.count('*').fetchone()[0]}\")\n",
        "    result.limit(5).show()\n",
        "\n",
        "    # Ler com glob pattern\n",
        "    print(\"\\n3.2 - Usando glob pattern:\")\n",
        "    result = con.sql(f\"SELECT * FROM '{temp_dir}/vendas_2024_*.parquet'\")\n",
        "    print(f\"Total de linhas: {result.count('*').fetchone()[0]}\")\n",
        "    result.limit(5).show()\n",
        "\n",
        "    # Com coluna filename\n",
        "    print(\"\\n3.3 - Com coluna filename:\")\n",
        "    result = con.sql(f\"\"\"\n",
        "        SELECT *, filename\n",
        "        FROM read_parquet('{temp_dir}/vendas_2024_*.parquet', filename=true)\n",
        "        LIMIT 5\n",
        "    \"\"\")\n",
        "    result.show()\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exemplo_04_escrita_parquet():\n",
        "    \"\"\"Exemplo 4: Escrita de arquivos Parquet com diferentes compressões.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 4: Escrita de Parquet com Compressão\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Criar dados de exemplo\n",
        "    con.sql(\"\"\"\n",
        "        CREATE OR REPLACE TABLE dados_teste AS\n",
        "        SELECT\n",
        "            range as id,\n",
        "            'texto_' || range as descricao,\n",
        "            (range % 100) as valor\n",
        "        FROM range(10000)\n",
        "    \"\"\")\n",
        "\n",
        "    compressoes = ['snappy', 'zstd', 'lz4', 'uncompressed']\n",
        "\n",
        "    print(\"\\n4.1 - Comparação de compressões:\")\n",
        "    print(f\"{'Compressão':<15} {'Tamanho':<15} {'Tempo':<10}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for comp in compressoes:\n",
        "        output_file = os.path.join(temp_dir, f'dados_{comp}.parquet')\n",
        "\n",
        "        start = time.time()\n",
        "        con.sql(f\"\"\"\n",
        "            COPY dados_teste TO '{output_file}'\n",
        "            (FORMAT parquet, COMPRESSION {comp})\n",
        "        \"\"\")\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        size = os.path.getsize(output_file)\n",
        "        print(f\"{comp:<15} {size:>10,} bytes {elapsed:>8.4f}s\")\n",
        "\n",
        "    # Parquet com metadados customizados\n",
        "    print(\"\\n4.2 - Parquet com metadados customizados:\")\n",
        "    metadata_file = os.path.join(temp_dir, 'dados_metadata.parquet')\n",
        "    con.sql(f\"\"\"\n",
        "        COPY (SELECT 42 AS resposta, 'DuckDB' as ferramenta)\n",
        "        TO '{metadata_file}' (\n",
        "            FORMAT parquet,\n",
        "            KV_METADATA {{\n",
        "                autor: 'Curso DuckDB',\n",
        "                versao: '1.0',\n",
        "                descricao: 'Arquivo de exemplo'\n",
        "            }}\n",
        "        )\n",
        "    \"\"\")\n",
        "    print(f\"Arquivo criado: {metadata_file}\")\n",
        "\n",
        "    # Ler metadados\n",
        "    print(\"\\n4.3 - Ler metadados do arquivo:\")\n",
        "    con.sql(f\"SELECT * FROM parquet_kv_metadata('{metadata_file}')\").show()\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exemplo_05_metadados_parquet():\n",
        "    \"\"\"Exemplo 5: Explorar metadados de arquivos Parquet.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 5: Metadados de Arquivos Parquet\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = criar_dados_exemplo()\n",
        "    parquet_file = os.path.join(temp_dir, 'vendas.parquet')\n",
        "\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # parquet_metadata\n",
        "    print(\"\\n5.1 - parquet_metadata (geral):\")\n",
        "    con.sql(f\"SELECT * FROM parquet_metadata('{parquet_file}')\").show()\n",
        "\n",
        "    # parquet_schema\n",
        "    print(\"\\n5.2 - parquet_schema:\")\n",
        "    con.sql(f\"SELECT * FROM parquet_schema('{parquet_file}')\").show()\n",
        "\n",
        "    # parquet_file_metadata\n",
        "    print(\"\\n5.3 - parquet_file_metadata:\")\n",
        "    con.sql(f\"SELECT * FROM parquet_file_metadata('{parquet_file}')\").show()\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exemplo_06_filter_pushdown():\n",
        "    \"\"\"Exemplo 6: Demonstração de Filter Pushdown.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 6: Filter Pushdown\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = criar_dados_exemplo()\n",
        "    parquet_file = os.path.join(temp_dir, 'vendas.parquet')\n",
        "\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Query sem filtro\n",
        "    print(\"\\n6.1 - Query sem filtro (lê tudo):\")\n",
        "    start = time.time()\n",
        "    result = con.sql(f\"SELECT count(*) FROM '{parquet_file}'\")\n",
        "    elapsed1 = time.time() - start\n",
        "    print(f\"Resultado: {result.fetchone()[0]} linhas\")\n",
        "    print(f\"Tempo: {elapsed1:.4f}s\")\n",
        "\n",
        "    # Query com filtro (filter pushdown)\n",
        "    print(\"\\n6.2 - Query com filtro (filter pushdown):\")\n",
        "    start = time.time()\n",
        "    result = con.sql(f\"\"\"\n",
        "        SELECT count(*)\n",
        "        FROM '{parquet_file}'\n",
        "        WHERE data >= '2024-06-01' AND quantidade > 50\n",
        "    \"\"\")\n",
        "    elapsed2 = time.time() - start\n",
        "    print(f\"Resultado: {result.fetchone()[0]} linhas\")\n",
        "    print(f\"Tempo: {elapsed2:.4f}s\")\n",
        "\n",
        "    # EXPLAIN ANALYZE\n",
        "    print(\"\\n6.3 - EXPLAIN ANALYZE mostra filter pushdown:\")\n",
        "    con.sql(f\"\"\"\n",
        "        EXPLAIN ANALYZE\n",
        "        SELECT * FROM '{parquet_file}'\n",
        "        WHERE data >= '2024-06-01'\n",
        "        LIMIT 5\n",
        "    \"\"\").show()\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exemplo_07_conversao_csv_parquet():\n",
        "    \"\"\"Exemplo 7: Conversão CSV para Parquet.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 7: Conversão CSV para Parquet\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "\n",
        "    # Criar CSV grande\n",
        "    csv_file = os.path.join(temp_dir, 'dados_grandes.csv')\n",
        "\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    print(\"\\n7.1 - Criando CSV de exemplo...\")\n",
        "    con.sql(f\"\"\"\n",
        "        COPY (\n",
        "            SELECT\n",
        "                range as id,\n",
        "                'Cliente_' || range as nome,\n",
        "                'email' || range || '@example.com' as email,\n",
        "                (range % 1000) + 100 as valor\n",
        "            FROM range(50000)\n",
        "        ) TO '{csv_file}'\n",
        "    \"\"\")\n",
        "\n",
        "    csv_size = os.path.getsize(csv_file)\n",
        "    print(f\"CSV criado: {csv_size:,} bytes ({csv_size / (1024*1024):.2f} MB)\")\n",
        "\n",
        "    # Converter para Parquet\n",
        "    parquet_file = os.path.join(temp_dir, 'dados_grandes.parquet')\n",
        "\n",
        "    print(\"\\n7.2 - Convertendo para Parquet (ZSTD)...\")\n",
        "    start = time.time()\n",
        "    con.sql(f\"\"\"\n",
        "        COPY (SELECT * FROM '{csv_file}')\n",
        "        TO '{parquet_file}'\n",
        "        (FORMAT parquet, COMPRESSION zstd)\n",
        "    \"\"\")\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    parquet_size = os.path.getsize(parquet_file)\n",
        "    print(f\"Parquet criado: {parquet_size:,} bytes ({parquet_size / (1024*1024):.2f} MB)\")\n",
        "    print(f\"Tempo de conversão: {elapsed:.4f}s\")\n",
        "    print(f\"Economia de espaço: {100 * (1 - parquet_size / csv_size):.1f}%\")\n",
        "    print(f\"Fator de compressão: {csv_size / parquet_size:.1f}x\")\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exemplo_08_python_api():\n",
        "    \"\"\"Exemplo 8: Uso da API Python com Parquet.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 8: API Python com Parquet\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = criar_dados_exemplo()\n",
        "    parquet_file = os.path.join(temp_dir, 'vendas.parquet')\n",
        "\n",
        "    # Ler Parquet e converter para DataFrame\n",
        "    print(\"\\n8.1 - Ler Parquet e converter para DataFrame:\")\n",
        "    df = duckdb.read_parquet(parquet_file).df()\n",
        "    print(df.head())\n",
        "    print(f\"\\nShape: {df.shape}\")\n",
        "\n",
        "    # Query e salvar como Parquet\n",
        "    output_file = os.path.join(temp_dir, 'vendas_python.parquet')\n",
        "    print(f\"\\n8.2 - Query e salvar como Parquet:\")\n",
        "    duckdb.sql(f\"\"\"\n",
        "        SELECT\n",
        "            categoria,\n",
        "            sum(quantidade) as total_quantidade,\n",
        "            avg(preco) as preco_medio\n",
        "        FROM '{parquet_file}'\n",
        "        GROUP BY categoria\n",
        "    \"\"\").write_parquet(output_file)\n",
        "\n",
        "    print(f\"Arquivo salvo: {output_file}\")\n",
        "\n",
        "    # Verificar arquivo salvo\n",
        "    print(\"\\n8.3 - Verificar arquivo salvo:\")\n",
        "    duckdb.sql(f\"SELECT * FROM '{output_file}'\").show()\n",
        "\n",
        "\n",
        "def exemplo_09_hive_partitioning():\n",
        "    \"\"\"Exemplo 9: Hive Partitioning.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLO 9: Hive Partitioning\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Criar dados\n",
        "    con.sql(\"\"\"\n",
        "        CREATE OR REPLACE TABLE vendas_part AS\n",
        "        SELECT\n",
        "            range as id,\n",
        "            'Produto_' || (range % 5) as produto,\n",
        "            (range % 12) + 1 as mes,\n",
        "            2024 as ano,\n",
        "            range * 10.5 as valor\n",
        "        FROM range(100)\n",
        "    \"\"\")\n",
        "\n",
        "    # Salvar com particionamento Hive\n",
        "    output_dir = os.path.join(temp_dir, 'vendas_particionadas')\n",
        "    print(f\"\\n9.1 - Salvando com particionamento Hive:\")\n",
        "    print(f\"Diretório: {output_dir}\")\n",
        "\n",
        "    con.sql(f\"\"\"\n",
        "        COPY vendas_part TO '{output_dir}'\n",
        "        (FORMAT parquet, PARTITION_BY (ano, mes))\n",
        "    \"\"\")\n",
        "\n",
        "    # Listar arquivos criados\n",
        "    print(\"\\n9.2 - Estrutura de diretórios criada:\")\n",
        "    for root, dirs, files in os.walk(output_dir):\n",
        "        level = root.replace(output_dir, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:3]:  # Mostrar apenas 3 primeiros\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 3:\n",
        "            print(f\"{subindent}... e mais {len(files) - 3} arquivos\")\n",
        "\n",
        "    # Ler dados particionados\n",
        "    print(\"\\n9.3 - Ler dados particionados:\")\n",
        "    result = con.sql(f\"\"\"\n",
        "        SELECT ano, mes, count(*) as total\n",
        "        FROM read_parquet('{output_dir}/**/*.parquet', hive_partitioning=true)\n",
        "        GROUP BY ano, mes\n",
        "        ORDER BY ano, mes\n",
        "        LIMIT 5\n",
        "    \"\"\")\n",
        "    result.show()\n",
        "\n",
        "    con.close()\n",
        "\n",
        "\n",
        "def exercicio_pratico():\n",
        "    \"\"\"Exercício prático completo.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXERCÍCIO PRÁTICO: Analytics Pipeline com Parquet\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    print(\"\\n1. Criar dados de vendas simulados:\")\n",
        "    con.sql(\"\"\"\n",
        "        CREATE OR REPLACE TABLE vendas_raw AS\n",
        "        SELECT\n",
        "            range as id,\n",
        "            DATE '2024-01-01' + INTERVAL (range % 365) DAY as data,\n",
        "            'Loja_' || ((range % 5) + 1) as loja,\n",
        "            'Produto_' || ((range % 20) + 1) as produto,\n",
        "            ((range % 10) + 1) as quantidade,\n",
        "            ((range % 500) + 50) as preco_unitario\n",
        "        FROM range(10000)\n",
        "    \"\"\")\n",
        "    print(\"Dados criados: 10,000 linhas\")\n",
        "\n",
        "    # Salvar como Parquet\n",
        "    raw_parquet = os.path.join(temp_dir, 'vendas_raw.parquet')\n",
        "    print(f\"\\n2. Salvando como Parquet: {raw_parquet}\")\n",
        "    con.sql(f\"\"\"\n",
        "        COPY vendas_raw TO '{raw_parquet}'\n",
        "        (FORMAT parquet, COMPRESSION zstd)\n",
        "    \"\"\")\n",
        "\n",
        "    size = os.path.getsize(raw_parquet)\n",
        "    print(f\"Tamanho: {size:,} bytes ({size / 1024:.2f} KB)\")\n",
        "\n",
        "    # Análise: Vendas por loja\n",
        "    print(\"\\n3. Análise: Top 5 lojas por receita:\")\n",
        "    con.sql(f\"\"\"\n",
        "        SELECT\n",
        "            loja,\n",
        "            count(*) as num_vendas,\n",
        "            sum(quantidade) as total_quantidade,\n",
        "            sum(quantidade * preco_unitario) as receita_total\n",
        "        FROM '{raw_parquet}'\n",
        "        GROUP BY loja\n",
        "        ORDER BY receita_total DESC\n",
        "        LIMIT 5\n",
        "    \"\"\").show()\n",
        "\n",
        "    # Análise: Produtos mais vendidos\n",
        "    print(\"\\n4. Análise: Top 5 produtos:\")\n",
        "    con.sql(f\"\"\"\n",
        "        SELECT\n",
        "            produto,\n",
        "            sum(quantidade) as total_vendido,\n",
        "            sum(quantidade * preco_unitario) as receita\n",
        "        FROM '{raw_parquet}'\n",
        "        GROUP BY produto\n",
        "        ORDER BY total_vendido DESC\n",
        "        LIMIT 5\n",
        "    \"\"\").show()\n",
        "\n",
        "    # Criar agregação mensal e salvar\n",
        "    monthly_parquet = os.path.join(temp_dir, 'vendas_mensais.parquet')\n",
        "    print(f\"\\n5. Criar agregação mensal: {monthly_parquet}\")\n",
        "    con.sql(f\"\"\"\n",
        "        COPY (\n",
        "            SELECT\n",
        "                date_trunc('month', data) as mes,\n",
        "                loja,\n",
        "                count(*) as num_vendas,\n",
        "                sum(quantidade * preco_unitario) as receita\n",
        "            FROM '{raw_parquet}'\n",
        "            GROUP BY mes, loja\n",
        "        ) TO '{monthly_parquet}'\n",
        "        (FORMAT parquet, COMPRESSION zstd)\n",
        "    \"\"\")\n",
        "\n",
        "    # Verificar resultado\n",
        "    print(\"\\n6. Verificar agregação mensal (primeiras linhas):\")\n",
        "    con.sql(f\"SELECT * FROM '{monthly_parquet}' LIMIT 10\").show()\n",
        "\n",
        "    con.close()\n",
        "    print(\"\\nExercício concluído!\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Função principal que executa todos os exemplos.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"CAPÍTULO 4: TRABALHANDO COM PARQUET - DUCKDB\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        exemplo_01_leitura_basica()\n",
        "        exemplo_02_comparacao_csv_parquet()\n",
        "        exemplo_03_multiplos_arquivos()\n",
        "        exemplo_04_escrita_parquet()\n",
        "        exemplo_05_metadados_parquet()\n",
        "        exemplo_06_filter_pushdown()\n",
        "        exemplo_07_conversao_csv_parquet()\n",
        "        exemplo_08_python_api()\n",
        "        exemplo_09_hive_partitioning()\n",
        "        exercicio_pratico()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TODOS OS EXEMPLOS EXECUTADOS COM SUCESSO!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nErro ao executar exemplos: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}