{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 05 Escrita Dados S3\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_05_escrita_dados_s3\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_05_escrita_dados_s3\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "con.execute(\"\"\"\n",
    "COPY table_name TO 's3://bucket-name/path/filename.extension';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Criar uma tabela de exemplo\n",
    "CREATE TABLE sales AS\n",
    "SELECT\n",
    "    range as id,\n",
    "    'Product_' || (range % 100) as product,\n",
    "    (random() * 1000)::INTEGER as amount\n",
    "FROM range(10000);\n",
    "\n",
    "-- Escrever para S3\n",
    "COPY sales TO 's3://my-bucket/sales.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Escrever resultado de uma query diretamente\n",
    "COPY (\n",
    "    SELECT\n",
    "        product,\n",
    "        sum(amount) as total_sales,\n",
    "        count(*) as transactions\n",
    "    FROM sales\n",
    "    GROUP BY product\n",
    ") TO 's3://my-bucket/sales_summary.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Formato Parquet com compressão Snappy (padrão)\n",
    "COPY sales TO 's3://my-bucket/data.parquet' (FORMAT parquet);\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Escrever como CSV\n",
    "COPY sales TO 's3://my-bucket/data.csv' (\n",
    "    FORMAT csv,\n",
    "    HEADER true,\n",
    "    DELIMITER ','\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Escrever como JSON\n",
    "COPY sales TO 's3://my-bucket/data.json' (FORMAT json);\n",
    "\n",
    "-- JSON Lines (NDJSON)\n",
    "COPY sales TO 's3://my-bucket/data.jsonl' (\n",
    "    FORMAT json,\n",
    "    ARRAY false\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Compressão Snappy (padrão, não precisa especificar)\n",
    "COPY (SELECT * FROM tbl) TO 's3://my-bucket/result-snappy.parquet' (\n",
    "    FORMAT parquet\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Compressão Zstd (melhor taxa de compressão)\n",
    "COPY (FROM generate_series(100_000)) TO 's3://my-bucket/test.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd,\n",
    "    ROW_GROUP_SIZE 100_000\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Compressão LZ4 (mais rápida)\n",
    "COPY (FROM generate_series(100_000)) TO 's3://my-bucket/result-lz4.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION lz4\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Compressão Brotli (máxima compressão)\n",
    "COPY (FROM generate_series(100_000)) TO 's3://my-bucket/result-brotli.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION brotli\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Sem compressão\n",
    "COPY 'test.csv' TO 's3://my-bucket/result-uncompressed.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION uncompressed\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Particionar por uma coluna\n",
    "COPY sales TO 's3://my-bucket/partitioned' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year)\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Particionar por múltiplas colunas\n",
    "COPY sales TO 's3://my-bucket/partitioned' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month)\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Criar tabela com dados temporais\n",
    "CREATE TABLE transactions AS\n",
    "SELECT\n",
    "    range as id,\n",
    "    DATE '2024-01-01' + INTERVAL (range % 365) DAY as date,\n",
    "    (random() * 1000)::INTEGER as amount\n",
    "FROM range(100000);\n",
    "\n",
    "-- Adicionar colunas de particionamento\n",
    "ALTER TABLE transactions ADD COLUMN year INTEGER;\n",
    "ALTER TABLE transactions ADD COLUMN month INTEGER;\n",
    "\n",
    "UPDATE transactions\n",
    "SET\n",
    "    year = EXTRACT(year FROM date),\n",
    "    month = EXTRACT(month FROM date);\n",
    "\n",
    "-- Escrever particionado\n",
    "COPY transactions TO 's3://my-bucket/transactions' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month),\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Sobrescrever arquivos existentes\n",
    "COPY table TO 's3://my-bucket/partitioned' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month),\n",
    "    OVERWRITE_OR_IGNORE true\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Definir tamanho do row group (padrão: 122880 linhas)\n",
    "COPY large_table TO 's3://my-bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 100_000\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Row groups menores para melhor pruning\n",
    "COPY events TO 's3://my-bucket/events.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 50_000,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Adicionar metadados ao arquivo Parquet\n",
    "COPY (\n",
    "    SELECT 42 AS number, true AS is_even\n",
    ") TO 's3://my-bucket/kv_metadata.parquet' (\n",
    "    FORMAT parquet,\n",
    "    KV_METADATA {\n",
    "        number: 'Answer to life, universe, and everything',\n",
    "        is_even: 'not ''odd'''\n",
    "    }\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Verificar metadados escritos\n",
    "SELECT * FROM parquet_kv_metadata('s3://my-bucket/kv_metadata.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Configurar tamanho do dictionary page para strings\n",
    "COPY lineitem TO 's3://my-bucket/lineitem-custom-dict.parquet' (\n",
    "    FORMAT parquet,\n",
    "    STRING_DICTIONARY_PAGE_SIZE_LIMIT 100_000\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Configurar secret com KMS key\n",
    "CREATE OR REPLACE SECRET encrypted_secret (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    CHAIN 'config',\n",
    "    REGION 'eu-west-1',\n",
    "    KMS_KEY_ID 'arn:aws:kms:eu-west-1:123456789:key/abcd-1234-5678',\n",
    "    SCOPE 's3://encrypted-bucket'\n",
    ");\n",
    "\n",
    "-- Escrever dados (serão criptografados automaticamente)\n",
    "COPY sensitive_data TO 's3://encrypted-bucket/data.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Configurar parâmetros de upload (via SET)\n",
    "SET s3_uploader_max_parts_per_file = 10000;\n",
    "SET s3_uploader_max_filesize = '5GB';\n",
    "SET s3_uploader_thread_limit = 50;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Escrever tabela muito grande (multipart upload automático)\n",
    "COPY huge_table TO 's3://my-bucket/huge_data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Primeira escrita\n",
    "COPY (\n",
    "    SELECT * FROM transactions\n",
    "    WHERE date >= '2024-01-01' AND date < '2024-02-01'\n",
    ") TO 's3://my-bucket/transactions' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month)\n",
    ");\n",
    "\n",
    "-- Segunda escrita (diferentes partições, não sobrescreve)\n",
    "COPY (\n",
    "    SELECT * FROM transactions\n",
    "    WHERE date >= '2024-02-01' AND date < '2024-03-01'\n",
    ") TO 's3://my-bucket/transactions' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month)\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Escrever com timestamp no nome do arquivo\n",
    "COPY batch_data TO 's3://my-bucket/data/batch_' || current_timestamp() || '.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "COPY sales TO 's3://my-bucket/sales.csv' (\n",
    "    FORMAT csv,\n",
    "    HEADER true,\n",
    "    DELIMITER ',',\n",
    "    QUOTE '\"'\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "COPY sales TO 's3://my-bucket/sales.tsv' (\n",
    "    FORMAT csv,\n",
    "    HEADER true,\n",
    "    DELIMITER '\\t'\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "COPY sales TO 's3://my-bucket/sales.csv.gz' (\n",
    "    FORMAT csv,\n",
    "    COMPRESSION gzip\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ✅ Parquet para dados analíticos\n",
    "COPY analytics_data TO 's3://my-bucket/data.parquet' (FORMAT parquet);\n",
    "\n",
    "-- ✅ CSV para interoperabilidade\n",
    "COPY export_data TO 's3://my-bucket/export.csv' (FORMAT csv);\n",
    "\n",
    "-- ✅ JSON para dados semi-estruturados\n",
    "COPY logs TO 's3://my-bucket/logs.jsonl' (FORMAT json);\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ✅ zstd para dados que serão lidos frequentemente\n",
    "COPY hot_data TO 's3://my-bucket/hot.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\n",
    "-- ✅ brotli para dados arquivados\n",
    "COPY archive_data TO 's3://my-bucket/archive.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION brotli\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ✅ Particionar por data para queries temporais\n",
    "COPY large_dataset TO 's3://my-bucket/data' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month, day)\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ✅ Ajustar row group para padrão de acesso\n",
    "COPY data TO 's3://my-bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 100_000,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Verifique se o bucket existe e você tem acesso\n",
    "SELECT which_secret('s3://my-bucket/test.parquet', 's3');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Aumentar timeout e threads\n",
    "SET s3_uploader_thread_limit = 10;\n",
    "SET http_timeout = 120000;  -- 2 minutos\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- 1. Criar tabela de teste\n",
    "CREATE TABLE test_data AS\n",
    "SELECT\n",
    "    range as id,\n",
    "    'Value_' || range as value\n",
    "FROM range(1000);\n",
    "\n",
    "-- 2. Escrever para S3\n",
    "COPY test_data TO 's3://your-bucket/test.parquet';\n",
    "\n",
    "-- 3. Verificar escrita\n",
    "SELECT count(*) FROM 's3://your-bucket/test.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Comparar tamanhos com diferentes compressões\n",
    "COPY test_data TO 's3://your-bucket/snappy.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION snappy\n",
    ");\n",
    "\n",
    "COPY test_data TO 's3://your-bucket/zstd.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\n",
    "-- Verificar tamanhos via AWS CLI ou console\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Criar dados com múltiplas categorias\n",
    "CREATE TABLE sales_data AS\n",
    "SELECT\n",
    "    range as id,\n",
    "    DATE '2024-01-01' + INTERVAL (range % 90) DAY as date,\n",
    "    'Region_' || (range % 3) as region,\n",
    "    (random() * 1000)::INTEGER as amount\n",
    "FROM range(10000);\n",
    "\n",
    "-- Adicionar colunas de partição\n",
    "ALTER TABLE sales_data ADD COLUMN year INTEGER;\n",
    "ALTER TABLE sales_data ADD COLUMN month INTEGER;\n",
    "\n",
    "UPDATE sales_data\n",
    "SET\n",
    "    year = EXTRACT(year FROM date),\n",
    "    month = EXTRACT(month FROM date);\n",
    "\n",
    "-- Escrever particionado\n",
    "COPY sales_data TO 's3://your-bucket/partitioned' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month)\n",
    ");\n",
    "\n",
    "-- Ler apenas uma partição\n",
    "SELECT count(*)\n",
    "FROM 's3://your-bucket/partitioned/year=2024/month=1/*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}