{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 07 Trabalhando Parquet S3\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_07_trabalhando_parquet_s3\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_07_trabalhando_parquet_s3\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Leitura básica\n",
    "SELECT * FROM 'test.parquet';\n",
    "\n",
    "-- Extensão alternativa (.parq)\n",
    "SELECT * FROM read_parquet('test.parq');\n",
    "\n",
    "-- Lista de arquivos\n",
    "SELECT * FROM read_parquet(['file1.parquet', 'file2.parquet', 'file3.parquet']);\n",
    "\n",
    "-- Glob pattern\n",
    "SELECT * FROM 'test/*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ver origem dos dados\n",
    "SELECT *, filename\n",
    "FROM read_parquet('test/*.parquet', filename = true);\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT * FROM parquet_metadata('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT * FROM parquet_schema('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT * FROM parquet_file_metadata('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT * FROM parquet_kv_metadata('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Snappy (padrão, bom balanço)\n",
    "COPY (SELECT * FROM tbl) TO 's3://my-bucket/result-snappy.parquet' (\n",
    "    FORMAT parquet\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Zstd: melhor taxa de compressão com boa velocidade\n",
    "COPY (FROM generate_series(100_000)) TO 's3://my-bucket/test.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd,\n",
    "    ROW_GROUP_SIZE 100_000\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- LZ4: compressão muito rápida\n",
    "COPY (FROM generate_series(100_000)) TO 's3://my-bucket/result-lz4.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION lz4\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Brotli: máxima compressão\n",
    "COPY (FROM generate_series(100_000)) TO 's3://my-bucket/result-brotli.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION brotli\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Uncompressed: sem compressão\n",
    "COPY 'test.csv' TO 's3://my-bucket/result-uncompressed.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION uncompressed\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Definir tamanho do row group\n",
    "COPY large_table TO 's3://my-bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 100_000\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Dados que serão frequentemente filtrados por timestamp\n",
    "COPY events TO 's3://my-bucket/events.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 50_000,  -- Menor para melhor pruning\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\n",
    "-- Query otimizada\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/events.parquet'\n",
    "WHERE timestamp BETWEEN '2024-01-01' AND '2024-01-02';\n",
    "-- DuckDB pode pular row groups inteiros baseado em min/max stats\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Adicionar metadados descritivos\n",
    "COPY (\n",
    "    SELECT 42 AS number, true AS is_even\n",
    ") TO 's3://my-bucket/kv_metadata.parquet' (\n",
    "    FORMAT parquet,\n",
    "    KV_METADATA {\n",
    "        number: 'Answer to life, universe, and everything',\n",
    "        is_even: 'not ''odd'''\n",
    "    }\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT * FROM parquet_kv_metadata('s3://my-bucket/kv_metadata.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Adicionar metadados de processamento\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM raw_data\n",
    "    WHERE date = current_date()\n",
    ") TO 's3://processed-bucket/daily_' || current_date() || '.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd,\n",
    "    KV_METADATA {\n",
    "        etl_version: '2.1.0',\n",
    "        processed_at: current_timestamp()::VARCHAR,\n",
    "        source_table: 'raw_data',\n",
    "        record_count: (SELECT count(*) FROM raw_data WHERE date = current_date())::VARCHAR\n",
    "    }\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Aumentar tamanho do dictionary\n",
    "COPY lineitem TO 's3://my-bucket/lineitem-custom-dict.parquet' (\n",
    "    FORMAT parquet,\n",
    "    STRING_DICTIONARY_PAGE_SIZE_LIMIT 100_000\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ❌ Baixa todas as 50 colunas\n",
    "SELECT * FROM 's3://my-bucket/wide_table.parquet';\n",
    "\n",
    "-- ✅ Baixa apenas 3 colunas\n",
    "SELECT id, name, amount\n",
    "FROM 's3://my-bucket/wide_table.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Tabela com 100 colunas, 1M linhas, 500MB\n",
    "\n",
    "-- Query 1: SELECT * (baixa 500MB)\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/large.parquet'\n",
    "WHERE id = 12345;\n",
    "-- Tempo: ~8 segundos\n",
    "\n",
    "-- Query 2: SELECT específico (baixa ~5MB)\n",
    "SELECT id, name, email\n",
    "FROM 's3://my-bucket/large.parquet'\n",
    "WHERE id = 12345;\n",
    "-- Tempo: ~0.3 segundos\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Parquet tem estatísticas por row group:\n",
    "-- Row Group 1: timestamp min=2024-01-01, max=2024-01-15\n",
    "-- Row Group 2: timestamp min=2024-01-16, max=2024-01-31\n",
    "-- Row Group 3: timestamp min=2024-02-01, max=2024-02-15\n",
    "\n",
    "-- Query com filtro\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/events.parquet'\n",
    "WHERE timestamp = '2024-02-05';\n",
    "\n",
    "-- DuckDB pula Row Groups 1 e 2 (fora do intervalo)\n",
    "-- Lê apenas Row Group 3\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Dados ordenados por timestamp\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM events\n",
    "    ORDER BY timestamp\n",
    ") TO 's3://my-bucket/events_sorted.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 100_000\n",
    ");\n",
    "\n",
    "-- Queries com filtro temporal são muito eficientes\n",
    "SELECT count(*)\n",
    "FROM 's3://my-bucket/events_sorted.parquet'\n",
    "WHERE timestamp >= '2024-01-15' AND timestamp < '2024-01-16';\n",
    "-- Muito rápido graças ao row group pruning\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- file1.parquet: id, name, age\n",
    "-- file2.parquet: id, name, salary, department\n",
    "\n",
    "SELECT *\n",
    "FROM read_parquet(\n",
    "    's3://my-bucket/*.parquet',\n",
    "    union_by_name = true\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- DuckDB detecta automaticamente partições Hive\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/data/**/*.parquet'\n",
    "WHERE year = 2024 AND month = 1;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM read_parquet(\n",
    "    's3://my-bucket/data/**/*.parquet',\n",
    "    hive_partitioning = true\n",
    ")\n",
    "WHERE year = 2024;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Escrever com particionamento Hive\n",
    "COPY sales TO 's3://my-bucket/sales' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month, day)\n",
    ");\n",
    "\n",
    "-- Estrutura resultante:\n",
    "-- s3://my-bucket/sales/\n",
    "--   year=2024/\n",
    "--     month=1/\n",
    "--       day=1/\n",
    "--         data_0.parquet\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ver estatísticas de todos os arquivos em um diretório\n",
    "SELECT\n",
    "    file_name,\n",
    "    sum(num_values) as total_values,\n",
    "    sum(total_compressed_size) as compressed_bytes,\n",
    "    sum(total_uncompressed_size) as uncompressed_bytes,\n",
    "    sum(total_uncompressed_size) / sum(total_compressed_size) as compression_ratio\n",
    "FROM parquet_metadata('s3://my-bucket/data/*.parquet')\n",
    "GROUP BY file_name\n",
    "ORDER BY compressed_bytes DESC;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Identificar arquivos com baixa compressão\n",
    "WITH compression_stats AS (\n",
    "    SELECT\n",
    "        file_name,\n",
    "        sum(total_compressed_size)::DOUBLE / sum(total_uncompressed_size) as ratio\n",
    "    FROM parquet_metadata('s3://my-bucket/**/*.parquet')\n",
    "    GROUP BY file_name\n",
    ")\n",
    "SELECT *\n",
    "FROM compression_stats\n",
    "WHERE ratio > 0.5  -- Menos de 50% de compressão\n",
    "ORDER BY ratio DESC;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Arquivo antigo: id, name\n",
    "-- Arquivo novo: id, name, email, phone\n",
    "\n",
    "-- Ler ambos com union_by_name\n",
    "SELECT *\n",
    "FROM read_parquet(\n",
    "    's3://my-bucket/*.parquet',\n",
    "    union_by_name = true\n",
    ");\n",
    "\n",
    "-- Colunas ausentes aparecem como NULL\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Se houver mudanças de tipo incompatíveis,\n",
    "-- carregar como VARCHAR e converter\n",
    "SELECT\n",
    "    id,\n",
    "    name,\n",
    "    TRY_CAST(problematic_column AS INTEGER) as problematic_column\n",
    "FROM 's3://my-bucket/data.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Dados ordenados melhoram compressão e pruning\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM large_table\n",
    "    ORDER BY timestamp, category\n",
    ") TO 's3://my-bucket/optimized.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd,\n",
    "    ROW_GROUP_SIZE 100_000\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Se queries frequentemente filtram por data e região\n",
    "COPY sales TO 's3://my-bucket/sales' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month, region)\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Hot data (acesso frequente): snappy ou lz4\n",
    "COPY hot_data TO 's3://hot-bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION lz4\n",
    ");\n",
    "\n",
    "-- Warm data (acesso ocasional): zstd\n",
    "COPY warm_data TO 's3://warm-bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\n",
    "-- Cold data (arquivamento): brotli\n",
    "COPY cold_data TO 's3://archive-bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION brotli\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- 1. Criar e escrever arquivo\n",
    "CREATE TABLE sample AS SELECT range as id, 'Value_' || range as name FROM range(10000);\n",
    "COPY sample TO 's3://your-bucket/sample.parquet';\n",
    "\n",
    "-- 2. Ver schema\n",
    "SELECT * FROM parquet_schema('s3://your-bucket/sample.parquet');\n",
    "\n",
    "-- 3. Ver metadados\n",
    "SELECT * FROM parquet_metadata('s3://your-bucket/sample.parquet');\n",
    "\n",
    "-- 4. Ver info do arquivo\n",
    "SELECT * FROM parquet_file_metadata('s3://your-bucket/sample.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Criar tabela de teste\n",
    "CREATE TABLE test AS SELECT range as id, random() as value FROM range(100000);\n",
    "\n",
    "-- Escrever com diferentes compressões\n",
    "COPY test TO 's3://your-bucket/snappy.parquet' (FORMAT parquet, COMPRESSION snappy);\n",
    "COPY test TO 's3://your-bucket/zstd.parquet' (FORMAT parquet, COMPRESSION zstd);\n",
    "COPY test TO 's3://your-bucket/lz4.parquet' (FORMAT parquet, COMPRESSION lz4);\n",
    "COPY test TO 's3://your-bucket/brotli.parquet' (FORMAT parquet, COMPRESSION brotli);\n",
    "\n",
    "-- Comparar tamanhos\n",
    "SELECT\n",
    "    file_name,\n",
    "    sum(total_compressed_size) / 1024 / 1024 as size_mb,\n",
    "    sum(total_uncompressed_size) / sum(total_compressed_size) as compression_ratio\n",
    "FROM parquet_metadata('s3://your-bucket/*.parquet')\n",
    "GROUP BY file_name;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- 1. Criar dados com datas\n",
    "CREATE TABLE events AS\n",
    "SELECT\n",
    "    range as id,\n",
    "    DATE '2024-01-01' + INTERVAL (range % 90) DAY as date,\n",
    "    'Event_' || range as description\n",
    "FROM range(10000);\n",
    "\n",
    "-- 2. Adicionar colunas de partição\n",
    "ALTER TABLE events ADD COLUMN year INTEGER;\n",
    "ALTER TABLE events ADD COLUMN month INTEGER;\n",
    "UPDATE events SET year = EXTRACT(year FROM date), month = EXTRACT(month FROM date);\n",
    "\n",
    "-- 3. Escrever particionado\n",
    "COPY events TO 's3://your-bucket/events' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month)\n",
    ");\n",
    "\n",
    "-- 4. Ler partição específica\n",
    "SELECT count(*) FROM 's3://your-bucket/events/year=2024/month=1/*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}