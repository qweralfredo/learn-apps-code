{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f034b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Cap√≠tulo 07: Trabalhando com Parquet no S3 ---\n",
      "\n",
      "--- 1. Globbing de Arquivos ---\n",
      "Total rows: 5\n",
      "\n",
      "--- 2. Hive Partitioning (Auto Discovery) ---\n",
      "   id  val  year month\n",
      "0   1  100  2023    01\n",
      "1   2  200  2023    01\n",
      "2   3  300  2023    02\n",
      "3   4  400  2024    01\n",
      "4   5  500  2024    01\n",
      "\n",
      "--- 3. Filename Metadata ---\n",
      "   id                                           filename\n",
      "0   1  s3://learn-duckdb-s3/data_ch07/year=2023/month...\n",
      "1   2  s3://learn-duckdb-s3/data_ch07/year=2023/month...\n",
      "\n",
      "--- 4. Filter Pushdown (Explicado) ---\n",
      "DuckDB usa os filtros na query para pular parti√ß√µes inteiras.\n",
      "Query com filtro executada.\n",
      "Linhas em 2024: 2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_07_trabalhando_parquet_s3\n",
    "\"\"\"\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# ==============================================================================\n",
    "# SETUP MINIO\n",
    "# ==============================================================================\n",
    "print(f\"--- Iniciando Cap√≠tulo 07: Trabalhando com Parquet no S3 ---\")\n",
    "\n",
    "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "BUCKET_NAME = \"learn-duckdb-s3\"\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=BUCKET_NAME)\n",
    "except ClientError:\n",
    "    pass\n",
    "\n",
    "# Helper\n",
    "def upload_df(df, key):\n",
    "    local_name = \"temp.parquet\"\n",
    "    df.to_parquet(local_name)\n",
    "    s3_client.upload_file(local_name, BUCKET_NAME, key)\n",
    "    os.remove(local_name)\n",
    "\n",
    "# Create Partitioned Data\n",
    "# year=2023/month=01/file.parquet\n",
    "df1 = pd.DataFrame({'id': [1, 2], 'val': [100, 200]})\n",
    "upload_df(df1, \"data_ch07/year=2023/month=01/part1.parquet\")\n",
    "\n",
    "# year=2023/month=02/file.parquet\n",
    "df2 = pd.DataFrame({'id': [3], 'val': [300]})\n",
    "upload_df(df2, \"data_ch07/year=2023/month=02/part1.parquet\")\n",
    "\n",
    "# year=2024/month=01/file.parquet\n",
    "df3 = pd.DataFrame({'id': [4, 5], 'val': [400, 500]})\n",
    "upload_df(df3, \"data_ch07/year=2024/month=01/part1.parquet\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EXEMPLOS DUCKDB\n",
    "# ==============================================================================\n",
    "con = duckdb.connect(database=':memory:')\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE SECRET secret_minio (\n",
    "    TYPE S3,\n",
    "    KEY_ID '{MINIO_ACCESS_KEY}',\n",
    "    SECRET '{MINIO_SECRET_KEY}',\n",
    "    REGION 'us-east-1',\n",
    "    ENDPOINT '{MINIO_ENDPOINT.replace(\"http://\", \"\")}',\n",
    "    URL_STYLE 'path',\n",
    "    USE_SSL 'false'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- 1. Globbing de Arquivos ---\")\n",
    "# Query All\n",
    "res = con.execute(f\"SELECT count(*) FROM 's3://{BUCKET_NAME}/data_ch07/**/*.parquet'\").fetchall()\n",
    "print(f\"Total rows: {res[0][0]}\")\n",
    "\n",
    "print(\"\\n--- 2. Hive Partitioning (Auto Discovery) ---\")\n",
    "# DuckDB consegue inferir year e month como colunas\n",
    "res = con.execute(f\"\"\"\n",
    "    SELECT id, val, year, month \n",
    "    FROM read_parquet('s3://{BUCKET_NAME}/data_ch07/**/*.parquet', hive_partitioning=1)\n",
    "    ORDER BY id\n",
    "\"\"\").df()\n",
    "print(res)\n",
    "\n",
    "print(\"\\n--- 3. Filename Metadata ---\")\n",
    "# Saber qual arquivo originou o dado\n",
    "res = con.execute(f\"\"\"\n",
    "    SELECT id, filename \n",
    "    FROM read_parquet('s3://{BUCKET_NAME}/data_ch07/**/*.parquet', filename=true)\n",
    "    LIMIT 2\n",
    "\"\"\").df()\n",
    "print(res)\n",
    "\n",
    "print(\"\\n--- 4. Filter Pushdown (Explicado) ---\")\n",
    "print(\"DuckDB usa os filtros na query para pular parti√ß√µes inteiras.\")\n",
    "query = f\"SELECT count(*) FROM read_parquet('s3://{BUCKET_NAME}/data_ch07/**/*.parquet', hive_partitioning=1) WHERE year='2024'\"\n",
    "explain = con.execute(f\"EXPLAIN {query}\").fetchall()\n",
    "# Printar explain √© muito verborr√°gico, mas podemos confirmar que executa\n",
    "print(\"Query com filtro executada.\")\n",
    "count_2024 = con.execute(query).fetchone()[0]\n",
    "print(f\"Linhas em 2024: {count_2024}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e2802",
   "metadata": {},
   "source": [
    "## üì¶ Instala√ß√£o de Depend√™ncias\n",
    "\n",
    "Instale as bibliotecas necess√°rias para este notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 07 Trabalhando Parquet S3\n",
    "\n",
    "Notebook gerado automaticamente a partir do c√≥digo fonte python.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
