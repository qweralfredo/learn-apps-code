{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 10 Otimizacao Boas Praticas\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_10_otimizacao_boas_praticas\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_10_otimizacao_boas_praticas\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Baixa TODAS as colunas (desperdício de banda e tempo)\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/wide_table.parquet'\n",
    "WHERE id = 12345;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Baixa apenas colunas necessárias\n",
    "SELECT id, name, email, created_at\n",
    "FROM 's3://my-bucket/wide_table.parquet'\n",
    "WHERE id = 12345;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Escrever dados ordenados\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM transactions\n",
    "    ORDER BY timestamp, customer_id\n",
    ") TO 's3://my-bucket/transactions.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 100_000,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Muito rápido - lê apenas metadados\n",
    "SELECT count(*) FROM 's3://my-bucket/large.parquet';\n",
    "\n",
    "-- Também rápido - lê apenas uma coluna\n",
    "SELECT count(id) FROM 's3://my-bucket/large.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Lista TODO o bucket (pode ser muito lento)\n",
    "SELECT * FROM 's3://huge-bucket/**/*.parquet'\n",
    "WHERE date = '2024-01-15';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Glob específico (lista menos objetos)\n",
    "SELECT * FROM 's3://huge-bucket/2024/01/15/*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Query eficiente com partições\n",
    "SELECT *\n",
    "FROM 's3://bucket/data/**/*.parquet'\n",
    "WHERE year = 2024 AND month = 1 AND day = 15;\n",
    "\n",
    "-- DuckDB lê apenas a partição específica\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Snappy ou LZ4: rápido, boa compressão\n",
    "COPY hot_data TO 's3://bucket/hot.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION snappy  -- ou lz4\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Zstd: melhor compressão, boa velocidade\n",
    "COPY warm_data TO 's3://bucket/warm.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Brotli: máxima compressão\n",
    "COPY cold_data TO 's3://bucket/archive.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION brotli\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Row groups menores = melhor pruning\n",
    "COPY filtered_data TO 's3://bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 50_000,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Row groups maiores = melhor compressão\n",
    "COPY bulk_data TO 's3://bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    ROW_GROUP_SIZE 500_000,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Se queries frequentemente filtram por data e região\n",
    "COPY sales TO 's3://bucket/sales' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (year, month, region),\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "COPY product_events TO 's3://bucket/events.parquet' (\n",
    "    FORMAT parquet,\n",
    "    STRING_DICTIONARY_PAGE_SIZE_LIMIT 100_000,\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Usa AWS credential chain (ENV, EC2 role, etc)\n",
    "CREATE PERSISTENT SECRET production (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- NÃO faça isso em produção\n",
    "CREATE SECRET bad_practice (\n",
    "    TYPE s3,\n",
    "    PROVIDER config,\n",
    "    KEY_ID 'AKIAIOSFODNN7EXAMPLE',  -- Exposto em código\n",
    "    SECRET 'wJalrXUtnFEMI/K7MDENG'  -- Risco de segurança\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE PERSISTENT SECRET finance_data (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    SCOPE 's3://finance-bucket/sensitive'\n",
    ");\n",
    "\n",
    "CREATE PERSISTENT SECRET public_data (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    SCOPE 's3://public-bucket'\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE PERSISTENT SECRET personal (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Arquivo fica sem criptografia em ~/.duckdb/stored_secrets\n",
    "-- Risco em ambientes compartilhados\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE SECRET temp_creds (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    REFRESH auto\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ajustar número de threads (padrão: número de CPUs)\n",
    "SET threads = 8;\n",
    "\n",
    "-- Para downloads de S3\n",
    "SET s3_uploader_thread_limit = 10;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Aumentar timeout para arquivos grandes\n",
    "SET http_timeout = 300000;  -- 5 minutos\n",
    "\n",
    "-- Para redes lentas\n",
    "SET http_timeout = 600000;  -- 10 minutos\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Limitar memória (útil em ambientes com recursos limitados)\n",
    "SET memory_limit = '4GB';\n",
    "\n",
    "-- Para máquinas com mais RAM\n",
    "SET memory_limit = '32GB';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Extract, Transform, Load em um comando\n",
    "COPY (\n",
    "    SELECT\n",
    "        id,\n",
    "        upper(name) as name,\n",
    "        amount * 1.1 as amount_with_tax,\n",
    "        current_timestamp() as processed_at\n",
    "    FROM 's3://raw-bucket/input/**/*.parquet'\n",
    "    WHERE date >= '2024-01-01'\n",
    "      AND status = 'active'\n",
    ") TO 's3://processed-bucket/output/' || current_date() || '.parquet' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (region),\n",
    "    COMPRESSION zstd,\n",
    "    KV_METADATA {\n",
    "        etl_version: '2.0',\n",
    "        processed_at: current_timestamp()::VARCHAR\n",
    "    }\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Tabela de controle (local ou em S3)\n",
    "CREATE TABLE IF NOT EXISTS watermark (\n",
    "    last_processed_date DATE\n",
    ");\n",
    "\n",
    "-- Processar dados incrementais\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM 's3://source-bucket/**/*.parquet'\n",
    "    WHERE date > (SELECT max(last_processed_date) FROM watermark)\n",
    ") TO 's3://target-bucket/incremental/' || current_date() || '.parquet';\n",
    "\n",
    "-- Atualizar watermark\n",
    "UPDATE watermark SET last_processed_date = current_date();\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Bronze → Silver\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM 's3://bronze-bucket/raw/**/*.parquet'\n",
    "    WHERE is_valid(data)  -- Validação\n",
    ") TO 's3://silver-bucket/cleaned/' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (date)\n",
    ");\n",
    "\n",
    "-- Silver → Gold\n",
    "COPY (\n",
    "    SELECT\n",
    "        date,\n",
    "        category,\n",
    "        sum(amount) as total,\n",
    "        count(*) as transactions\n",
    "    FROM 's3://silver-bucket/cleaned/**/*.parquet'\n",
    "    GROUP BY date, category\n",
    ") TO 's3://gold-bucket/aggregated/' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (date)\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Baixa apenas 3 colunas ao invés de 100\n",
    "SELECT id, name, amount\n",
    "FROM 's3://bucket/wide_table.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Dados acessados frequentemente\n",
    "CREATE SECRET r2_hot (\n",
    "    TYPE r2,\n",
    "    KEY_ID 'key',\n",
    "    SECRET 'secret',\n",
    "    ACCOUNT_ID 'account'\n",
    ");\n",
    "\n",
    "-- Ler do R2 sem custos de saída\n",
    "SELECT * FROM 'r2://hot-data/**/*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "COPY archive_data TO 's3://archive-bucket/data.parquet' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION brotli  -- Máxima compressão\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ver plano de execução\n",
    "EXPLAIN SELECT *\n",
    "FROM 's3://bucket/data.parquet'\n",
    "WHERE date = '2024-01-15';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Comparar compressed vs uncompressed\n",
    "SELECT\n",
    "    file_name,\n",
    "    sum(total_compressed_size) / 1024 / 1024 as compressed_mb,\n",
    "    sum(total_uncompressed_size) / 1024 / 1024 as uncompressed_mb,\n",
    "    sum(total_uncompressed_size) / sum(total_compressed_size) as ratio\n",
    "FROM parquet_metadata('s3://bucket/**/*.parquet')\n",
    "GROUP BY file_name;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Habilitar profiling\n",
    "PRAGMA enable_profiling = 'json';\n",
    "PRAGMA profiling_output = '/tmp/profile.json';\n",
    "\n",
    "-- Executar query\n",
    "SELECT * FROM 's3://bucket/data.parquet' WHERE ...;\n",
    "\n",
    "-- Analisar profile.json\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE SECRET encrypted (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    KMS_KEY_ID 'arn:aws:kms:region:account:key/id',\n",
    "    SCOPE 's3://sensitive-bucket'\n",
    ");\n",
    "\n",
    "-- Dados escritos são criptografados automaticamente\n",
    "COPY sensitive_data TO 's3://sensitive-bucket/data.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- 1. Configurar secrets por ambiente\n",
    "CREATE PERSISTENT SECRET prod_raw (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    CHAIN 'env;config',\n",
    "    SCOPE 's3://prod-raw-data',\n",
    "    REFRESH auto\n",
    ");\n",
    "\n",
    "CREATE PERSISTENT SECRET prod_processed (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    SCOPE 's3://prod-processed-data',\n",
    "    KMS_KEY_ID 'arn:aws:kms:us-east-1:123456789:key/abcd',\n",
    "    REFRESH auto\n",
    ");\n",
    "\n",
    "-- 2. Configurar performance\n",
    "SET threads = 16;\n",
    "SET memory_limit = '64GB';\n",
    "SET s3_uploader_thread_limit = 20;\n",
    "\n",
    "-- 3. Pipeline ETL otimizado\n",
    "COPY (\n",
    "    SELECT\n",
    "        -- Column pruning: apenas colunas necessárias\n",
    "        id,\n",
    "        customer_id,\n",
    "        product_id,\n",
    "        amount,\n",
    "        timestamp,\n",
    "        -- Enriquecimento\n",
    "        EXTRACT(year FROM timestamp) as year,\n",
    "        EXTRACT(month FROM timestamp) as month,\n",
    "        EXTRACT(day FROM timestamp) as day,\n",
    "        -- Validação\n",
    "        CASE\n",
    "            WHEN amount > 0 AND amount < 1000000 THEN 'valid'\n",
    "            ELSE 'invalid'\n",
    "        END as validation_status\n",
    "    FROM 's3://prod-raw-data/transactions/**/*.parquet'\n",
    "    WHERE\n",
    "        -- Filtro temporal\n",
    "        timestamp >= current_date() - INTERVAL '1 day'\n",
    "        AND timestamp < current_date()\n",
    "        -- Filtro de qualidade\n",
    "        AND customer_id IS NOT NULL\n",
    "        AND amount > 0\n",
    "    -- Ordenação para melhor compression e pruning\n",
    "    ORDER BY timestamp, customer_id\n",
    ") TO 's3://prod-processed-data/transactions/' || current_date() || '/' (\n",
    "    FORMAT parquet,\n",
    "    COMPRESSION zstd,\n",
    "    ROW_GROUP_SIZE 100_000,\n",
    "    PARTITION_BY (year, month, day),\n",
    "    OVERWRITE_OR_IGNORE true,\n",
    "    KV_METADATA {\n",
    "        pipeline_version: '3.1.0',\n",
    "        processed_at: current_timestamp()::VARCHAR,\n",
    "        source_bucket: 'prod-raw-data',\n",
    "        row_count: (\n",
    "            SELECT count(*)::VARCHAR\n",
    "            FROM 's3://prod-raw-data/transactions/**/*.parquet'\n",
    "            WHERE timestamp >= current_date() - INTERVAL '1 day'\n",
    "        )\n",
    "    }\n",
    ");\n",
    "\n",
    "-- 4. Validação\n",
    "SELECT\n",
    "    'Processed Records: ' || count(*) as summary\n",
    "FROM 's3://prod-processed-data/transactions/' || current_date() || '/**/*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- 1. Setup\n",
    "CREATE PERSISTENT SECRET my_project (TYPE s3, PROVIDER credential_chain);\n",
    "SET threads = 8;\n",
    "SET memory_limit = '16GB';\n",
    "\n",
    "-- 2. Ingestão (Bronze)\n",
    "COPY raw_events TO 's3://my-project/bronze/events/' || current_date() || '.parquet';\n",
    "\n",
    "-- 3. Limpeza (Silver)\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM 's3://my-project/bronze/events/**/*.parquet'\n",
    "    WHERE is_valid(event_data)\n",
    ") TO 's3://my-project/silver/events/' (\n",
    "    FORMAT parquet,\n",
    "    PARTITION_BY (date),\n",
    "    COMPRESSION zstd\n",
    ");\n",
    "\n",
    "-- 4. Agregação (Gold)\n",
    "COPY (\n",
    "    SELECT\n",
    "        date,\n",
    "        event_type,\n",
    "        count(*) as events,\n",
    "        count(DISTINCT user_id) as unique_users\n",
    "    FROM 's3://my-project/silver/events/**/*.parquet'\n",
    "    GROUP BY date, event_type\n",
    ") TO 's3://my-project/gold/daily_summary.parquet';\n",
    "\n",
    "-- 5. Análise\n",
    "SELECT *\n",
    "FROM 's3://my-project/gold/daily_summary.parquet'\n",
    "ORDER BY date DESC, events DESC;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}