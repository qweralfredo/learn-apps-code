{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f212472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Cap칤tulo 10: Otimiza칞칚o e Boas Pr치ticas ---\n",
      "Configurando Buckets no MinIO...\n",
      "Gerando dados de teste...\n",
      "\n",
      "--- 1. Projection Pushdown (Selecionar poucas colunas) ---\n",
      "   id    name               email\n",
      "0   0  Name_0  user_0@example.com\n",
      "1   1  Name_1  user_1@example.com\n",
      "2   2  Name_2  user_2@example.com\n",
      "3   3  Name_3  user_3@example.com\n",
      "4   4  Name_4  user_4@example.com\n",
      "\n",
      "--- 2. Metadata Reading (Count r치pido) ---\n",
      "Count(*): 100\n",
      "\n",
      "--- 3. Partition Pruning ---\n",
      "Rows in partition: 10\n",
      "\n",
      "--- 4. Compression Formats (Simula칞칚o de Escrita) ---\n",
      "Escrita com Snappy: OK\n",
      "Escrita com Zstd: OK\n",
      "\n",
      "--- 5. Security & Secrets Management (Conceitual) ---\n",
      "Secret 'finance_data' criado com SCOPE.\n",
      "\n",
      "--- 6. Settings Tuning ---\n",
      "Settings ajustadas.\n",
      "\n",
      "--- 7. ETL Pipeline Example ---\n",
      "ETL conclu칤do com sucesso.\n",
      "\n",
      "--- 8. Data Layering (Consumo camadas) ---\n",
      "Simula칞칚o de carga incremental (Query preparada).\n",
      "\n",
      "--- Fim do Cap칤tulo 10 ---\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_10_otimizacao_boas_praticas\n",
    "\"\"\"\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"--- Iniciando Cap칤tulo 10: Otimiza칞칚o e Boas Pr치ticas ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SETUP Simulation (MinIO)\n",
    "# ==============================================================================\n",
    "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "\n",
    "# Buckets needed for examples\n",
    "BUCKETS = [\n",
    "    \"my-bucket\", \"huge-bucket\", \"bucket\", \"raw-bucket\", \n",
    "    \"processed-bucket\", \"source-bucket\", \"target-bucket\", \n",
    "    \"bronze-bucket\", \"silver-bucket\", \"gold-bucket\",\n",
    "    \"finance-bucket\", \"public-bucket\"\n",
    "]\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "print(\"Configurando Buckets no MinIO...\")\n",
    "for b in BUCKETS:\n",
    "    try:\n",
    "        s3_client.create_bucket(Bucket=b)\n",
    "    except ClientError:\n",
    "        pass # Bucket Exists\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create Dummy Data\n",
    "# ------------------------------------------------------------------------------\n",
    "def upload_parquet_df(df, bucket, key):\n",
    "    filename = \"temp_data.parquet\"\n",
    "    df.to_parquet(filename)\n",
    "    s3_client.upload_file(filename, bucket, key)\n",
    "    os.remove(filename)\n",
    "\n",
    "print(\"Gerando dados de teste...\")\n",
    "\n",
    "# 1. Wide Table (Many Columns)\n",
    "df_wide = pd.DataFrame(np.random.randint(0,100,size=(100, 50)), columns=[f'col_{i}' for i in range(50)])\n",
    "df_wide['id'] = range(100)\n",
    "df_wide['name'] = [f\"Name_{i}\" for i in range(100)]\n",
    "df_wide['email'] = [f\"user_{i}@example.com\" for i in range(100)]\n",
    "df_wide['created_at'] = datetime.now()\n",
    "upload_parquet_df(df_wide, \"my-bucket\", \"wide_table.parquet\")\n",
    "\n",
    "# 2. Large Parquet (Just simulating name)\n",
    "upload_parquet_df(df_wide, \"my-bucket\", \"large.parquet\")\n",
    "\n",
    "# 3. Partitioned Data (Simulated)\n",
    "# s3://huge-bucket/2024/01/15/data.parquet\n",
    "df_part = pd.DataFrame({'id': range(10), 'date': '2024-01-15'})\n",
    "upload_parquet_df(df_part, \"huge-bucket\", \"2024/01/15/data.parquet\")\n",
    "\n",
    "# 4. Data for Copy/Compression Tests\n",
    "df_hot = pd.DataFrame({'id': range(1000), 'val': np.random.randn(1000)})\n",
    "upload_parquet_df(df_hot, \"bucket\", \"input_data.parquet\")\n",
    "\n",
    "# 5. Pipeline Data\n",
    "# s3://raw-bucket/input/2024-01-10.parquet\n",
    "df_pipeline = pd.DataFrame({\n",
    "    'id': range(50), \n",
    "    'name': [f\"prod_{i}\" for i in range(50)], \n",
    "    'amount': np.random.rand(50)*100,\n",
    "    'date': '2024-01-10',\n",
    "    'status': 'active',\n",
    "    'region': 'us-east-1'\n",
    "})\n",
    "upload_parquet_df(df_pipeline, \"raw-bucket\", \"input/2024-01-10.parquet\")\n",
    "\n",
    "# ==============================================================================\n",
    "# DUCKDB CONFIG\n",
    "# ==============================================================================\n",
    "con = duckdb.connect(database=':memory:')\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "\n",
    "# Setup Default Secret for MinIO\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE SECRET minio_default (\n",
    "    TYPE S3,\n",
    "    KEY_ID '{MINIO_ACCESS_KEY}',\n",
    "    SECRET '{MINIO_SECRET_KEY}',\n",
    "    ENDPOINT '{MINIO_ENDPOINT.replace(\"http://\", \"\")}',\n",
    "    USE_SSL 'false',\n",
    "    URL_STYLE 'path'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# ==============================================================================\n",
    "# EXAMPLES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- 1. Projection Pushdown (Selecionar poucas colunas) ---\")\n",
    "# Baixa apenas colunas necess치rias\n",
    "res = con.execute(\"\"\"\n",
    "SELECT id, name, email\n",
    "FROM 's3://my-bucket/wide_table.parquet'\n",
    "LIMIT 5;\n",
    "\"\"\").df()\n",
    "print(res)\n",
    "\n",
    "print(\"\\n--- 2. Metadata Reading (Count r치pido) ---\")\n",
    "res = con.execute(\"SELECT count(*) FROM 's3://my-bucket/large.parquet'\").fetchone()\n",
    "print(f\"Count(*): {res[0]}\")\n",
    "\n",
    "print(\"\\n--- 3. Partition Pruning ---\")\n",
    "# Query eficiente com parti칞칫es (DuckDB entende estrutura de diret칩rios data=... se usar hive partitioning,\n",
    "# aqui estamos usando globbing manual ou estrutura de data).\n",
    "# O exemplo original: 's3://huge-bucket/2024/01/15/*.parquet'\n",
    "try:\n",
    "    res = con.execute(\"\"\"\n",
    "    SELECT count(*) FROM 's3://huge-bucket/2024/01/15/*.parquet';\n",
    "    \"\"\").fetchone()\n",
    "    print(f\"Rows in partition: {res[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro partition: {e}\")\n",
    "\n",
    "print(\"\\n--- 4. Compression Formats (Simula칞칚o de Escrita) ---\")\n",
    "# Nota: Compress칚o real depende do conte칰do\n",
    "try:\n",
    "    con.execute(\"\"\"\n",
    "    COPY (SELECT * FROM 's3://bucket/input_data.parquet') \n",
    "    TO 's3://bucket/hot.parquet' (FORMAT parquet, COMPRESSION 'snappy');\n",
    "    \"\"\")\n",
    "    print(\"Escrita com Snappy: OK\")\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "    COPY (SELECT * FROM 's3://bucket/input_data.parquet') \n",
    "    TO 's3://bucket/warm.parquet' (FORMAT parquet, COMPRESSION 'zstd');\n",
    "    \"\"\")\n",
    "    print(\"Escrita com Zstd: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro compress칚o: {e}\")\n",
    "\n",
    "print(\"\\n--- 5. Security & Secrets Management (Conceitual) ---\")\n",
    "# Exemplo de cria칞칚o de segredo (Bad Practice vs Good Practice)\n",
    "# Aqui s칩 criamos para validar sintaxe, apontando para MinIO para n칚o falhar\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE SECRET finance_data (\n",
    "    TYPE S3,\n",
    "    SCOPE 's3://finance-bucket',\n",
    "    KEY_ID '{MINIO_ACCESS_KEY}',\n",
    "    SECRET '{MINIO_SECRET_KEY}',\n",
    "    ENDPOINT '{MINIO_ENDPOINT.replace(\"http://\", \"\")}',\n",
    "    USE_SSL 'false',\n",
    "    URL_STYLE 'path'\n",
    ");\n",
    "\"\"\")\n",
    "print(\"Secret 'finance_data' criado com SCOPE.\")\n",
    "\n",
    "print(\"\\n--- 6. Settings Tuning ---\")\n",
    "con.execute(\"SET threads = 4;\")\n",
    "con.execute(\"SET s3_uploader_thread_limit = 4;\")\n",
    "con.execute(\"SET http_timeout = 30000;\") # ms\n",
    "print(\"Settings ajustadas.\")\n",
    "\n",
    "print(\"\\n--- 7. ETL Pipeline Example ---\")\n",
    "# Leitura e Transforma칞칚o\n",
    "try:\n",
    "    con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT\n",
    "            id,\n",
    "            upper(name) as name,\n",
    "            amount * 1.1 as amount_with_tax,\n",
    "            current_timestamp as processed_at\n",
    "        FROM 's3://raw-bucket/input/**/*.parquet'\n",
    "        WHERE status = 'active'\n",
    "    ) TO 's3://processed-bucket/output_etl.parquet' (\n",
    "        FORMAT parquet,\n",
    "        COMPRESSION zstd\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"ETL conclu칤do com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ETL: {e}\")\n",
    "\n",
    "print(\"\\n--- 8. Data Layering (Consumo camadas) ---\")\n",
    "# Simples query para validar acesso\n",
    "try:\n",
    "    con.execute(\"CREATE TABLE IF NOT EXISTS watermark (last_processed_date DATE);\")\n",
    "    con.execute(\"INSERT INTO watermark VALUES ('2023-01-01');\")\n",
    "    \n",
    "    # Simula칞칚o de Incremental\n",
    "    # Vamos apenas listar o que existiria\n",
    "    print(\"Simula칞칚o de carga incremental (Query preparada).\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro Data Layering: {e}\")\n",
    "\n",
    "print(\"\\n--- Fim do Cap칤tulo 10 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f98a12",
   "metadata": {},
   "source": [
    "## 游닍 Instala칞칚o de Depend칡ncias\n",
    "\n",
    "Instale as bibliotecas necess치rias para este notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 10 Otimizacao Boas Praticas\n",
    "\n",
    "Notebook gerado automaticamente a partir do c칩digo fonte python.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
