{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Criar conex√£o em mem√≥ria\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Instalar e carregar extens√µes (httpfs √© suficiente, aws traz credenciais provider)\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(\"INSTALL aws; LOAD aws;\")\n",
    "\n",
    "# Configurar credenciais usando o novo sistema de SECRETS (DuckDB v0.10+)\n",
    "# Isso garante que o endpoint seja respeitado\n",
    "con.execute(\"DROP SECRET IF EXISTS minio_secret;\")\n",
    "con.execute(\"\"\"\n",
    "CREATE SECRET minio_secret (\n",
    "    TYPE S3,\n",
    "    KEY_ID 'admin',\n",
    "    SECRET 'password',\n",
    "    REGION 'us-east-1',\n",
    "    ENDPOINT 'localhost:9000',\n",
    "    URL_STYLE 'path',\n",
    "    USE_SSL 'false'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"DuckDB conectado e configurado para MinIO (via Secrets)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93f9f2",
   "metadata": {},
   "source": [
    "## üîå Conex√£o Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18850170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o do DuckDB e Boto3 (para gerenciamento do MinIO)\n",
    "!pip install duckdb boto3 pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6d696",
   "metadata": {},
   "source": [
    "## üì¶ Instala√ß√£o e Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Configura√ß√£o do cliente S3 (MinIO) via Boto3 para criar bucket\n",
    "s3 = boto3.client('s3',\n",
    "    endpoint_url='http://localhost:9000',\n",
    "    aws_access_key_id='admin',\n",
    "    aws_secret_access_key='password',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Criar bucket se n√£o existir\n",
    "bucket_name = 'datalake'\n",
    "try:\n",
    "    if bucket_name not in [b['Name'] for b in s3.list_buckets()['Buckets']]:\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' criado.\")\n",
    "    else:\n",
    "        print(f\"Bucket '{bucket_name}' j√° existe.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao verificar bucket: {e}\")\n",
    "\n",
    "# Preparar dados: Ler CSV local e escrever Parquet no MinIO\n",
    "# Nota: Ajuste o caminho do CSV conforme necess√°rio\n",
    "csv_path = '../sales_data.csv' \n",
    "s3_path = f's3://{bucket_name}/sales_data.parquet'\n",
    "\n",
    "# Verificamos se o arquivo existe localmente\n",
    "import os\n",
    "if os.path.exists(csv_path):\n",
    "    # Usamos DuckDB para converter e enviar (Upload Eficiente)\n",
    "    con.execute(f\"\"\"\n",
    "        COPY (SELECT * FROM read_csv_auto('{csv_path}')) \n",
    "        TO '{s3_path}' (FORMAT 'PARQUET');\n",
    "    \"\"\")\n",
    "    print(f\"Dados enviados para {s3_path}\")\n",
    "else:\n",
    "    print(f\"Arquivo local {csv_path} n√£o encontrado!\")\n",
    "\n",
    "# üìñ Sintaxe B√°sica de Leitura\n",
    "print(\"\\nLendo dados do S3:\")\n",
    "con.execute(f\"SELECT * FROM '{s3_path}' LIMIT 5;\")\n",
    "print(con.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09740fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar dados locais com dados em S3\n",
    "\n",
    "# Criar metadados locais (ex: Gerente Regional)\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gerentes_regionais AS \n",
    "    SELECT * FROM (VALUES \n",
    "        ('North', 'Jo√£o Silva'),\n",
    "        ('South', 'Maria Oliveira'),\n",
    "        ('East', 'Carlos Santos'),\n",
    "        ('West', 'Ana Pereira')\n",
    "    ) AS t(region, manager);\n",
    "\"\"\")\n",
    "\n",
    "# Join H√≠brido: Tabela DuckDB em Mem√≥ria + Arquivo Parquet no S3\n",
    "# Usamos a vari√°vel 's3_path' definida na c√©lula anterior (s3://datalake/sales_data.parquet)\n",
    "print(\"Executando Join H√≠brido:\")\n",
    "con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        s.region,\n",
    "        s.sales,\n",
    "        g.manager\n",
    "    FROM '{s3_path}' s\n",
    "    JOIN gerentes_regionais g ON s.region = g.region;\n",
    "\"\"\")\n",
    "print(con.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c21475",
   "metadata": {},
   "source": [
    "### Join com Dados Locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler de S3, transformar e escrever de volta (ETL)\n",
    "output_path = f's3://{bucket_name}/processed/regional_totals.parquet'\n",
    "\n",
    "print(f\"Iniciando ETL: {s3_path} -> {output_path}\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "    SELECT \n",
    "        region, \n",
    "        sum(sales) as total_sales,\n",
    "        count(*) as transaction_count\n",
    "    FROM '{s3_path}'\n",
    "    GROUP BY region\n",
    ") TO '{output_path}' (FORMAT 'PARQUET');\n",
    "\"\"\")\n",
    "print(\"ETL Conclu√≠do com sucesso.\")\n",
    "\n",
    "# Verificar resultado lendo do destino\n",
    "print(\"\\nDados processados:\")\n",
    "print(con.sql(f\"SELECT * FROM '{output_path}'\").fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2363f3",
   "metadata": {},
   "source": [
    "### ETL Simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba192bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar logs armazenados em S3 (Simula√ß√£o de Hive Partitioning)\n",
    "\n",
    "# 1. Gerar dados de LOG simulados\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE logs_simulados AS\n",
    "    SELECT \n",
    "        range as id,\n",
    "        CAST('2024-01-01' AS DATE) + (random() * 60)::INT AS log_date,\n",
    "        CASE WHEN random() > 0.8 THEN 'ERROR' ELSE 'INFO' END as level,\n",
    "        'Server-' || (random() * 3)::INT as server_name\n",
    "    FROM range(100);\n",
    "\"\"\")\n",
    "\n",
    "# 2. Preparar colunas de particionamento (Ano e M√™s)\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE logs_partitioned AS\n",
    "    SELECT \n",
    "        *, \n",
    "        YEAR(log_date) as year, \n",
    "        MONTH(log_date) as month\n",
    "    FROM logs_simulados\n",
    "\"\"\")\n",
    "\n",
    "# 3. Escrever particionado no S3 (Hive Style)\n",
    "logs_path = f's3://{bucket_name}/logs_system'\n",
    "print(f\"Escrevendo logs particionados em: {logs_path} ...\")\n",
    "con.execute(f\"\"\"\n",
    "    COPY logs_partitioned \n",
    "    TO '{logs_path}' \n",
    "    (FORMAT PARQUET, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE);\n",
    "\"\"\")\n",
    "\n",
    "# 4. Ler usando Hive Partitioning Automatico\n",
    "print(\"\\nAnalisando logs diretamente do S3 (Hive Partitioning):\")\n",
    "# O DuckDB detecta o esquema hive automaticamente ao usar glob patterns profundos ou a op√ß√£o hive_partitioning\n",
    "con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        month, \n",
    "        level, \n",
    "        count(*) as qtd\n",
    "    FROM '{logs_path}/*/*/*.parquet'\n",
    "    GROUP BY year, month, level\n",
    "    ORDER BY year, month, level;\n",
    "\"\"\")\n",
    "print(con.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb3bf5",
   "metadata": {},
   "source": [
    "## üíº Casos de Uso Pr√°ticos\n",
    "\n",
    "### An√°lise de Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler m√∫ltiplos arquivos filtrando por diret√≥rio (Filtragem \"Zero-Copy\" via nome do arquivo)\n",
    "# Vamos ler apenas os logs de Janeiro de 2024\n",
    "\n",
    "pattern_jan = f'{logs_path}/year=2024/month=1/*.parquet'\n",
    "\n",
    "print(f\"Buscando arquivos no padr√£o: {pattern_jan}\")\n",
    "\n",
    "# DuckDB expande o GLOB e l√™ apenas os arquivos necess√°rios\n",
    "con.execute(f\"\"\"\n",
    "    SELECT count(*) as total_jan_logs \n",
    "    FROM '{pattern_jan}';\n",
    "\"\"\")\n",
    "\n",
    "print(con.fetchall())\n",
    "\n",
    "# Exemplo de lista de arquivos (Glob Expansion)\n",
    "print(\"\\nArquivos encontrados (primeiros 3):\")\n",
    "files = con.execute(f\"SELECT * FROM glob('{pattern_jan}') LIMIT 3\").fetchall()\n",
    "for f in files:\n",
    "    print(f[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30b7a9",
   "metadata": {},
   "source": [
    "## üîç Leitura com Glob Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ec912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrever dados para S3 em formato diferente (CSV com Compress√£o GZIP)\n",
    "backup_path = f's3://{bucket_name}/backups/logs_backup.csv.gz'\n",
    "\n",
    "print(f\"Exportando backup para: {backup_path}\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY logs_simulados \n",
    "    TO '{backup_path}' \n",
    "    (FORMAT CSV, COMPRESSION GZIP, HEADER);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Backup finalizado com sucesso.\")\n",
    "\n",
    "# Validar a leitura do arquivo comprimido diretamente do S3\n",
    "print(\"\\nValidando backup (Count):\")\n",
    "res = con.sql(f\"SELECT count(*) FROM '{backup_path}'\").fetchall()\n",
    "print(f\"Linhas no backup: {res[0][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
