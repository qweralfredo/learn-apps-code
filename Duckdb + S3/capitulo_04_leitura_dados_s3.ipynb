{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 04 Leitura Dados S3\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "capitulo_04_leitura_dados_s3\n",
    "\"\"\"\n",
    "\n",
    "# capitulo_04_leitura_dados_s3\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Exemplo/Bloco 1\n",
    "import duckdb\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT * FROM 's3://your-bucket/filename.extension';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ler arquivo Parquet completo\n",
    "SELECT * FROM 's3://my-bucket/data/sales.parquet';\n",
    "\n",
    "-- Ler apenas algumas colunas\n",
    "SELECT product_id, quantity, price\n",
    "FROM 's3://my-bucket/data/sales.parquet';\n",
    "\n",
    "-- Ler com filtro\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/data/sales.parquet'\n",
    "WHERE date >= '2024-01-01';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Sintaxe básica\n",
    "SELECT * FROM read_parquet('s3://your-bucket/file.parquet');\n",
    "\n",
    "-- Com múltiplas colunas\n",
    "SELECT column_a, column_b\n",
    "FROM read_parquet('s3://your-bucket/file.parquet');\n",
    "\n",
    "-- Com agregação\n",
    "SELECT category, sum(amount) as total\n",
    "FROM read_parquet('s3://my-bucket/transactions.parquet')\n",
    "GROUP BY category;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Leitura automática de CSV (detecta delimitador, tipos, etc)\n",
    "SELECT * FROM read_csv_auto('s3://my-bucket/data.csv');\n",
    "\n",
    "-- Com parâmetros específicos\n",
    "SELECT * FROM read_csv_auto(\n",
    "    's3://my-bucket/data.csv',\n",
    "    header = true,\n",
    "    delim = ',',\n",
    "    quote = '\"'\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Leitura de JSON\n",
    "SELECT * FROM read_json_auto('s3://my-bucket/data.json');\n",
    "\n",
    "-- JSON Lines (NDJSON)\n",
    "SELECT * FROM read_json_auto('s3://my-bucket/logs.jsonl');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ler múltiplos arquivos Parquet\n",
    "SELECT * FROM read_parquet([\n",
    "    's3://my-bucket/data/2024-01.parquet',\n",
    "    's3://my-bucket/data/2024-02.parquet',\n",
    "    's3://my-bucket/data/2024-03.parquet'\n",
    "]);\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Consolidar dados de vários meses\n",
    "SELECT\n",
    "    date_trunc('month', date) as month,\n",
    "    count(*) as total_transactions,\n",
    "    sum(amount) as total_amount\n",
    "FROM read_parquet([\n",
    "    's3://sales-bucket/2024/january.parquet',\n",
    "    's3://sales-bucket/2024/february.parquet',\n",
    "    's3://sales-bucket/2024/march.parquet'\n",
    "])\n",
    "GROUP BY month\n",
    "ORDER BY month;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ler todos os arquivos Parquet em um diretório\n",
    "SELECT * FROM 's3://my-bucket/data/*.parquet';\n",
    "\n",
    "-- Ler todos os CSV\n",
    "SELECT * FROM read_csv_auto('s3://my-bucket/csv-files/*.csv');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ler todos os Parquet em todos os subdiretórios\n",
    "SELECT * FROM 's3://my-bucket/data/**/*.parquet';\n",
    "\n",
    "-- Exemplo com estrutura hierárquica\n",
    "SELECT * FROM 's3://logs-bucket/year=2024/**/*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Caractere único (?)\n",
    "SELECT count(*) FROM 's3://my-bucket/data/file_?.parquet';\n",
    "-- Corresponde: file_1.parquet, file_2.parquet, file_a.parquet\n",
    "\n",
    "-- Conjunto de caracteres ([])\n",
    "SELECT count(*) FROM 's3://my-bucket/data/file_[0-9].parquet';\n",
    "-- Corresponde: file_0.parquet, file_1.parquet, ..., file_9.parquet\n",
    "\n",
    "-- Padrão complexo\n",
    "SELECT count(*)\n",
    "FROM read_parquet('s3://my-bucket/folder*/100?/t[0-9].parquet');\n",
    "-- Corresponde: folder1/1001/t5.parquet, folder2/1009/t3.parquet, etc.\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Estrutura: s3://logs/2024/01/01/app.parquet\n",
    "--           s3://logs/2024/01/02/app.parquet\n",
    "--           ...\n",
    "\n",
    "-- Ler todos os logs de janeiro de 2024\n",
    "SELECT *\n",
    "FROM 's3://logs-bucket/2024/01/**/*.parquet'\n",
    "WHERE timestamp >= '2024-01-01'\n",
    "  AND timestamp < '2024-02-01';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Adicionar nome do arquivo como coluna\n",
    "SELECT *\n",
    "FROM read_parquet('s3://my-bucket/data/*.parquet', filename = true);\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ver de qual arquivo cada registro veio\n",
    "SELECT\n",
    "    filename,\n",
    "    count(*) as records,\n",
    "    sum(amount) as total\n",
    "FROM read_parquet('s3://sales-bucket/2024/*.parquet', filename = true)\n",
    "GROUP BY filename\n",
    "ORDER BY filename;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ver o schema do arquivo sem baixar os dados\n",
    "SELECT * FROM parquet_schema('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Metadados completos do arquivo\n",
    "SELECT * FROM parquet_metadata('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Metadados de nível de arquivo (informações sobre row groups)\n",
    "SELECT * FROM parquet_file_metadata('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Metadados key-value customizados\n",
    "SELECT * FROM parquet_kv_metadata('s3://my-bucket/data.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Baixa apenas a coluna 'product_id' (não o arquivo inteiro)\n",
    "SELECT product_id\n",
    "FROM 's3://my-bucket/large_sales.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Lê apenas metadados (não baixa dados das colunas)\n",
    "SELECT count(*) FROM 's3://my-bucket/data.parquet';\n",
    "\n",
    "-- Muito mais rápido que baixar o arquivo inteiro!\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ❌ Ineficiente: baixa todas as 50 colunas\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/wide_table.parquet'\n",
    "WHERE id = 123;\n",
    "\n",
    "-- ✅ Eficiente: baixa apenas coluna 'id' e as colunas selecionadas\n",
    "SELECT id, name, amount\n",
    "FROM 's3://my-bucket/wide_table.parquet'\n",
    "WHERE id = 123;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- DuckDB detecta automaticamente as partições Hive\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/sales/**/*.parquet'\n",
    "WHERE year = 2024 AND month = 1;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Forçar uso de Hive partitioning\n",
    "SELECT *\n",
    "FROM read_parquet('s3://my-bucket/sales/**/*.parquet', hive_partitioning = true)\n",
    "WHERE year = 2024;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Arquivos podem ter colunas diferentes\n",
    "-- file1.parquet: id, name, age\n",
    "-- file2.parquet: id, name, salary\n",
    "\n",
    "SELECT *\n",
    "FROM read_parquet(\n",
    "    's3://my-bucket/*.parquet',\n",
    "    union_by_name = true\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ler apenas primeiras 1000 linhas\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/huge_file.parquet'\n",
    "LIMIT 1000;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Amostragem de 10% dos dados\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/large_file.parquet'\n",
    "USING SAMPLE 10%;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Pushdown de filtros: DuckDB aplica filtro antes de baixar dados\n",
    "SELECT *\n",
    "FROM 's3://my-bucket/data.parquet'\n",
    "WHERE date = '2024-01-15'  -- Filtro aplicado no Parquet row group level\n",
    "  AND status = 'active';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- CSV com delimitador customizado\n",
    "SELECT * FROM read_csv_auto(\n",
    "    's3://my-bucket/data.csv',\n",
    "    delim = ';',\n",
    "    header = true,\n",
    "    nullstr = 'NULL'\n",
    ");\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- JSON com estrutura aninhada\n",
    "SELECT\n",
    "    id,\n",
    "    data->>'name' as name,\n",
    "    data->>'email' as email\n",
    "FROM read_json_auto('s3://my-bucket/users.json');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Cada linha é um objeto JSON\n",
    "SELECT *\n",
    "FROM read_json_auto('s3://my-bucket/logs.jsonl');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Criar tabela local\n",
    "CREATE TABLE customer_segments (\n",
    "    customer_id INTEGER,\n",
    "    segment VARCHAR\n",
    ");\n",
    "\n",
    "-- Join com dados no S3\n",
    "SELECT\n",
    "    t.transaction_id,\n",
    "    t.customer_id,\n",
    "    t.amount,\n",
    "    c.segment\n",
    "FROM 's3://transactions-bucket/2024/*.parquet' t\n",
    "JOIN customer_segments c ON t.customer_id = c.customer_id\n",
    "WHERE c.segment = 'premium';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Análise de vendas por região\n",
    "SELECT\n",
    "    region,\n",
    "    date_trunc('month', date) as month,\n",
    "    count(*) as transactions,\n",
    "    sum(amount) as revenue,\n",
    "    avg(amount) as avg_order_value\n",
    "FROM 's3://sales-bucket/2024/**/*.parquet'\n",
    "GROUP BY region, month\n",
    "ORDER BY region, month;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Ranking de produtos por vendas mensais\n",
    "SELECT\n",
    "    product_id,\n",
    "    month,\n",
    "    revenue,\n",
    "    rank() OVER (PARTITION BY month ORDER BY revenue DESC) as rank\n",
    "FROM (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        date_trunc('month', date) as month,\n",
    "        sum(amount) as revenue\n",
    "    FROM 's3://sales-bucket/2024/*.parquet'\n",
    "    GROUP BY product_id, month\n",
    ")\n",
    "ORDER BY month, rank;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ❌ Erro se arquivo não existir\n",
    "SELECT * FROM 's3://my-bucket/nonexistent.parquet';\n",
    "-- Error: HTTP Error 404: Not Found\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ❌ Erro se não tiver permissão de leitura\n",
    "SELECT * FROM 's3://restricted-bucket/data.parquet';\n",
    "-- Error: Access Denied\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- ❌ Erro se tentar ler CSV como Parquet\n",
    "SELECT * FROM read_parquet('s3://my-bucket/data.csv');\n",
    "-- Error: Invalid Parquet file\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- 1. Ler um arquivo Parquet do S3\n",
    "SELECT * FROM 's3://your-bucket/test.parquet' LIMIT 10;\n",
    "\n",
    "-- 2. Contar total de registros\n",
    "SELECT count(*) FROM 's3://your-bucket/test.parquet';\n",
    "\n",
    "-- 3. Ver schema\n",
    "SELECT * FROM parquet_schema('s3://your-bucket/test.parquet');\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- 1. Ler todos os arquivos de um diretório\n",
    "SELECT count(*) FROM 's3://your-bucket/data/*.parquet';\n",
    "\n",
    "-- 2. Adicionar coluna filename\n",
    "SELECT filename, count(*) as records\n",
    "FROM read_parquet('s3://your-bucket/data/*.parquet', filename = true)\n",
    "GROUP BY filename;\n",
    "\n",
    "-- 3. Filtrar por padrão específico\n",
    "SELECT * FROM 's3://your-bucket/data/2024-0[1-3]-*.parquet';\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Análise consolidada de múltiplos arquivos\n",
    "SELECT\n",
    "    date_trunc('day', timestamp) as day,\n",
    "    count(*) as events,\n",
    "    count(DISTINCT user_id) as unique_users\n",
    "FROM 's3://logs-bucket/2024/**/*.parquet'\n",
    "WHERE timestamp >= '2024-01-01'\n",
    "GROUP BY day\n",
    "ORDER BY day;\n",
    "\"\"\")\n",
    "print(con.fetchall()) # Inspect result\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}