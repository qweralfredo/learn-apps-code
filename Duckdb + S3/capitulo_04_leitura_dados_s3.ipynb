{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 04 Leitura Dados S3\n",
    "\n",
    "Notebook gerado automaticamente a partir do código fonte python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac0b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Capítulo 04: Leitura de Dados S3 ---\n",
      "Dados de teste (sales.parquet, data.csv, data.json) criados no Bucket.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ==============================================================================\n",
    "# SETUP MINIO\n",
    "# ==============================================================================\n",
    "print(f\"--- Iniciando Capítulo 04: Leitura de Dados S3 ---\")\n",
    "\n",
    "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "BUCKET_NAME = \"learn-duckdb-s3\"\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=BUCKET_NAME)\n",
    "except ClientError:\n",
    "    pass\n",
    "\n",
    "# Helper to upload data\n",
    "def upload_data(filename, content, is_json=False):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)\n",
    "    s3_client.upload_file(filename, BUCKET_NAME, filename)\n",
    "    os.remove(filename)\n",
    "\n",
    "# 1. Parquet Data (creating locally with pandas then uploading)\n",
    "df = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3],\n",
    "    'quantity': [10, 5, 2],\n",
    "    'price': [100.0, 50.0, 1000.0],\n",
    "    'date': ['2023-01-01', '2024-01-02', '2023-12-31']\n",
    "})\n",
    "df.to_parquet(\"sales.parquet\")\n",
    "s3_client.upload_file(\"sales.parquet\", BUCKET_NAME, \"sales.parquet\")\n",
    "os.remove(\"sales.parquet\")\n",
    "\n",
    "# 2. CSV Data\n",
    "upload_data(\"data.csv\", \"id,name\\n1,Alice\\n2,Bob\")\n",
    "\n",
    "# 3. JSON Data\n",
    "upload_data(\"data.json\", '[{\"id\":1, \"data\":\"info\"}, {\"id\":2, \"data\":\"more\"}]')\n",
    "print(\"Dados de teste (sales.parquet, data.csv, data.json) criados no Bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb2ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Leitura Simples (Parquet) ---\n",
      "   product_id  quantity   price        date\n",
      "0           1        10   100.0  2023-01-01\n",
      "1           2         5    50.0  2024-01-02\n",
      "2           3         2  1000.0  2023-12-31\n",
      "\n",
      "--- Leitura com Filtro ---\n",
      "   product_id   price\n",
      "0           1   100.0\n",
      "1           3  1000.0\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EXEMPLOS DUCKDB\n",
    "# ==============================================================================\n",
    "con = duckdb.connect(database=':memory:')\n",
    "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE SECRET secret_minio (\n",
    "    TYPE S3,\n",
    "    KEY_ID '{MINIO_ACCESS_KEY}',\n",
    "    SECRET '{MINIO_SECRET_KEY}',\n",
    "    REGION 'us-east-1',\n",
    "    ENDPOINT '{MINIO_ENDPOINT.replace(\"http://\", \"\")}',\n",
    "    URL_STYLE 'path',\n",
    "    USE_SSL 'false'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- Leitura Simples (Parquet) ---\")\n",
    "res = con.execute(f\"SELECT * FROM 's3://{BUCKET_NAME}/sales.parquet'\").df()\n",
    "print(res)\n",
    "\n",
    "print(\"\\n--- Leitura com Filtro ---\")\n",
    "res = con.execute(f\"\"\"\n",
    "    SELECT product_id, price \n",
    "    FROM 's3://{BUCKET_NAME}/sales.parquet' \n",
    "    WHERE price > 50\n",
    "\"\"\").df()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8db855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Leitura CSV ---\n",
      "   id   name\n",
      "0   1  Alice\n",
      "1   2    Bob\n",
      "\n",
      "--- Leitura JSON ---\n",
      "   id  data\n",
      "0   1  info\n",
      "1   2  more\n",
      "\n",
      "--- Verificação de Schema ---\n",
      "[('product_id', 'BIGINT', 'YES', None, None, None), ('quantity', 'BIGINT', 'YES', None, None, None), ('price', 'DOUBLE', 'YES', None, None, None), ('date', 'VARCHAR', 'YES', None, None, None)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Leitura CSV ---\")\n",
    "res = con.execute(f\"SELECT * FROM 's3://{BUCKET_NAME}/data.csv'\").df()\n",
    "print(res)\n",
    "\n",
    "print(\"\\n--- Leitura JSON ---\")\n",
    "res = con.execute(f\"SELECT * FROM read_json_auto('s3://{BUCKET_NAME}/data.json')\").df()\n",
    "print(res)\n",
    "\n",
    "print(\"\\n--- Verificação de Schema ---\")\n",
    "con.execute(f\"DESCRIBE SELECT * FROM 's3://{BUCKET_NAME}/sales.parquet'\")\n",
    "print(con.fetchall())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
